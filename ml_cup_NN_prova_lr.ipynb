{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(train_losses, test_losses, train_mees, test_mees, hyperparams):\n",
    "    \"\"\"\n",
    "    Plot training and test curves for loss and Mean Euclidean Error (MEE).\n",
    "\n",
    "    Parameters:\n",
    "    - epoch (int): The total number of training epochs.\n",
    "    - train_losses (list): List of training losses for each epoch.\n",
    "    - test_losses (list): List of test losses for each epoch.\n",
    "    - train_mees (list): List of training MEE values for each epoch.\n",
    "    - test_mees (list): List of test MEE values for each epoch.\n",
    "    - hyperparams (list): List of hyperparameters used for the plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    Plots four subplots:\n",
    "    1. Training and test loss curves.\n",
    "    2. Training and test MEE curves.\n",
    "    3. Zoomed-in training and test loss curves with y-axis limit [0, 10].\n",
    "    4. Zoomed-in training and test MEE curves with y-axis limit [0, 10].\n",
    "\n",
    "    The hyperparameters are used in the plot title to provide additional context.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.suptitle(f'Batch Size={hyperparams[3]},Activation Function={hyperparams[5]}, Layers={hyperparams[6]} Hidden Units={hyperparams[0]}, Eta={hyperparams[1]}, Alpha={hyperparams[2]}, Lambda={hyperparams[4]}, dropout = {hyperparams[7]}')\n",
    "    # Loss plots\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss', color = 'red')\n",
    "    plt.plot(test_losses, label='Validation Loss', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "\n",
    "    # MEE plots\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(train_mees, label='Training MEE', color='red')\n",
    "    plt.plot(test_mees, label='Validation MEE', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss plots\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(train_losses, label='Training Loss', color = 'red')\n",
    "    plt.plot(test_losses, label='Validation Loss', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "\n",
    "    # MEE plots\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(train_mees, label='Training MEE', color='red')\n",
    "    plt.plot(test_mees, label='Validation MEE', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_euclidean_error(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Compute the mean Euclidean error between two sets of 3D vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor1: PyTorch tensor of size (N, 3) representing the first set of 3D vectors\n",
    "    - tensor2: PyTorch tensor of size (N, 3) representing the second set of 3D vectors\n",
    "\n",
    "    Returns:\n",
    "    - mean_error: Mean Euclidean error between the two sets of vectors\n",
    "    \"\"\"\n",
    "    # Check if the tensors have the correct shape\n",
    "    if tensor1.shape[1] != 3 or tensor2.shape[1] != 3 or tensor1.shape[0] != tensor2.shape[0]:\n",
    "        raise ValueError(\"Input tensors must be of size (N, 3)\")\n",
    "\n",
    "\n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = torch.norm(tensor1 - tensor2, dim=1)\n",
    "\n",
    "    # Calculate the mean Euclidean error\n",
    "    mean_error = torch.mean(euclidean_distance)\n",
    "\n",
    "    return mean_error.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regression eural network\n",
    "\n",
    "class RegressorNN(nn.Module):\n",
    "    def __init__(self, hidden_size, activation_function, num_layers, dropout_prob, input_size=10, output_size=3):\n",
    "        super(RegressorNN, self).__init__()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers = [nn.Linear(input_size, hidden_size)]\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(activation_function)\n",
    "            self.layers.append(nn.Dropout(p=dropout_prob))  # Add dropout layer\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        # Create a Sequential container for the layers\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def training_model(x_train, y_train, x_test, y_test, neuron_number,learning_rate, momentum, bs, reg_coeff, activation, layers, dropout, factor, lr_patience, optimiz, num_epochs=1000, plot_curves=False, return_history=False):\n",
    "    \"\"\"\n",
    "    Train the regression model and evaluate it on a test (can also be validation, depending on the context).\n",
    "\n",
    "    Parameters:\n",
    "    - x_train (torch.Tensor): Training input data.\n",
    "    - y_train (torch.Tensor): Training target data.\n",
    "    - x_test (torch.Tensor): Test input data.\n",
    "    - y_test (torch.Tensor): Test target data.\n",
    "    - neuron_number (int): Total number of neurons across all layers.\n",
    "    - learning_rate (float): Learning rate for the optimizer.\n",
    "    - momentum (float): Momentum for the optimizer.\n",
    "    - bs (int): Batch size for training.\n",
    "    - reg_coeff (float): Regularization coefficient for weight decay.\n",
    "    - activation (torch.nn.Module): Activation function for the model.\n",
    "    - layers (int): Number of hidden layers in the model.\n",
    "    - num_epochs (int, optional): Number of training epochs (default: 1000).\n",
    "    - plot_curves (bool, optional): Whether to plot training curves (default: False).\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Tuple containing the number of epochs, final training loss, final test loss, final training MEE, and final test MEE.\n",
    "\n",
    "    The function trains a neural network regression model using the specified hyperparameters and evaluates its performance on the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_size = int(neuron_number/layers)\n",
    "    #print(hidden_size)\n",
    "    # Create an instance of the model\n",
    "    model = RegressorNN(hidden_size=hidden_size, dropout_prob=dropout, activation_function=activation, num_layers=layers)\n",
    "    model.to(device)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    if optimiz == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=reg_coeff)\n",
    "    if optimiz == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg_coeff)\n",
    "\n",
    "    # Set the initial learning rate and create a learning rate scheduler\n",
    "    initial_lr = learning_rate\n",
    "    #scheduler = StepLR(optimizer, step_size=30, gamma=0.1)  # Adjust step_size and gamma as needed\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=lr_patience, verbose=True)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "    # Lists to store training and test losses for plotting\n",
    "    train_losses, test_losses, train_mees, test_mees = [], [], [], []\n",
    "\n",
    "    # parameters to stop at training convergence\n",
    "    min_mee = float('inf')\n",
    "    patience_counter, patience = 0, 20\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        for inputs, labels in train_dataloader:\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels) #Compute the loss\n",
    "\n",
    "            optimizer.zero_grad()   # Zero the gradients\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        #calculate loss\n",
    "        y_pred = model(x_train)\n",
    "        train_loss = criterion(y_pred, y_train)\n",
    "        # Calculate mee\n",
    "        train_mee = mean_euclidean_error(y_pred,y_train)\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_mees.append(train_mee)\n",
    "\n",
    "        # Evaluation on the test/validation set\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(x_test)\n",
    "            test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "            # Calculate test mee\n",
    "            test_mee = mean_euclidean_error(test_outputs,y_test)\n",
    "            test_mees.append(test_mee)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}],Learning Rate: {optimizer.param_groups[0][\"lr\"]}, Loss: {train_loss.item():.4f}, '\n",
    "            f'MEE: {train_mee:.4f} | Test - Epoch [{epoch+1}/{num_epochs}], '\n",
    "            f'Loss: {test_loss.item():.4f} MEE: {test_mee:.4f} ', end='\\r')\n",
    "\n",
    "\n",
    "        # Check for convergence\n",
    "        if train_mee < min_mee and abs(train_mee-min_mee)>1e-3:\n",
    "            patience_counter = 0\n",
    "            min_mee = train_mee\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter == patience:\n",
    "        #    print(f\"Convergence reached. at epoch {epoch+1} Stopping training.\")\n",
    "            break\n",
    "\n",
    "\n",
    "        # Update the learning rate using the scheduler\n",
    "        scheduler.step(train_mee)\n",
    "\n",
    "    print(f'N. Epochs = {epoch+1} - Loss (train | test/val )= ({train_loss.item():.4} | {test_loss.item():.4} ) - MEE (train | test/val ) = ({train_mee:.4} | {test_mee:.4} )')\n",
    "\n",
    "    if plot_curves:\n",
    "        hyperparams = [hidden_size,learning_rate, momentum, bs, reg_coeff, activation, layers, dropout, factor,lr_patience, num_epochs]\n",
    "        plot_training_curves(train_losses, test_losses, train_mees, test_mees, hyperparams)\n",
    "\n",
    "    if return_history:\n",
    "        return model, epoch+1, train_loss.item(), test_loss.item(), train_mee, test_mee, np.array(train_losses), np.array(test_losses), np.array(train_mees), np.array(test_mees)\n",
    "    else:\n",
    "        return model, epoch+1, train_loss.item(), test_loss.item(), train_mee, test_mee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_history(N,hist):\n",
    "    min_len = float('inf')\n",
    "    for i in range(0,N):\n",
    "        #print(len(train_losses_hist[i]))\n",
    "        if len(hist[i]) < min_len:\n",
    "            min_len = len(hist[i])\n",
    "\n",
    "    for i in range(0,N):\n",
    "        hist[i]=hist[i][:min_len]\n",
    "\n",
    "    return hist\n",
    "\n",
    "def train_with_different_initializations(x_train, y_train, x_test, y_test,\n",
    "                                         neuron_number,learning_rate, momentum,\n",
    "                                         bs, reg_coeff, activation,layers, dropout, factor, lr_patience,optimiz, max_num_epochs=1000,\n",
    "                                         plot_curves=False, N = 5, return_history=False):\n",
    "    \"\"\"\n",
    "    Train the model multiple times with different weight initializations to estimate performance mean and variance.\n",
    "\n",
    "    Parameters:\n",
    "    - x_train (torch.Tensor): Training input data.\n",
    "    - y_train (torch.Tensor): Training target data.\n",
    "    - x_test (torch.Tensor): Test input data.\n",
    "    - y_test (torch.Tensor): Test target data.\n",
    "    - neuron_number (int): Total number of neurons across all layers.\n",
    "    - learning_rate (float): Learning rate for the optimizer.\n",
    "    - momentum (float): Momentum for the optimizer.\n",
    "    - bs (int): Batch size for training.\n",
    "    - reg_coeff (float): Regularization coefficient for weight decay.\n",
    "    - activation (torch.nn.Module): Activation function for the model.\n",
    "    - layers (int): Number of hidden layers in the model.\n",
    "    - num_epochs (int, optional): Number of training epochs (default: 1000).\n",
    "    - plot_curves (bool, optional): Whether to plot training curves (default: False).\n",
    "    - N (int, optional): Number of times to train the model with different initializations (default: 5).\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    Prints the mean and standard deviation of training and test loss, as well as training and test Mean Euclidean Error (MEE).\n",
    "    \"\"\"\n",
    "    final_train_loss = []\n",
    "    final_test_loss = []\n",
    "    final_train_mee = []\n",
    "    final_test_mee = []\n",
    "    train_losses_hist, test_losses_hist, train_mees_hist, test_mees_hist = [[] for _ in range(N)],  [[] for _ in range(N)],  [[] for _ in range(N)],  [[] for _ in range(N)]\n",
    "    models = []\n",
    "    for i in range (0,N):\n",
    "        #print(f'\\n initialization {i+1}/{N}')\n",
    "        model , num_epochs,train_loss, test_loss, train_mee, test_mee, train_losses_hist[i], test_losses_hist[i], train_mees_hist[i], test_mees_hist[i] = training_model(x_train,\n",
    "                        y_train,\n",
    "                        x_test,\n",
    "                        y_test,\n",
    "                        neuron_number,\n",
    "                        learning_rate,\n",
    "                        momentum,\n",
    "                        bs,\n",
    "                        reg_coeff,\n",
    "                        activation,\n",
    "                        layers,\n",
    "                        dropout,\n",
    "                        factor,\n",
    "                        lr_patience,\n",
    "                        optimiz,\n",
    "                        plot_curves=False,\n",
    "                        num_epochs=max_num_epochs,\n",
    "                        return_history=True)\n",
    "\n",
    "\n",
    "        final_train_loss.append(train_loss)\n",
    "        final_test_loss.append(test_loss)\n",
    "        final_train_mee.append(train_mee)\n",
    "        final_test_mee.append(test_mee)\n",
    "        models.append(model)\n",
    "\n",
    "    if plot_curves:\n",
    "        hyperparams = [int(neuron_number/layers),learning_rate, momentum, bs, reg_coeff, activation, layers, dropout, num_epochs]\n",
    "        plot_training_curves(train_losses=train_losses_hist[i],test_losses=test_losses_hist[i],train_mees=train_mees_hist[i],test_mees=test_mees_hist[i],hyperparams=hyperparams)\n",
    "\n",
    "    plt.show()\n",
    "    print(f'Avg of {N} initializations: Loss (train | test/val )= ({np.mean(final_train_loss):.4} +- {np.std(final_train_loss):.4} | {np.mean(final_test_loss):.4} +- {np.std(final_test_loss):.4})'\n",
    "          f'- MEE (train | test/val ) = ( {np.mean(final_train_mee):.4} +-  {np.std(final_train_mee):.4} | {np.mean(final_test_mee):.4} +- {np.std(final_test_mee):.4})')\n",
    "\n",
    "\n",
    "    if return_history:\n",
    "        train_losses_hist = reshape_history(N,train_losses_hist)\n",
    "        test_losses_hist = reshape_history(N,test_losses_hist)\n",
    "        train_mees_hist = reshape_history(N,train_mees_hist)\n",
    "        test_mees_hist = reshape_history(N,test_mees_hist)\n",
    "        return models, train_losses_hist, test_losses_hist, train_mees_hist, test_mees_hist\n",
    "    else:\n",
    "        return np.mean(final_train_loss), np.mean(final_test_loss), np.mean(final_train_mee), np.mean(final_test_mee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search_kfold(neuron_numbers, learning_rates, momentums, batch_sizes, reg_coeffs, activations, layerss, dropouts,factors, lr_patiences, optimiz, k_folds, x, y, plot_curves=False, num_epochs=1000, number_of_init=5):\n",
    "    \"\"\"\n",
    "    Perform grid search with k-fold cross-validation for hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - neuron_numbers (list): List of neuron numbers to search.\n",
    "    - learning_rates (list): List of learning rates to search.\n",
    "    - momentums (list): List of momentum values to search.\n",
    "    - batch_sizes (list): List of batch sizes to search.\n",
    "    - reg_coeffs (list): List of regularization coefficients to search.\n",
    "    - activations (list): List of activation functions to search.\n",
    "    - layerss (list): List of numbers of hidden layers to search.\n",
    "    - k_folds (int): Number of folds for cross-validation.\n",
    "    - x (numpy.ndarray): Input data.\n",
    "    - y (numpy.ndarray): Target data.\n",
    "    - plot_curves (bool, optional): Whether to plot training curves (default: False).\n",
    "    - num_epochs (int, optional): Number of training epochs (default: 1000).\n",
    "\n",
    "    Returns:\n",
    "    - list: List of best hyperparameters.\n",
    "\n",
    "    The function performs grid search with k-fold cross-validation for Monk classifier hyperparameters and returns the best hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    best_mee = float('inf')\n",
    "    best_hyperparams = []\n",
    "\n",
    "    for neuron_number, learning_rate, momentum, bs, reg_coeff, activation, layers, dropout, factor,lr_patience in product(neuron_numbers,learning_rates,momentums,batch_sizes, reg_coeffs, activations, layerss, dropouts, factors, lr_patiences):\n",
    "        print(f'activation={activation}; layers={layers}; neuron_number={neuron_number}; lr={learning_rate}; alpha = {momentum}; batch size = {bs}; lambda = {reg_coeff}; optim = {optimiz}; factor={factor};lr_patience={lr_patience}')\n",
    "\n",
    "        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        # Lists to store training and validation losses and MEEs for each epoch\n",
    "        train_losses, val_losses, train_mees, val_mees = [], [], [], []\n",
    "\n",
    "        # Perform K-fold cross-validation\n",
    "        for fold, (train_indices, val_indices) in enumerate(kf.split(x,y)):\n",
    "            #print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "\n",
    "            # Split the data into training and validation (or test) sets\n",
    "            X_train, X_val = x[train_indices], x[val_indices]\n",
    "            Y_train, Y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "            train_loss, val_loss, train_mee, val_mee = train_with_different_initializations(\n",
    "                x_train=X_train, y_train=Y_train, x_test=X_val, y_test=Y_val,\n",
    "                neuron_number=neuron_number, learning_rate=learning_rate, momentum=momentum,\n",
    "                bs=bs, reg_coeff=reg_coeff, activation=activation, layers=layers,dropout=dropout,factor=factor, lr_patience=lr_patience, optimiz=optimiz ,plot_curves=plot_curves, max_num_epochs=num_epochs, N=number_of_init)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_mees.append(train_mee)\n",
    "            val_mees.append(val_mee)\n",
    "\n",
    "        print(f'Final Results: activation={activation}; layers={layers}; neuron number={neuron_number}; lr={learning_rate}; alpha = {momentum}; batch size = {bs}; lambda = {reg_coeff} --> '\n",
    "            f'train_loss = {np.mean(train_losses):.4} +- {np.std(train_losses):.4} | '\n",
    "            f'val_loss = {np.mean(val_losses):.4} +- {np.std(val_losses):.4}'\n",
    "            f'train_mee = {np.mean(train_mees):.4} +- {np.std(train_mees):.4} | '\n",
    "            f'val_mee = {np.mean(val_mees):.4} +- {np.std(val_mees):.4}')\n",
    "\n",
    "        if np.mean(val_mees) < best_mee:\n",
    "\n",
    "            best_mee = np.mean(val_mees)\n",
    "            best_hyperparams = [neuron_number, learning_rate, momentum, bs, reg_coeff, activation, layers, dropout, factor, lr_patience]\n",
    "\n",
    "    print(best_hyperparams,'n Best val MEE:',best_mee)\n",
    "    return best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_std(N,train_hist, test_hist,label):\n",
    "    mean_tr = np.array(train_hist).mean(axis=0)\n",
    "    std_tr = np.array(train_hist).std(axis=0)\n",
    "    mean_te = np.array(test_hist).mean(axis=0)\n",
    "    std_te = np.array(test_hist).std(axis=0)\n",
    "\n",
    "    plt.figure(figsize=(9, 8))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(mean_tr, label=f'Training {label} (mean $\\pm$ std)', color = 'red', linewidth=1)\n",
    "    plt.fill_between(range(0,len(train_hist[0])),mean_tr-std_tr, mean_tr+std_tr, color='crimson', alpha=0.3)\n",
    "\n",
    "    plt.plot(mean_te, label=f'Test {label} (mean $\\pm$ std)', color = 'blue', linestyle='--', linewidth=1)\n",
    "    plt.fill_between(range(0,len(test_hist[0])),mean_te-std_te, mean_te+std_te, color='blue', alpha=0.3)\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(label)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "\n",
    "    plt.plot(mean_tr, label=f'Training {label} (mean $\\pm$ std)', color = 'red', linewidth=1)\n",
    "    plt.fill_between(range(0,len(train_hist[0])),mean_tr-std_tr, mean_tr+std_tr, color='crimson', alpha=0.3)\n",
    "\n",
    "    plt.plot(mean_te, label=f'Test {label} (mean $\\pm$ std)', color = 'blue', linestyle='--', linewidth=1)\n",
    "    plt.fill_between(range(0,len(test_hist[0])),mean_te-std_te, mean_te+std_te, color='blue', alpha=0.3)\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(label)\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('ML-CUP23-TR.csv', delimiter=',')\n",
    "X = dataset[:,1:11]\n",
    "y = dataset[:,11:14]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# Split the data into training and testing sets (80%/20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERFORM GRID SEARCH TO FIND BEST HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.5;lr_patience=10\n",
      "Epoch [11/5000],Learning Rate: 0.0002, Loss: 535.7044, MEE: 34.9922 | Test - Epoch [11/5000], Loss: 486.3030 MEE: 33.0248 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00904: reducing learning rate of group 0 to 1.0000e-04.4948 | Test - Epoch [904/5000], Loss: 0.3561 MEE: 0.6941       \n",
      "Epoch 00957: reducing learning rate of group 0 to 5.0000e-05.4369 | Test - Epoch [957/5000], Loss: 0.3601 MEE: 0.6720 \n",
      "Epoch 01187: reducing learning rate of group 0 to 2.5000e-05.3668 | Test - Epoch [1187/5000], Loss: 0.3317 MEE: 0.6307 \n",
      "Epoch 01300: reducing learning rate of group 0 to 1.2500e-05.0.3483 | Test - Epoch [1300/5000], Loss: 0.3269 MEE: 0.6227 \n",
      "Epoch 01376: reducing learning rate of group 0 to 6.2500e-06. 0.3415 | Test - Epoch [1376/5000], Loss: 0.3215 MEE: 0.6127 \n",
      "N. Epochs = 1413 - Loss (train | test/val )= (0.09976 | 0.3195 ) - MEE (train | test/val ) = (0.3387 | 0.6139 )EE: 0.6139 \n",
      "Epoch 00843: reducing learning rate of group 0 to 1.0000e-04.5862 | Test - Epoch [843/5000], Loss: 0.4578 MEE: 0.8087       \n",
      "Epoch 01026: reducing learning rate of group 0 to 5.0000e-05..4379 | Test - Epoch [1026/5000], Loss: 0.3653 MEE: 0.6901 \n",
      "Epoch 01147: reducing learning rate of group 0 to 2.5000e-05.3955 | Test - Epoch [1147/5000], Loss: 0.3808 MEE: 0.6818 \n",
      "Epoch 01277: reducing learning rate of group 0 to 1.2500e-05.0.3688 | Test - Epoch [1277/5000], Loss: 0.3578 MEE: 0.6659 \n",
      "N. Epochs = 1302 - Loss (train | test/val )= (0.1095 | 0.3575 ) - MEE (train | test/val ) = (0.3655 | 0.6591 )MEE: 0.6591 \n",
      "Epoch 00931: reducing learning rate of group 0 to 1.0000e-04.5673 | Test - Epoch [931/5000], Loss: 0.4096 MEE: 0.7630       \n",
      "Epoch 00977: reducing learning rate of group 0 to 5.0000e-05.5169 | Test - Epoch [977/5000], Loss: 0.3871 MEE: 0.7231 \n",
      "Epoch 01089: reducing learning rate of group 0 to 2.5000e-05.4718 | Test - Epoch [1089/5000], Loss: 0.3551 MEE: 0.6932 \n",
      "Epoch 01843: reducing learning rate of group 0 to 1.2500e-05.0.3234 | Test - Epoch [1843/5000], Loss: 0.2894 MEE: 0.5993 \n",
      "Epoch 01900: reducing learning rate of group 0 to 6.2500e-06. 0.3140 | Test - Epoch [1900/5000], Loss: 0.2881 MEE: 0.5973 \n",
      "N. Epochs = 1924 - Loss (train | test/val )= (0.1219 | 0.2869 ) - MEE (train | test/val ) = (0.3117 | 0.5966 )MEE: 0.5966 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1104 +- 0.009051 | 0.3213 +- 0.02884)- MEE (train | test/val ) = ( 0.3386 +-  0.02194 | 0.6232 +- 0.02634)\n",
      "Epoch 00808: reducing learning rate of group 0 to 1.0000e-04.6161 | Test - Epoch [808/5000], Loss: 1.1070 MEE: 1.0894       \n",
      "Epoch 00883: reducing learning rate of group 0 to 5.0000e-05.5334 | Test - Epoch [883/5000], Loss: 0.9806 MEE: 1.0111 \n",
      "Epoch 01345: reducing learning rate of group 0 to 2.5000e-05.3645 | Test - Epoch [1345/5000], Loss: 0.6423 MEE: 0.8695 \n",
      "Epoch 01475: reducing learning rate of group 0 to 1.2500e-05.0.3388 | Test - Epoch [1475/5000], Loss: 0.6182 MEE: 0.8523 \n",
      "Epoch 01495: reducing learning rate of group 0 to 6.2500e-06. 0.3357 | Test - Epoch [1495/5000], Loss: 0.6138 MEE: 0.8482 \n",
      "N. Epochs = 1533 - Loss (train | test/val )= (0.0893 | 0.6141 ) - MEE (train | test/val ) = (0.3326 | 0.8486 )MEE: 0.8486 \n",
      "Epoch 00812: reducing learning rate of group 0 to 1.0000e-04.6356 | Test - Epoch [812/5000], Loss: 1.2707 MEE: 1.0395       \n",
      "Epoch 01155: reducing learning rate of group 0 to 5.0000e-05..3772 | Test - Epoch [1155/5000], Loss: 0.6334 MEE: 0.8142 \n",
      "Epoch 01230: reducing learning rate of group 0 to 2.5000e-05.3335 | Test - Epoch [1230/5000], Loss: 0.6100 MEE: 0.7840 \n",
      "Epoch 01304: reducing learning rate of group 0 to 1.2500e-05.0.3240 | Test - Epoch [1304/5000], Loss: 0.5997 MEE: 0.7763 \n",
      "Epoch 01326: reducing learning rate of group 0 to 6.2500e-06. 0.3207 | Test - Epoch [1326/5000], Loss: 0.6019 MEE: 0.7787 \n",
      "N. Epochs = 1333 - Loss (train | test/val )= (0.1245 | 0.6002 ) - MEE (train | test/val ) = (0.3195 | 0.776 ) MEE: 0.7760 \n",
      "Epoch 00893: reducing learning rate of group 0 to 1.0000e-04.4502 | Test - Epoch [893/5000], Loss: 0.7377 MEE: 0.9238       \n",
      "Epoch 00934: reducing learning rate of group 0 to 5.0000e-05.4120 | Test - Epoch [934/5000], Loss: 0.7103 MEE: 0.8960 \n",
      "Epoch 01024: reducing learning rate of group 0 to 2.5000e-05.3855 | Test - Epoch [1024/5000], Loss: 0.6536 MEE: 0.8753 \n",
      "N. Epochs = 1127 - Loss (train | test/val )= (0.1277 | 0.6415 ) - MEE (train | test/val ) = (0.3682 | 0.8675 )EE: 0.8675 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1138 +- 0.01739 | 0.6186 +- 0.01715)- MEE (train | test/val ) = ( 0.3401 +-  0.02058 | 0.8307 +- 0.03941)\n",
      "Epoch 00824: reducing learning rate of group 0 to 1.0000e-04.5304 | Test - Epoch [824/5000], Loss: 0.7837 MEE: 1.0538       \n",
      "Epoch 01005: reducing learning rate of group 0 to 5.0000e-05..3950 | Test - Epoch [1005/5000], Loss: 0.6102 MEE: 0.9343 \n",
      "Epoch 01101: reducing learning rate of group 0 to 2.5000e-05.3613 | Test - Epoch [1101/5000], Loss: 0.5751 MEE: 0.9087 \n",
      "Epoch 01393: reducing learning rate of group 0 to 1.2500e-05.0.3214 | Test - Epoch [1393/5000], Loss: 0.5218 MEE: 0.8741 \n",
      "Epoch 01427: reducing learning rate of group 0 to 6.2500e-06. 0.3171 | Test - Epoch [1427/5000], Loss: 0.5174 MEE: 0.8702 \n",
      "N. Epochs = 1436 - Loss (train | test/val )= (0.08018 | 0.5185 ) - MEE (train | test/val ) = (0.3149 | 0.8709 )EE: 0.8709 \n",
      "Epoch 00951: reducing learning rate of group 0 to 1.0000e-04.4507 | Test - Epoch [951/5000], Loss: 0.7718 MEE: 0.9689       \n",
      "Epoch 00999: reducing learning rate of group 0 to 5.0000e-05.3892 | Test - Epoch [999/5000], Loss: 0.7130 MEE: 0.9217 \n",
      "Epoch 01069: reducing learning rate of group 0 to 2.5000e-05.3735 | Test - Epoch [1069/5000], Loss: 0.6743 MEE: 0.9013 \n",
      "Epoch 01129: reducing learning rate of group 0 to 1.2500e-05.0.3656 | Test - Epoch [1129/5000], Loss: 0.6671 MEE: 0.8957 \n",
      "Epoch 01178: reducing learning rate of group 0 to 6.2500e-06. 0.3590 | Test - Epoch [1178/5000], Loss: 0.6631 MEE: 0.8901 \n",
      "N. Epochs = 1187 - Loss (train | test/val )= (0.2063 | 0.6626 ) - MEE (train | test/val ) = (0.3587 | 0.89 )6 MEE: 0.8900 \n",
      "Epoch 00957: reducing learning rate of group 0 to 1.0000e-04.4116 | Test - Epoch [957/5000], Loss: 0.6713 MEE: 0.9188       \n",
      "Epoch 00979: reducing learning rate of group 0 to 5.0000e-05.3907 | Test - Epoch [979/5000], Loss: 0.6490 MEE: 0.8956 \n",
      "N. Epochs = 1038 - Loss (train | test/val )= (0.2052 | 0.6307 ) - MEE (train | test/val ) = (0.3673 | 0.8754 ): 0.8754 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1639 +- 0.05918 | 0.604 +- 0.06183)- MEE (train | test/val ) = ( 0.347 +-  0.02297 | 0.8788 +- 0.008164)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1294 +- 0.02445 | val_loss = 0.5146 +- 0.1368train_mee = 0.3419 +- 0.003645 | val_mee = 0.7776 +- 0.1109\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.5;lr_patience=15\n",
      "Epoch 01013: reducing learning rate of group 0 to 1.0000e-04..4164 | Test - Epoch [1013/5000], Loss: 0.3112 MEE: 0.6704     \n",
      "N. Epochs = 1137 - Loss (train | test/val )= (0.08844 | 0.3017 ) - MEE (train | test/val ) = (0.3419 | 0.6284 ): 0.6284 \n",
      "Epoch 00936: reducing learning rate of group 0 to 1.0000e-04.5193 | Test - Epoch [936/5000], Loss: 0.3743 MEE: 0.7483       \n",
      "Epoch 01192: reducing learning rate of group 0 to 5.0000e-05..3456 | Test - Epoch [1192/5000], Loss: 0.2946 MEE: 0.6136 \n",
      "N. Epochs = 1224 - Loss (train | test/val )= (0.08765 | 0.2978 ) - MEE (train | test/val ) = (0.3352 | 0.6192 ) 0.6192 \n",
      "Epoch 01007: reducing learning rate of group 0 to 1.0000e-04..4369 | Test - Epoch [1007/5000], Loss: 0.3040 MEE: 0.6539     \n",
      "Epoch 01235: reducing learning rate of group 0 to 5.0000e-05..3484 | Test - Epoch [1235/5000], Loss: 0.3010 MEE: 0.6185 \n",
      "N. Epochs = 1239 - Loss (train | test/val )= (0.0839 | 0.2937 ) - MEE (train | test/val ) = (0.3262 | 0.6003 ): 0.6003 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.08666 +- 0.00198 | 0.2977 +- 0.003259)- MEE (train | test/val ) = ( 0.3345 +-  0.006423 | 0.616 +- 0.0117)\n",
      "Epoch 01024: reducing learning rate of group 0 to 1.0000e-04..3910 | Test - Epoch [1024/5000], Loss: 0.6044 MEE: 0.8269   7 \n",
      "Epoch 01081: reducing learning rate of group 0 to 5.0000e-05..3495 | Test - Epoch [1081/5000], Loss: 0.5639 MEE: 0.7982 \n",
      "N. Epochs = 1327 - Loss (train | test/val )= (0.0857 | 0.5149 ) - MEE (train | test/val ) = (0.3028 | 0.7632 ): 0.7632 \n",
      "Epoch 01009: reducing learning rate of group 0 to 1.0000e-04..3800 | Test - Epoch [1009/5000], Loss: 0.6188 MEE: 0.8345     \n",
      "Epoch 01053: reducing learning rate of group 0 to 5.0000e-05..3464 | Test - Epoch [1053/5000], Loss: 0.5801 MEE: 0.8049 \n",
      "N. Epochs = 1199 - Loss (train | test/val )= (0.09691 | 0.5473 ) - MEE (train | test/val ) = (0.3142 | 0.7834 ) 0.7834 \n",
      "Epoch 01009: reducing learning rate of group 0 to 1.0000e-04..4426 | Test - Epoch [1009/5000], Loss: 0.9063 MEE: 0.9640     \n",
      "Epoch 01076: reducing learning rate of group 0 to 5.0000e-05..3640 | Test - Epoch [1076/5000], Loss: 0.8506 MEE: 0.9120 \n",
      "Epoch 01347: reducing learning rate of group 0 to 2.5000e-05.3253 | Test - Epoch [1347/5000], Loss: 0.7590 MEE: 0.8822 \n",
      "N. Epochs = 1351 - Loss (train | test/val )= (0.09813 | 0.7536 ) - MEE (train | test/val ) = (0.3152 | 0.8733 )E: 0.8733 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.09358 +- 0.005597 | 0.6053 +- 0.1057)- MEE (train | test/val ) = ( 0.3108 +-  0.005623 | 0.8066 +- 0.04788)\n",
      "Epoch 00937: reducing learning rate of group 0 to 1.0000e-04.4387 | Test - Epoch [937/5000], Loss: 0.7510 MEE: 0.9768       \n",
      "Epoch 01173: reducing learning rate of group 0 to 5.0000e-05..3403 | Test - Epoch [1173/5000], Loss: 0.5719 MEE: 0.8744 \n",
      "Epoch 01246: reducing learning rate of group 0 to 2.5000e-05.3193 | Test - Epoch [1246/5000], Loss: 0.5522 MEE: 0.8542 \n",
      "N. Epochs = 1324 - Loss (train | test/val )= (0.1195 | 0.5414 ) - MEE (train | test/val ) = (0.311 | 0.8463 )MEE: 0.8463 \n",
      "Epoch 00864: reducing learning rate of group 0 to 1.0000e-04.4928 | Test - Epoch [864/5000], Loss: 0.7833 MEE: 0.9999       \n",
      "Epoch 01043: reducing learning rate of group 0 to 5.0000e-05..3813 | Test - Epoch [1043/5000], Loss: 0.6098 MEE: 0.8964 \n",
      "Epoch 01172: reducing learning rate of group 0 to 2.5000e-05.3583 | Test - Epoch [1172/5000], Loss: 0.5784 MEE: 0.8847 \n",
      "Epoch 01217: reducing learning rate of group 0 to 1.2500e-05.0.3456 | Test - Epoch [1217/5000], Loss: 0.5669 MEE: 0.8742 \n",
      "N. Epochs = 1221 - Loss (train | test/val )= (0.1301 | 0.5666 ) - MEE (train | test/val ) = (0.3431 | 0.8716 )MEE: 0.8716 \n",
      "Epoch 00911: reducing learning rate of group 0 to 1.0000e-04.5007 | Test - Epoch [911/5000], Loss: 0.7067 MEE: 0.9488       \n",
      "N. Epochs = 1034 - Loss (train | test/val )= (0.2009 | 0.618 ) - MEE (train | test/val ) = (0.4056 | 0.8802 )EE: 0.8802 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1502 +- 0.03614 | 0.5753 +- 0.03187)- MEE (train | test/val ) = ( 0.3532 +-  0.03927 | 0.866 +- 0.01439)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1101 +- 0.02845 | val_loss = 0.4928 +- 0.1385train_mee = 0.3328 +- 0.01737 | val_mee = 0.7629 +- 0.1066\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.6;lr_patience=10\n",
      "Epoch 00949: reducing learning rate of group 0 to 1.2000e-04.5383 | Test - Epoch [949/5000], Loss: 0.3480 MEE: 0.7317       \n",
      "Epoch 00998: reducing learning rate of group 0 to 7.2000e-05..4002 | Test - Epoch [998/5000], Loss: 0.3104 MEE: 0.6358 \n",
      "Epoch 01034: reducing learning rate of group 0 to 4.3200e-05.0.3846 | Test - Epoch [1034/5000], Loss: 0.3069 MEE: 0.6205 \n",
      "Epoch 01141: reducing learning rate of group 0 to 2.5920e-05. 0.3660 | Test - Epoch [1141/5000], Loss: 0.2941 MEE: 0.6049 \n",
      "N. Epochs = 1326 - Loss (train | test/val )= (0.091 | 0.2846 ) - MEE (train | test/val ) = (0.3424 | 0.593 )46 MEE: 0.5930 \n",
      "Epoch 00877: reducing learning rate of group 0 to 1.2000e-04.4957 | Test - Epoch [877/5000], Loss: 0.3686 MEE: 0.7048       \n",
      "Epoch 00932: reducing learning rate of group 0 to 7.2000e-05..4423 | Test - Epoch [932/5000], Loss: 0.3474 MEE: 0.6777 \n",
      "Epoch 01122: reducing learning rate of group 0 to 4.3200e-05.0.3739 | Test - Epoch [1122/5000], Loss: 0.2913 MEE: 0.6236 \n",
      "Epoch 01154: reducing learning rate of group 0 to 2.5920e-05. 0.3658 | Test - Epoch [1154/5000], Loss: 0.2847 MEE: 0.6135 \n",
      "N. Epochs = 1252 - Loss (train | test/val )= (0.09858 | 0.2804 ) - MEE (train | test/val ) = (0.3492 | 0.6053 )MEE: 0.6053 \n",
      "Epoch 00833: reducing learning rate of group 0 to 1.2000e-04.6491 | Test - Epoch [833/5000], Loss: 0.4318 MEE: 0.7727       \n",
      "Epoch 01000: reducing learning rate of group 0 to 7.2000e-05.0.4800 | Test - Epoch [1000/5000], Loss: 0.3290 MEE: 0.6664 \n",
      "Epoch 01035: reducing learning rate of group 0 to 4.3200e-05.0.4632 | Test - Epoch [1035/5000], Loss: 0.3216 MEE: 0.6544 \n",
      "Epoch 01227: reducing learning rate of group 0 to 2.5920e-05. 0.4007 | Test - Epoch [1227/5000], Loss: 0.3007 MEE: 0.6207 \n",
      "Epoch 01532: reducing learning rate of group 0 to 1.5552e-05.: 0.3445 | Test - Epoch [1532/5000], Loss: 0.2875 MEE: 0.5977 \n",
      "Epoch 01557: reducing learning rate of group 0 to 9.3312e-06.E: 0.3405 | Test - Epoch [1557/5000], Loss: 0.2841 MEE: 0.5955 \n",
      "Epoch 01578: reducing learning rate of group 0 to 5.5987e-06.E: 0.3375 | Test - Epoch [1578/5000], Loss: 0.2894 MEE: 0.5987 \n",
      "Epoch 01601: reducing learning rate of group 0 to 3.3592e-06.EE: 0.3368 | Test - Epoch [1601/5000], Loss: 0.2862 MEE: 0.5947 \n",
      "N. Epochs = 1602 - Loss (train | test/val )= (0.1095 | 0.2864 ) - MEE (train | test/val ) = (0.3367 | 0.5946 )864 MEE: 0.5946 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.09971 +- 0.007605 | 0.2838 +- 0.002503)- MEE (train | test/val ) = ( 0.3427 +-  0.005113 | 0.5976 +- 0.005475)\n",
      "Epoch 00893: reducing learning rate of group 0 to 1.2000e-04.4910 | Test - Epoch [893/5000], Loss: 0.8086 MEE: 0.9816       \n",
      "Epoch 00940: reducing learning rate of group 0 to 7.2000e-05..4285 | Test - Epoch [940/5000], Loss: 0.7766 MEE: 0.9420 \n",
      "Epoch 01015: reducing learning rate of group 0 to 4.3200e-05.0.3980 | Test - Epoch [1015/5000], Loss: 0.7557 MEE: 0.9169 \n",
      "N. Epochs = 1079 - Loss (train | test/val )= (0.1359 | 0.7431 ) - MEE (train | test/val ) = (0.3757 | 0.9015 )MEE: 0.9015 \n",
      "Epoch 00900: reducing learning rate of group 0 to 1.2000e-04.4795 | Test - Epoch [900/5000], Loss: 0.7807 MEE: 0.8898       \n",
      "Epoch 00932: reducing learning rate of group 0 to 7.2000e-05..4378 | Test - Epoch [932/5000], Loss: 0.7542 MEE: 0.8714 \n",
      "Epoch 01013: reducing learning rate of group 0 to 4.3200e-05.0.4055 | Test - Epoch [1013/5000], Loss: 0.6953 MEE: 0.8390 \n",
      "Epoch 01071: reducing learning rate of group 0 to 2.5920e-05. 0.3882 | Test - Epoch [1071/5000], Loss: 0.6734 MEE: 0.8316 \n",
      "N. Epochs = 1120 - Loss (train | test/val )= (0.1687 | 0.665 ) - MEE (train | test/val ) = (0.3803 | 0.8233 )0 MEE: 0.8233 \n",
      "Epoch 00814: reducing learning rate of group 0 to 1.2000e-04.6455 | Test - Epoch [814/5000], Loss: 1.0987 MEE: 1.0641       \n",
      "Epoch 00989: reducing learning rate of group 0 to 7.2000e-05..4251 | Test - Epoch [989/5000], Loss: 0.6945 MEE: 0.8842 \n",
      "Epoch 01026: reducing learning rate of group 0 to 4.3200e-05.0.4039 | Test - Epoch [1026/5000], Loss: 0.6784 MEE: 0.8683 \n",
      "Epoch 01094: reducing learning rate of group 0 to 2.5920e-05. 0.3829 | Test - Epoch [1094/5000], Loss: 0.6624 MEE: 0.8536 \n",
      "Epoch 01285: reducing learning rate of group 0 to 1.5552e-05.: 0.3530 | Test - Epoch [1285/5000], Loss: 0.6020 MEE: 0.8386 \n",
      "Epoch 01356: reducing learning rate of group 0 to 9.3312e-06.E: 0.3431 | Test - Epoch [1356/5000], Loss: 0.5883 MEE: 0.8301 \n",
      "N. Epochs = 1413 - Loss (train | test/val )= (0.09864 | 0.5841 ) - MEE (train | test/val ) = (0.3389 | 0.8283 ) MEE: 0.8283 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1344 +- 0.02861 | 0.6641 +- 0.06492)- MEE (train | test/val ) = ( 0.365 +-  0.01852 | 0.851 +- 0.03572)\n",
      "Epoch 00795: reducing learning rate of group 0 to 1.2000e-04.5764 | Test - Epoch [795/5000], Loss: 0.9263 MEE: 1.0634       \n",
      "Epoch 00925: reducing learning rate of group 0 to 7.2000e-05..4601 | Test - Epoch [925/5000], Loss: 0.7335 MEE: 0.9612 \n",
      "Epoch 00948: reducing learning rate of group 0 to 4.3200e-05..4453 | Test - Epoch [948/5000], Loss: 0.7199 MEE: 0.9519 \n",
      "Epoch 00971: reducing learning rate of group 0 to 2.5920e-05.0.4312 | Test - Epoch [971/5000], Loss: 0.7079 MEE: 0.9384 \n",
      "Epoch 01448: reducing learning rate of group 0 to 1.5552e-05.: 0.3506 | Test - Epoch [1448/5000], Loss: 0.5718 MEE: 0.8682 \n",
      "Epoch 01583: reducing learning rate of group 0 to 9.3312e-06.E: 0.3360 | Test - Epoch [1583/5000], Loss: 0.5481 MEE: 0.8544 \n",
      "Epoch 01637: reducing learning rate of group 0 to 5.5987e-06.E: 0.3305 | Test - Epoch [1637/5000], Loss: 0.5436 MEE: 0.8496 \n",
      "N. Epochs = 1646 - Loss (train | test/val )= (0.1327 | 0.5415 ) - MEE (train | test/val ) = (0.3293 | 0.8478 )15 MEE: 0.8478 \n",
      "Epoch 00832: reducing learning rate of group 0 to 1.2000e-04.5358 | Test - Epoch [832/5000], Loss: 0.9796 MEE: 1.0416       \n",
      "Epoch 01018: reducing learning rate of group 0 to 7.2000e-05.0.4087 | Test - Epoch [1018/5000], Loss: 0.7203 MEE: 0.9239 \n",
      "Epoch 01043: reducing learning rate of group 0 to 4.3200e-05.0.3949 | Test - Epoch [1043/5000], Loss: 0.7017 MEE: 0.9100 \n",
      "Epoch 01137: reducing learning rate of group 0 to 2.5920e-05. 0.3702 | Test - Epoch [1137/5000], Loss: 0.6645 MEE: 0.8940 \n",
      "Epoch 01339: reducing learning rate of group 0 to 1.5552e-05.: 0.3416 | Test - Epoch [1339/5000], Loss: 0.5806 MEE: 0.8618 \n",
      "Epoch 01373: reducing learning rate of group 0 to 9.3312e-06.E: 0.3375 | Test - Epoch [1373/5000], Loss: 0.5709 MEE: 0.8561 \n",
      "N. Epochs = 1379 - Loss (train | test/val )= (0.1499 | 0.57 ) - MEE (train | test/val ) = (0.3368 | 0.8545 )700 MEE: 0.8545 \n",
      "Epoch 00796: reducing learning rate of group 0 to 1.2000e-04.5640 | Test - Epoch [796/5000], Loss: 1.0486 MEE: 1.0939       \n",
      "Epoch 00978: reducing learning rate of group 0 to 7.2000e-05..4445 | Test - Epoch [978/5000], Loss: 0.7643 MEE: 0.9567 \n",
      "Epoch 01049: reducing learning rate of group 0 to 4.3200e-05.0.4114 | Test - Epoch [1049/5000], Loss: 0.7225 MEE: 0.9378 \n",
      "Epoch 01091: reducing learning rate of group 0 to 2.5920e-05. 0.3929 | Test - Epoch [1091/5000], Loss: 0.6916 MEE: 0.9211 \n",
      "N. Epochs = 1164 - Loss (train | test/val )= (0.2154 | 0.674 ) - MEE (train | test/val ) = (0.3827 | 0.9072 )0 MEE: 0.9072 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.166 +- 0.03562 | 0.5952 +- 0.05695)- MEE (train | test/val ) = ( 0.3496 +-  0.02362 | 0.8699 +- 0.02656)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1334 +- 0.02707 | val_loss = 0.5143 +- 0.1654train_mee = 0.3525 +- 0.009301 | val_mee = 0.7728 +- 0.1241\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.6;lr_patience=15\n",
      "N. Epochs = 1050 - Loss (train | test/val )= (0.1768 | 0.3666 ) - MEE (train | test/val ) = (0.4699 | 0.7327 )E: 0.7327     \n",
      "Epoch 01035: reducing learning rate of group 0 to 1.2000e-04..5356 | Test - Epoch [1035/5000], Loss: 0.3673 MEE: 0.7076     \n",
      "N. Epochs = 1039 - Loss (train | test/val )= (0.3707 | 0.329 ) - MEE (train | test/val ) = (0.482 | 0.6838 ) MEE: 0.6838 \n",
      "Epoch 00918: reducing learning rate of group 0 to 1.2000e-04.5403 | Test - Epoch [918/5000], Loss: 0.3765 MEE: 0.7340       \n",
      "Epoch 01070: reducing learning rate of group 0 to 7.2000e-05.0.4197 | Test - Epoch [1070/5000], Loss: 0.3523 MEE: 0.6785 \n",
      "Epoch 01186: reducing learning rate of group 0 to 4.3200e-05.0.3661 | Test - Epoch [1186/5000], Loss: 0.3541 MEE: 0.6525 \n",
      "N. Epochs = 1190 - Loss (train | test/val )= (0.114 | 0.3532 ) - MEE (train | test/val ) = (0.3619 | 0.6501 ) MEE: 0.6501 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.2205 +- 0.1093 | 0.3496 +- 0.01555)- MEE (train | test/val ) = ( 0.4379 +-  0.05397 | 0.6889 +- 0.0339)\n",
      "Epoch 00849: reducing learning rate of group 0 to 1.2000e-04.5226 | Test - Epoch [849/5000], Loss: 0.8498 MEE: 0.9575       \n",
      "Epoch 01094: reducing learning rate of group 0 to 7.2000e-05.0.3629 | Test - Epoch [1094/5000], Loss: 0.7029 MEE: 0.8687 \n",
      "Epoch 01142: reducing learning rate of group 0 to 4.3200e-05.0.3516 | Test - Epoch [1142/5000], Loss: 0.6884 MEE: 0.8589 \n",
      "N. Epochs = 1143 - Loss (train | test/val )= (0.124 | 0.6837 ) - MEE (train | test/val ) = (0.3468 | 0.8489 ) MEE: 0.8489 \n",
      "Epoch 00907: reducing learning rate of group 0 to 1.2000e-04.4238 | Test - Epoch [907/5000], Loss: 0.6275 MEE: 0.8883       \n",
      "Epoch 01146: reducing learning rate of group 0 to 7.2000e-05.0.3180 | Test - Epoch [1146/5000], Loss: 0.4997 MEE: 0.8030 \n",
      "N. Epochs = 1150 - Loss (train | test/val )= (0.0772 | 0.5018 ) - MEE (train | test/val ) = (0.3164 | 0.8028 )EE: 0.8028 \n",
      "Epoch 01023: reducing learning rate of group 0 to 1.2000e-04..4015 | Test - Epoch [1023/5000], Loss: 0.6147 MEE: 0.8848     \n",
      "Epoch 01164: reducing learning rate of group 0 to 7.2000e-05.0.3008 | Test - Epoch [1164/5000], Loss: 0.5890 MEE: 0.8283 \n",
      "N. Epochs = 1217 - Loss (train | test/val )= (0.06193 | 0.5783 ) - MEE (train | test/val ) = (0.2828 | 0.8135 )E: 0.8135 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.0877 +- 0.0264 | 0.5879 +- 0.07457)- MEE (train | test/val ) = ( 0.3153 +-  0.02613 | 0.8217 +- 0.01973)\n",
      "Epoch 00917: reducing learning rate of group 0 to 1.2000e-04.4610 | Test - Epoch [917/5000], Loss: 0.7272 MEE: 0.9413       \n",
      "Epoch 01013: reducing learning rate of group 0 to 7.2000e-05.0.3882 | Test - Epoch [1013/5000], Loss: 0.6212 MEE: 0.8811 \n",
      "Epoch 01208: reducing learning rate of group 0 to 4.3200e-05.0.3317 | Test - Epoch [1208/5000], Loss: 0.5259 MEE: 0.8216 \n",
      "Epoch 01262: reducing learning rate of group 0 to 2.5920e-05. 0.3234 | Test - Epoch [1262/5000], Loss: 0.5096 MEE: 0.8105 \n",
      "N. Epochs = 1303 - Loss (train | test/val )= (0.137 | 0.5039 ) - MEE (train | test/val ) = (0.3166 | 0.8039 )9 MEE: 0.8039 \n",
      "Epoch 00981: reducing learning rate of group 0 to 1.2000e-04.4199 | Test - Epoch [981/5000], Loss: 0.6502 MEE: 0.9170       \n",
      "Epoch 01155: reducing learning rate of group 0 to 7.2000e-05.0.3158 | Test - Epoch [1155/5000], Loss: 0.5007 MEE: 0.8324 \n",
      "Epoch 01217: reducing learning rate of group 0 to 4.3200e-05.0.2995 | Test - Epoch [1217/5000], Loss: 0.4715 MEE: 0.8130 \n",
      "N. Epochs = 1289 - Loss (train | test/val )= (0.08229 | 0.4599 ) - MEE (train | test/val ) = (0.2927 | 0.8121 )EE: 0.8121 \n",
      "Epoch 00967: reducing learning rate of group 0 to 1.2000e-04.4114 | Test - Epoch [967/5000], Loss: 0.6304 MEE: 0.9032       \n",
      "Epoch 01167: reducing learning rate of group 0 to 7.2000e-05.0.3078 | Test - Epoch [1167/5000], Loss: 0.4802 MEE: 0.8063 \n",
      "Epoch 01190: reducing learning rate of group 0 to 4.3200e-05.0.2962 | Test - Epoch [1190/5000], Loss: 0.4766 MEE: 0.8027 \n",
      "Epoch 01295: reducing learning rate of group 0 to 2.5920e-05. 0.2841 | Test - Epoch [1295/5000], Loss: 0.4611 MEE: 0.7910 \n",
      "Epoch 01317: reducing learning rate of group 0 to 1.5552e-05.: 0.2806 | Test - Epoch [1317/5000], Loss: 0.4550 MEE: 0.7873 \n",
      "N. Epochs = 1321 - Loss (train | test/val )= (0.07718 | 0.4568 ) - MEE (train | test/val ) = (0.2787 | 0.7892 ) MEE: 0.7892 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.09883 +- 0.02709 | 0.4735 +- 0.0215)- MEE (train | test/val ) = ( 0.296 +-  0.01563 | 0.8017 +- 0.00951)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1357 +- 0.06014 | val_loss = 0.4703 +- 0.09731train_mee = 0.3497 +- 0.06285 | val_mee = 0.7708 +- 0.0585\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.7;lr_patience=10\n",
      "Epoch 00921: reducing learning rate of group 0 to 1.4000e-04.5069 | Test - Epoch [921/5000], Loss: 0.3423 MEE: 0.7210       \n",
      "Epoch 01044: reducing learning rate of group 0 to 9.8000e-05.0.4031 | Test - Epoch [1044/5000], Loss: 0.3151 MEE: 0.6542 \n",
      "Epoch 01071: reducing learning rate of group 0 to 6.8600e-05. 0.1276, MEE: 0.3731 | Test - Epoch [1071/5000], Loss: 0.3062 MEE: 0.6458 \n",
      "Epoch 01155: reducing learning rate of group 0 to 4.8020e-05. 0.1050, MEE: 0.3499 | Test - Epoch [1155/5000], Loss: 0.2997 MEE: 0.6357 \n",
      "Epoch 01232: reducing learning rate of group 0 to 3.3614e-05. 0.0929, MEE: 0.3352 | Test - Epoch [1232/5000], Loss: 0.2977 MEE: 0.6261 \n",
      "Epoch 01247: reducing learning rate of group 0 to 2.3530e-05.: 0.0894, MEE: 0.3294 | Test - Epoch [1247/5000], Loss: 0.2924 MEE: 0.6192 \n",
      "Epoch 01284: reducing learning rate of group 0 to 1.6471e-05.: 0.0854, MEE: 0.3244 | Test - Epoch [1284/5000], Loss: 0.2947 MEE: 0.6183 \n",
      "N. Epochs = 1336 - Loss (train | test/val )= (0.08206 | 0.2931 ) - MEE (train | test/val ) = (0.3206 | 0.6166 )Loss: 0.2931 MEE: 0.6166 \n",
      "Epoch 00875: reducing learning rate of group 0 to 1.4000e-04.5321 | Test - Epoch [875/5000], Loss: 0.3761 MEE: 0.7188       \n",
      "Epoch 01008: reducing learning rate of group 0 to 9.8000e-05.0.4276 | Test - Epoch [1008/5000], Loss: 0.3193 MEE: 0.6562 \n",
      "Epoch 01026: reducing learning rate of group 0 to 6.8600e-05. 0.1738, MEE: 0.4094 | Test - Epoch [1026/5000], Loss: 0.3074 MEE: 0.6351 \n",
      "Epoch 01078: reducing learning rate of group 0 to 4.8020e-05. 0.1589, MEE: 0.3815 | Test - Epoch [1078/5000], Loss: 0.2999 MEE: 0.6220 \n",
      "N. Epochs = 1189 - Loss (train | test/val )= (0.1316 | 0.2934 ) - MEE (train | test/val ) = (0.3611 | 0.6085 )Loss: 0.2934 MEE: 0.6085 \n",
      "Epoch 00965: reducing learning rate of group 0 to 1.4000e-04.5073 | Test - Epoch [965/5000], Loss: 0.3265 MEE: 0.7286       \n",
      "Epoch 01009: reducing learning rate of group 0 to 9.8000e-05.0.4375 | Test - Epoch [1009/5000], Loss: 0.3014 MEE: 0.6540 \n",
      "Epoch 01073: reducing learning rate of group 0 to 6.8600e-05. 0.1757, MEE: 0.3994 | Test - Epoch [1073/5000], Loss: 0.3006 MEE: 0.6516 \n",
      "Epoch 01096: reducing learning rate of group 0 to 4.8020e-05. 0.1591, MEE: 0.3901 | Test - Epoch [1096/5000], Loss: 0.3005 MEE: 0.6379 \n",
      "Epoch 01168: reducing learning rate of group 0 to 3.3614e-05. 0.1394, MEE: 0.3681 | Test - Epoch [1168/5000], Loss: 0.2998 MEE: 0.6341 \n",
      "Epoch 01234: reducing learning rate of group 0 to 2.3530e-05.: 0.1222, MEE: 0.3563 | Test - Epoch [1234/5000], Loss: 0.3008 MEE: 0.6269 \n",
      "Epoch 01396: reducing learning rate of group 0 to 1.6471e-05.: 0.1011, MEE: 0.3355 | Test - Epoch [1396/5000], Loss: 0.2934 MEE: 0.6163 \n",
      "Epoch 01431: reducing learning rate of group 0 to 1.1530e-05.: 0.0973, MEE: 0.3302 | Test - Epoch [1431/5000], Loss: 0.2931 MEE: 0.6159 \n",
      "N. Epochs = 1433 - Loss (train | test/val )= (0.09689 | 0.2921 ) - MEE (train | test/val ) = (0.3289 | 0.6132 )Loss: 0.2921 MEE: 0.6132 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1035 +- 0.02077 | 0.2929 +- 0.0005375)- MEE (train | test/val ) = ( 0.3369 +-  0.01744 | 0.6128 +- 0.003329)\n",
      "Epoch 00810: reducing learning rate of group 0 to 1.4000e-04.6513 | Test - Epoch [810/5000], Loss: 1.2289 MEE: 1.0974       \n",
      "Epoch 00982: reducing learning rate of group 0 to 9.8000e-05..4428 | Test - Epoch [982/5000], Loss: 0.8187 MEE: 0.9220 \n",
      "Epoch 01043: reducing learning rate of group 0 to 6.8600e-05. 0.1771, MEE: 0.4122 | Test - Epoch [1043/5000], Loss: 0.7775 MEE: 0.9129 \n",
      "Epoch 01090: reducing learning rate of group 0 to 4.8020e-05. 0.1491, MEE: 0.3992 | Test - Epoch [1090/5000], Loss: 0.7164 MEE: 0.8939 \n",
      "Epoch 01311: reducing learning rate of group 0 to 3.3614e-05. 0.0961, MEE: 0.3345 | Test - Epoch [1311/5000], Loss: 0.6142 MEE: 0.8390 \n",
      "N. Epochs = 1369 - Loss (train | test/val )= (0.08883 | 0.6021 ) - MEE (train | test/val ) = (0.3252 | 0.8302 )Loss: 0.6021 MEE: 0.8302 \n",
      "Epoch 00854: reducing learning rate of group 0 to 1.4000e-04.5254 | Test - Epoch [854/5000], Loss: 0.8029 MEE: 0.9535       \n",
      "Epoch 00986: reducing learning rate of group 0 to 9.8000e-05..4042 | Test - Epoch [986/5000], Loss: 0.6368 MEE: 0.8598 \n",
      "N. Epochs = 1086 - Loss (train | test/val )= (0.1197 | 0.5914 ) - MEE (train | test/val ) = (0.3593 | 0.8255 )Loss: 0.5914 MEE: 0.8255 \n",
      "Epoch 00717: reducing learning rate of group 0 to 1.4000e-04.7760 | Test - Epoch [717/5000], Loss: 1.4013 MEE: 1.2271       \n",
      "Epoch 00986: reducing learning rate of group 0 to 9.8000e-05..4503 | Test - Epoch [986/5000], Loss: 0.8400 MEE: 0.9583 \n",
      "Epoch 01059: reducing learning rate of group 0 to 6.8600e-05. 0.1505, MEE: 0.4009 | Test - Epoch [1059/5000], Loss: 0.7614 MEE: 0.9198 \n",
      "Epoch 01179: reducing learning rate of group 0 to 4.8020e-05. 0.1165, MEE: 0.3522 | Test - Epoch [1179/5000], Loss: 0.7323 MEE: 0.8949 \n",
      "Epoch 01321: reducing learning rate of group 0 to 3.3614e-05. 0.0889, MEE: 0.3222 | Test - Epoch [1321/5000], Loss: 0.6808 MEE: 0.8658 \n",
      "N. Epochs = 1397 - Loss (train | test/val )= (0.07954 | 0.6639 ) - MEE (train | test/val ) = (0.3069 | 0.8537 )Loss: 0.6639 MEE: 0.8537 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.09603 +- 0.01718 | 0.6191 +- 0.03196)- MEE (train | test/val ) = ( 0.3305 +-  0.02173 | 0.8365 +- 0.01236)\n",
      "Epoch 00785: reducing learning rate of group 0 to 1.4000e-04.5719 | Test - Epoch [785/5000], Loss: 1.0471 MEE: 1.0873       \n",
      "Epoch 00856: reducing learning rate of group 0 to 9.8000e-05..5204 | Test - Epoch [856/5000], Loss: 0.9357 MEE: 1.0422 \n",
      "Epoch 00887: reducing learning rate of group 0 to 6.8600e-05.0.3377, MEE: 0.4836 | Test - Epoch [887/5000], Loss: 0.8828 MEE: 1.0097 \n",
      "Epoch 01019: reducing learning rate of group 0 to 4.8020e-05. 0.2645, MEE: 0.4286 | Test - Epoch [1019/5000], Loss: 0.7474 MEE: 0.9439 \n",
      "Epoch 01065: reducing learning rate of group 0 to 3.3614e-05. 0.2524, MEE: 0.4180 | Test - Epoch [1065/5000], Loss: 0.7276 MEE: 0.9352 \n",
      "Epoch 01326: reducing learning rate of group 0 to 2.3530e-05.: 0.2087, MEE: 0.3637 | Test - Epoch [1326/5000], Loss: 0.6434 MEE: 0.8771 \n",
      "Epoch 01366: reducing learning rate of group 0 to 1.6471e-05.: 0.1971, MEE: 0.3568 | Test - Epoch [1366/5000], Loss: 0.6250 MEE: 0.8707 \n",
      "N. Epochs = 1469 - Loss (train | test/val )= (0.1869 | 0.6032 ) - MEE (train | test/val ) = (0.3464 | 0.8587 ) Loss: 0.6032 MEE: 0.8587 \n",
      "Epoch 00871: reducing learning rate of group 0 to 1.4000e-04.4611 | Test - Epoch [871/5000], Loss: 0.6732 MEE: 0.9650       \n",
      "Epoch 00962: reducing learning rate of group 0 to 9.8000e-05..3988 | Test - Epoch [962/5000], Loss: 0.5556 MEE: 0.8908 \n",
      "Epoch 00995: reducing learning rate of group 0 to 6.8600e-05.0.1235, MEE: 0.3692 | Test - Epoch [995/5000], Loss: 0.5338 MEE: 0.8702 \n",
      "Epoch 01067: reducing learning rate of group 0 to 4.8020e-05. 0.1094, MEE: 0.3510 | Test - Epoch [1067/5000], Loss: 0.5055 MEE: 0.8572 \n",
      "Epoch 01176: reducing learning rate of group 0 to 3.3614e-05. 0.0959, MEE: 0.3327 | Test - Epoch [1176/5000], Loss: 0.4805 MEE: 0.8392 \n",
      "N. Epochs = 1229 - Loss (train | test/val )= (0.08989 | 0.4671 ) - MEE (train | test/val ) = (0.318 | 0.8233 ) Loss: 0.4671 MEE: 0.8233 \n",
      "Epoch 00887: reducing learning rate of group 0 to 1.4000e-04.4841 | Test - Epoch [887/5000], Loss: 0.8423 MEE: 1.0157       \n",
      "Epoch 00957: reducing learning rate of group 0 to 9.8000e-05..4445 | Test - Epoch [957/5000], Loss: 0.7392 MEE: 0.9698 \n",
      "Epoch 01049: reducing learning rate of group 0 to 6.8600e-05. 0.1910, MEE: 0.3774 | Test - Epoch [1049/5000], Loss: 0.6499 MEE: 0.9075 \n",
      "Epoch 01074: reducing learning rate of group 0 to 4.8020e-05. 0.1835, MEE: 0.3653 | Test - Epoch [1074/5000], Loss: 0.6302 MEE: 0.8941 \n",
      "Epoch 01127: reducing learning rate of group 0 to 3.3614e-05. 0.1706, MEE: 0.3567 | Test - Epoch [1127/5000], Loss: 0.6046 MEE: 0.8794 \n",
      "Epoch 01210: reducing learning rate of group 0 to 2.3530e-05.: 0.1603, MEE: 0.3414 | Test - Epoch [1210/5000], Loss: 0.5865 MEE: 0.8667 \n",
      "N. Epochs = 1219 - Loss (train | test/val )= (0.1592 | 0.5826 ) - MEE (train | test/val ) = (0.3399 | 0.8636 ) Loss: 0.5826 MEE: 0.8636 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1453 +- 0.04078 | 0.551 +- 0.05987)- MEE (train | test/val ) = ( 0.3348 +-  0.01217 | 0.8485 +- 0.01798)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.115 +- 0.02168 | val_loss = 0.4876 +- 0.1405train_mee = 0.334 +- 0.002666 | val_mee = 0.7659 +- 0.1084\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.7;lr_patience=15\n",
      "Epoch 00945: reducing learning rate of group 0 to 1.4000e-04.4585 | Test - Epoch [945/5000], Loss: 0.3425 MEE: 0.7110       \n",
      "Epoch 01023: reducing learning rate of group 0 to 9.8000e-05.0.3879 | Test - Epoch [1023/5000], Loss: 0.3103 MEE: 0.6468 \n",
      "N. Epochs = 1027 - Loss (train | test/val )= (0.1139 | 0.3044 ) - MEE (train | test/val ) = (0.3799 | 0.65 ), Loss: 0.3044 MEE: 0.6500 \n",
      "Epoch 00988: reducing learning rate of group 0 to 1.4000e-04.4284 | Test - Epoch [988/5000], Loss: 0.3664 MEE: 0.6832       \n",
      "Epoch 01048: reducing learning rate of group 0 to 9.8000e-05.0.3884 | Test - Epoch [1048/5000], Loss: 0.3584 MEE: 0.6675 \n",
      "Epoch 01210: reducing learning rate of group 0 to 6.8600e-05. 0.0811, MEE: 0.3273 | Test - Epoch [1210/5000], Loss: 0.3497 MEE: 0.6269 \n",
      "Epoch 01244: reducing learning rate of group 0 to 4.8020e-05. 0.0740, MEE: 0.3164 | Test - Epoch [1244/5000], Loss: 0.3554 MEE: 0.6283 \n",
      "N. Epochs = 1325 - Loss (train | test/val )= (0.06335 | 0.3549 ) - MEE (train | test/val ) = (0.2983 | 0.6189 )oss: 0.3549 MEE: 0.6189 \n",
      "Epoch 01003: reducing learning rate of group 0 to 1.4000e-04..4761 | Test - Epoch [1003/5000], Loss: 0.3137 MEE: 0.6936     \n",
      "Epoch 01119: reducing learning rate of group 0 to 9.8000e-05.0.3738 | Test - Epoch [1119/5000], Loss: 0.2926 MEE: 0.6283 \n",
      "N. Epochs = 1123 - Loss (train | test/val )= (0.1034 | 0.2888 ) - MEE (train | test/val ) = (0.3595 | 0.6257 )Loss: 0.2888 MEE: 0.6257 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.09355 +- 0.02178 | 0.3161 +- 0.0282)- MEE (train | test/val ) = ( 0.3459 +-  0.03467 | 0.6315 +- 0.01334)\n",
      "Epoch 01013: reducing learning rate of group 0 to 1.4000e-04..3985 | Test - Epoch [1013/5000], Loss: 0.6921 MEE: 0.8919     \n",
      "Epoch 01051: reducing learning rate of group 0 to 9.8000e-05.0.3588 | Test - Epoch [1051/5000], Loss: 0.6776 MEE: 0.8673 \n",
      "Epoch 01100: reducing learning rate of group 0 to 6.8600e-05. 0.0906, MEE: 0.3258 | Test - Epoch [1100/5000], Loss: 0.6560 MEE: 0.8400 \n",
      "N. Epochs = 1141 - Loss (train | test/val )= (0.08193 | 0.6486 ) - MEE (train | test/val ) = (0.3101 | 0.8373 )oss: 0.6486 MEE: 0.8373 \n",
      "N. Epochs = 887 - Loss (train | test/val )= (0.2304 | 0.6787 ) - MEE (train | test/val ) = (0.508 | 0.9004 )E: 0.9004       \n",
      "Epoch 00937: reducing learning rate of group 0 to 1.4000e-04.4255 | Test - Epoch [937/5000], Loss: 0.6896 MEE: 0.9109       \n",
      "Epoch 01006: reducing learning rate of group 0 to 9.8000e-05.0.3891 | Test - Epoch [1006/5000], Loss: 0.6268 MEE: 0.8777 \n",
      "Epoch 01079: reducing learning rate of group 0 to 6.8600e-05. 0.0992, MEE: 0.3552 | Test - Epoch [1079/5000], Loss: 0.5885 MEE: 0.8497 \n",
      "Epoch 01156: reducing learning rate of group 0 to 4.8020e-05. 0.0832, MEE: 0.3227 | Test - Epoch [1156/5000], Loss: 0.5711 MEE: 0.8354 \n",
      "N. Epochs = 1213 - Loss (train | test/val )= (0.07687 | 0.5686 ) - MEE (train | test/val ) = (0.3118 | 0.8294 )oss: 0.5686 MEE: 0.8294 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1297 +- 0.07123 | 0.632 +- 0.04644)- MEE (train | test/val ) = ( 0.3766 +-  0.09289 | 0.8557 +- 0.03179)\n",
      "N. Epochs = 881 - Loss (train | test/val )= (0.2845 | 0.7917 ) - MEE (train | test/val ) = (0.5028 | 1.021 )E: 1.0215       \n",
      "Epoch 00971: reducing learning rate of group 0 to 1.4000e-04.4772 | Test - Epoch [971/5000], Loss: 0.6339 MEE: 0.9444       \n",
      "Epoch 01035: reducing learning rate of group 0 to 9.8000e-05.0.3622 | Test - Epoch [1035/5000], Loss: 0.5688 MEE: 0.8709 \n",
      "Epoch 01225: reducing learning rate of group 0 to 6.8600e-05. 0.0921, MEE: 0.3075 | Test - Epoch [1225/5000], Loss: 0.4589 MEE: 0.8024 \n",
      "Epoch 01257: reducing learning rate of group 0 to 4.8020e-05. 0.0875, MEE: 0.3013 | Test - Epoch [1257/5000], Loss: 0.4546 MEE: 0.8078 \n",
      "Epoch 01297: reducing learning rate of group 0 to 3.3614e-05. 0.0810, MEE: 0.2906 | Test - Epoch [1297/5000], Loss: 0.4448 MEE: 0.7972 \n",
      "N. Epochs = 1301 - Loss (train | test/val )= (0.08049 | 0.4452 ) - MEE (train | test/val ) = (0.2901 | 0.7975 )Loss: 0.4452 MEE: 0.7975 \n",
      "N. Epochs = 967 - Loss (train | test/val )= (0.165 | 0.6143 ) - MEE (train | test/val ) = (0.4468 | 0.9383 )E: 0.9383       \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1767 +- 0.0837 | 0.617 +- 0.1415)- MEE (train | test/val ) = ( 0.4132 +-  0.09005 | 0.9191 +- 0.09245)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1333 +- 0.03402 | val_loss = 0.5217 +- 0.1455train_mee = 0.3786 +- 0.02753 | val_mee = 0.8021 +- 0.1234\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.8;lr_patience=10\n",
      "Epoch 00935: reducing learning rate of group 0 to 1.6000e-04.5320 | Test - Epoch [935/5000], Loss: 0.3295 MEE: 0.7398       \n",
      "Epoch 00984: reducing learning rate of group 0 to 1.2800e-04..4431 | Test - Epoch [984/5000], Loss: 0.2904 MEE: 0.6558 \n",
      "Epoch 01026: reducing learning rate of group 0 to 1.0240e-04.: 0.1437, MEE: 0.4096 | Test - Epoch [1026/5000], Loss: 0.2806 MEE: 0.6429 \n",
      "Epoch 01113: reducing learning rate of group 0 to 8.1920e-05.: 0.1117, MEE: 0.3616 | Test - Epoch [1113/5000], Loss: 0.2673 MEE: 0.6103 \n",
      "Epoch 01172: reducing learning rate of group 0 to 6.5536e-05. 0.0891, MEE: 0.3409 | Test - Epoch [1172/5000], Loss: 0.2669 MEE: 0.6050 \n",
      "Epoch 01210: reducing learning rate of group 0 to 5.2429e-05. 0.0811, MEE: 0.3299 | Test - Epoch [1210/5000], Loss: 0.2637 MEE: 0.5981 \n",
      "Epoch 01277: reducing learning rate of group 0 to 4.1943e-05. 0.0722, MEE: 0.3150 | Test - Epoch [1277/5000], Loss: 0.2571 MEE: 0.5868 \n",
      "Epoch 01326: reducing learning rate of group 0 to 3.3554e-05.: 0.0674, MEE: 0.3068 | Test - Epoch [1326/5000], Loss: 0.2505 MEE: 0.5824 \n",
      "Epoch 01390: reducing learning rate of group 0 to 2.6844e-05. 0.0633, MEE: 0.3007 | Test - Epoch [1390/5000], Loss: 0.2505 MEE: 0.5800 \n",
      "N. Epochs = 1399 - Loss (train | test/val )= (0.06211 | 0.2477 ) - MEE (train | test/val ) = (0.2956 | 0.5799 )Loss: 0.2477 MEE: 0.5799 \n",
      "Epoch 00828: reducing learning rate of group 0 to 1.6000e-04.6730 | Test - Epoch [828/5000], Loss: 0.4627 MEE: 0.8073       \n",
      "Epoch 00861: reducing learning rate of group 0 to 1.2800e-04..6392 | Test - Epoch [861/5000], Loss: 0.4521 MEE: 0.8016 \n",
      "Epoch 00916: reducing learning rate of group 0 to 1.0240e-04. 0.4316, MEE: 0.5665 | Test - Epoch [916/5000], Loss: 0.4123 MEE: 0.7624 \n",
      "Epoch 00957: reducing learning rate of group 0 to 8.1920e-05. 0.3487, MEE: 0.5286 | Test - Epoch [957/5000], Loss: 0.3909 MEE: 0.7409 \n",
      "Epoch 01031: reducing learning rate of group 0 to 6.5536e-05. 0.2716, MEE: 0.4905 | Test - Epoch [1031/5000], Loss: 0.3626 MEE: 0.7085 \n",
      "Epoch 01082: reducing learning rate of group 0 to 5.2429e-05. 0.2191, MEE: 0.4525 | Test - Epoch [1082/5000], Loss: 0.3538 MEE: 0.6909 \n",
      "Epoch 01128: reducing learning rate of group 0 to 4.1943e-05. 0.1959, MEE: 0.4327 | Test - Epoch [1128/5000], Loss: 0.3431 MEE: 0.6728 \n",
      "Epoch 01165: reducing learning rate of group 0 to 3.3554e-05.: 0.1842, MEE: 0.4248 | Test - Epoch [1165/5000], Loss: 0.3393 MEE: 0.6672 \n",
      "Epoch 01291: reducing learning rate of group 0 to 2.6844e-05. 0.1402, MEE: 0.3877 | Test - Epoch [1291/5000], Loss: 0.3281 MEE: 0.6478 \n",
      "Epoch 01386: reducing learning rate of group 0 to 2.1475e-05.: 0.1227, MEE: 0.3711 | Test - Epoch [1386/5000], Loss: 0.3216 MEE: 0.6421 \n",
      "Epoch 01589: reducing learning rate of group 0 to 1.7180e-05.: 0.0934, MEE: 0.3392 | Test - Epoch [1589/5000], Loss: 0.3108 MEE: 0.6241 \n",
      "N. Epochs = 1618 - Loss (train | test/val )= (0.09078 | 0.3108 ) - MEE (train | test/val ) = (0.3359 | 0.6194 )Loss: 0.3108 MEE: 0.6194 \n",
      "Epoch 00909: reducing learning rate of group 0 to 1.6000e-04.5084 | Test - Epoch [909/5000], Loss: 0.3065 MEE: 0.6907       \n",
      "Epoch 00983: reducing learning rate of group 0 to 1.2800e-04..4543 | Test - Epoch [983/5000], Loss: 0.2900 MEE: 0.6741 \n",
      "Epoch 01036: reducing learning rate of group 0 to 1.0240e-04.: 0.1686, MEE: 0.4009 | Test - Epoch [1036/5000], Loss: 0.2695 MEE: 0.6237 \n",
      "Epoch 01086: reducing learning rate of group 0 to 8.1920e-05.: 0.1410, MEE: 0.3746 | Test - Epoch [1086/5000], Loss: 0.2593 MEE: 0.6161 \n",
      "Epoch 01118: reducing learning rate of group 0 to 6.5536e-05. 0.1258, MEE: 0.3629 | Test - Epoch [1118/5000], Loss: 0.2549 MEE: 0.6036 \n",
      "Epoch 01161: reducing learning rate of group 0 to 5.2429e-05. 0.1133, MEE: 0.3580 | Test - Epoch [1161/5000], Loss: 0.2581 MEE: 0.6131 \n",
      "Epoch 01222: reducing learning rate of group 0 to 4.1943e-05. 0.0977, MEE: 0.3336 | Test - Epoch [1222/5000], Loss: 0.2544 MEE: 0.6027 \n",
      "Epoch 01399: reducing learning rate of group 0 to 3.3554e-05.: 0.0711, MEE: 0.3082 | Test - Epoch [1399/5000], Loss: 0.2493 MEE: 0.5972 \n",
      "Epoch 01462: reducing learning rate of group 0 to 2.6844e-05. 0.0649, MEE: 0.2937 | Test - Epoch [1462/5000], Loss: 0.2489 MEE: 0.5895 \n",
      "Epoch 01498: reducing learning rate of group 0 to 2.1475e-05.: 0.0622, MEE: 0.2890 | Test - Epoch [1498/5000], Loss: 0.2442 MEE: 0.5821 \n",
      "Epoch 01550: reducing learning rate of group 0 to 1.7180e-05.: 0.0595, MEE: 0.2842 | Test - Epoch [1550/5000], Loss: 0.2426 MEE: 0.5765 \n",
      "Epoch 01572: reducing learning rate of group 0 to 1.3744e-05.: 0.0583, MEE: 0.2809 | Test - Epoch [1572/5000], Loss: 0.2437 MEE: 0.5783 \n",
      "N. Epochs = 1579 - Loss (train | test/val )= (0.05818 | 0.2422 ) - MEE (train | test/val ) = (0.2808 | 0.5745 )Loss: 0.2422 MEE: 0.5745 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.07036 +- 0.01453 | 0.2669 +- 0.03113)- MEE (train | test/val ) = ( 0.3041 +-  0.0233 | 0.5913 +- 0.02003)\n",
      "Epoch 00864: reducing learning rate of group 0 to 1.6000e-04.4975 | Test - Epoch [864/5000], Loss: 0.8135 MEE: 0.9607       \n",
      "Epoch 00914: reducing learning rate of group 0 to 1.2800e-04..4352 | Test - Epoch [914/5000], Loss: 0.6808 MEE: 0.8889 \n",
      "Epoch 01015: reducing learning rate of group 0 to 1.0240e-04.: 0.1282, MEE: 0.3764 | Test - Epoch [1015/5000], Loss: 0.5991 MEE: 0.8478 \n",
      "Epoch 01036: reducing learning rate of group 0 to 8.1920e-05.: 0.1211, MEE: 0.3717 | Test - Epoch [1036/5000], Loss: 0.5974 MEE: 0.8454 \n",
      "Epoch 01097: reducing learning rate of group 0 to 6.5536e-05. 0.1016, MEE: 0.3408 | Test - Epoch [1097/5000], Loss: 0.5676 MEE: 0.8305 \n",
      "Epoch 01147: reducing learning rate of group 0 to 5.2429e-05. 0.0926, MEE: 0.3320 | Test - Epoch [1147/5000], Loss: 0.5479 MEE: 0.8180 \n",
      "Epoch 01192: reducing learning rate of group 0 to 4.1943e-05. 0.0866, MEE: 0.3324 | Test - Epoch [1192/5000], Loss: 0.5386 MEE: 0.8115 \n",
      "Epoch 01296: reducing learning rate of group 0 to 3.3554e-05.: 0.0729, MEE: 0.3058 | Test - Epoch [1296/5000], Loss: 0.5225 MEE: 0.8043 \n",
      "N. Epochs = 1336 - Loss (train | test/val )= (0.06918 | 0.5129 ) - MEE (train | test/val ) = (0.2997 | 0.7935 )oss: 0.5129 MEE: 0.7935 \n",
      "Epoch 00695: reducing learning rate of group 0 to 1.6000e-04.8860 | Test - Epoch [695/5000], Loss: 2.3337 MEE: 1.3763       \n",
      "Epoch 00989: reducing learning rate of group 0 to 1.2800e-04..4875 | Test - Epoch [989/5000], Loss: 0.8000 MEE: 0.9016 \n",
      "Epoch 01137: reducing learning rate of group 0 to 1.0240e-04.: 0.1320, MEE: 0.3491 | Test - Epoch [1137/5000], Loss: 0.6313 MEE: 0.8109 \n",
      "Epoch 01167: reducing learning rate of group 0 to 8.1920e-05.: 0.1209, MEE: 0.3254 | Test - Epoch [1167/5000], Loss: 0.6171 MEE: 0.7936 \n",
      "Epoch 01213: reducing learning rate of group 0 to 6.5536e-05. 0.1135, MEE: 0.3235 | Test - Epoch [1213/5000], Loss: 0.6085 MEE: 0.7892 \n",
      "Epoch 01231: reducing learning rate of group 0 to 5.2429e-05. 0.1099, MEE: 0.3125 | Test - Epoch [1231/5000], Loss: 0.6051 MEE: 0.7850 \n",
      "Epoch 01244: reducing learning rate of group 0 to 4.1943e-05. 0.1070, MEE: 0.3027 | Test - Epoch [1244/5000], Loss: 0.6051 MEE: 0.7806 \n",
      "Epoch 01279: reducing learning rate of group 0 to 3.3554e-05.: 0.1039, MEE: 0.2978 | Test - Epoch [1279/5000], Loss: 0.5991 MEE: 0.7782 \n",
      "Epoch 01316: reducing learning rate of group 0 to 2.6844e-05. 0.1009, MEE: 0.2931 | Test - Epoch [1316/5000], Loss: 0.5962 MEE: 0.7749 \n",
      "N. Epochs = 1318 - Loss (train | test/val )= (0.1008 | 0.5952 ) - MEE (train | test/val ) = (0.2924 | 0.7729 ) Loss: 0.5952 MEE: 0.7729 \n",
      "Epoch 00834: reducing learning rate of group 0 to 1.6000e-04.5537 | Test - Epoch [834/5000], Loss: 0.9654 MEE: 0.9890       \n",
      "Epoch 00968: reducing learning rate of group 0 to 1.2800e-04..4298 | Test - Epoch [968/5000], Loss: 0.7158 MEE: 0.8892 \n",
      "Epoch 01058: reducing learning rate of group 0 to 1.0240e-04.: 0.1239, MEE: 0.3743 | Test - Epoch [1058/5000], Loss: 0.6280 MEE: 0.8600 \n",
      "Epoch 01142: reducing learning rate of group 0 to 8.1920e-05.: 0.0936, MEE: 0.3306 | Test - Epoch [1142/5000], Loss: 0.5646 MEE: 0.8170 \n",
      "N. Epochs = 1151 - Loss (train | test/val )= (0.09194 | 0.5661 ) - MEE (train | test/val ) = (0.3295 | 0.8209 )oss: 0.5661 MEE: 0.8209 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.0873 +- 0.01331 | 0.5581 +- 0.03408)- MEE (train | test/val ) = ( 0.3072 +-  0.01604 | 0.7958 +- 0.01967)\n",
      "Epoch 00869: reducing learning rate of group 0 to 1.6000e-04.4874 | Test - Epoch [869/5000], Loss: 0.7028 MEE: 0.9780       \n",
      "Epoch 00936: reducing learning rate of group 0 to 1.2800e-04..4590 | Test - Epoch [936/5000], Loss: 0.6219 MEE: 0.9519 \n",
      "Epoch 00963: reducing learning rate of group 0 to 1.0240e-04. 0.1618, MEE: 0.4003 | Test - Epoch [963/5000], Loss: 0.5874 MEE: 0.9085 \n",
      "Epoch 00994: reducing learning rate of group 0 to 8.1920e-05. 0.1504, MEE: 0.3880 | Test - Epoch [994/5000], Loss: 0.5660 MEE: 0.8920 \n",
      "Epoch 01046: reducing learning rate of group 0 to 6.5536e-05. 0.1296, MEE: 0.3719 | Test - Epoch [1046/5000], Loss: 0.5420 MEE: 0.8803 \n",
      "Epoch 01070: reducing learning rate of group 0 to 5.2429e-05. 0.1222, MEE: 0.3566 | Test - Epoch [1070/5000], Loss: 0.5304 MEE: 0.8683 \n",
      "N. Epochs = 1119 - Loss (train | test/val )= (0.111 | 0.5101 ) - MEE (train | test/val ) = (0.351 | 0.8542 ), Loss: 0.5101 MEE: 0.8542 \n",
      "Epoch 00860: reducing learning rate of group 0 to 1.6000e-04.4657 | Test - Epoch [860/5000], Loss: 0.8046 MEE: 0.9582       \n",
      "Epoch 00967: reducing learning rate of group 0 to 1.2800e-04..4149 | Test - Epoch [967/5000], Loss: 0.6820 MEE: 0.9020 \n",
      "Epoch 01020: reducing learning rate of group 0 to 1.0240e-04.: 0.2171, MEE: 0.3683 | Test - Epoch [1020/5000], Loss: 0.6393 MEE: 0.8706 \n",
      "Epoch 01073: reducing learning rate of group 0 to 8.1920e-05.: 0.2082, MEE: 0.3734 | Test - Epoch [1073/5000], Loss: 0.6148 MEE: 0.8613 \n",
      "Epoch 01093: reducing learning rate of group 0 to 6.5536e-05. 0.1996, MEE: 0.3408 | Test - Epoch [1093/5000], Loss: 0.6054 MEE: 0.8434 \n",
      "Epoch 01120: reducing learning rate of group 0 to 5.2429e-05. 0.1950, MEE: 0.3358 | Test - Epoch [1120/5000], Loss: 0.5959 MEE: 0.8374 \n",
      "Epoch 01135: reducing learning rate of group 0 to 4.1943e-05. 0.1931, MEE: 0.3347 | Test - Epoch [1135/5000], Loss: 0.5950 MEE: 0.8387 \n",
      "N. Epochs = 1180 - Loss (train | test/val )= (0.1884 | 0.5832 ) - MEE (train | test/val ) = (0.3249 | 0.8293 ) Loss: 0.5832 MEE: 0.8293 \n",
      "Epoch 00872: reducing learning rate of group 0 to 1.6000e-04.4913 | Test - Epoch [872/5000], Loss: 0.9463 MEE: 1.0056       \n",
      "Epoch 00946: reducing learning rate of group 0 to 1.2800e-04..4544 | Test - Epoch [946/5000], Loss: 0.8315 MEE: 0.9642 \n",
      "Epoch 00963: reducing learning rate of group 0 to 1.0240e-04. 0.2839, MEE: 0.4222 | Test - Epoch [963/5000], Loss: 0.8129 MEE: 0.9430 \n",
      "Epoch 01068: reducing learning rate of group 0 to 8.1920e-05.: 0.2263, MEE: 0.3828 | Test - Epoch [1068/5000], Loss: 0.7069 MEE: 0.8978 \n",
      "Epoch 01102: reducing learning rate of group 0 to 6.5536e-05. 0.2059, MEE: 0.3692 | Test - Epoch [1102/5000], Loss: 0.6722 MEE: 0.8870 \n",
      "Epoch 01207: reducing learning rate of group 0 to 5.2429e-05. 0.1800, MEE: 0.3615 | Test - Epoch [1207/5000], Loss: 0.6255 MEE: 0.8738 \n",
      "Epoch 01225: reducing learning rate of group 0 to 4.1943e-05. 0.1731, MEE: 0.3390 | Test - Epoch [1225/5000], Loss: 0.6127 MEE: 0.8540 \n",
      "Epoch 01242: reducing learning rate of group 0 to 3.3554e-05.: 0.1706, MEE: 0.3369 | Test - Epoch [1242/5000], Loss: 0.6129 MEE: 0.8611 \n",
      "Epoch 01256: reducing learning rate of group 0 to 2.6844e-05. 0.1683, MEE: 0.3316 | Test - Epoch [1256/5000], Loss: 0.6017 MEE: 0.8487 \n",
      "Epoch 01270: reducing learning rate of group 0 to 2.1475e-05.: 0.1667, MEE: 0.3319 | Test - Epoch [1270/5000], Loss: 0.6043 MEE: 0.8534 \n",
      "N. Epochs = 1326 - Loss (train | test/val )= (0.1621 | 0.5955 ) - MEE (train | test/val ) = (0.3259 | 0.8482 ) Loss: 0.5955 MEE: 0.8482 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1538 +- 0.03213 | 0.5629 +- 0.03769)- MEE (train | test/val ) = ( 0.3339 +-  0.01208 | 0.8439 +- 0.0106)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1038 +- 0.03603 | val_loss = 0.4626 +- 0.1384train_mee = 0.3151 +- 0.01339 | val_mee = 0.7437 +- 0.1095\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.8;lr_patience=15\n",
      "Epoch 00825: reducing learning rate of group 0 to 1.6000e-04.6429 | Test - Epoch [825/5000], Loss: 0.4771 MEE: 0.8028       \n",
      "N. Epochs = 1046 - Loss (train | test/val )= (0.1911 | 0.3502 ) - MEE (train | test/val ) = (0.4328 | 0.6586 )EE: 0.6586 \n",
      "Epoch 01051: reducing learning rate of group 0 to 1.6000e-04..4509 | Test - Epoch [1051/5000], Loss: 0.2649 MEE: 0.6639     \n",
      "N. Epochs = 1055 - Loss (train | test/val )= (0.186 | 0.2426 ) - MEE (train | test/val ) = (0.4107 | 0.617 ) MEE: 0.6170 \n",
      "Epoch 01039: reducing learning rate of group 0 to 1.6000e-04..4226 | Test - Epoch [1039/5000], Loss: 0.3525 MEE: 0.6745     \n",
      "N. Epochs = 1197 - Loss (train | test/val )= (0.09582 | 0.3002 ) - MEE (train | test/val ) = (0.3287 | 0.6059 )E: 0.6059 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1576 +- 0.04376 | 0.2977 +- 0.04396)- MEE (train | test/val ) = ( 0.3907 +-  0.04476 | 0.6272 +- 0.02267)\n",
      "Epoch 01000: reducing learning rate of group 0 to 1.6000e-04..4002 | Test - Epoch [1000/5000], Loss: 0.7000 MEE: 0.9219     \n",
      "Epoch 01108: reducing learning rate of group 0 to 1.2800e-04.0.3243 | Test - Epoch [1108/5000], Loss: 0.6436 MEE: 0.8593 \n",
      "Epoch 01175: reducing learning rate of group 0 to 1.0240e-04.: 0.0711, MEE: 0.3309 | Test - Epoch [1175/5000], Loss: 0.6477 MEE: 0.8788 \n",
      "N. Epochs = 1219 - Loss (train | test/val )= (0.05759 | 0.6279 ) - MEE (train | test/val ) = (0.2859 | 0.8435 )Loss: 0.6279 MEE: 0.8435 \n",
      "N. Epochs = 969 - Loss (train | test/val )= (0.1502 | 0.7716 ) - MEE (train | test/val ) = (0.4243 | 0.9631 ): 0.9631       \n",
      "Epoch 00927: reducing learning rate of group 0 to 1.6000e-04.5243 | Test - Epoch [927/5000], Loss: 0.7127 MEE: 0.9760       \n",
      "N. Epochs = 931 - Loss (train | test/val )= (0.1895 | 0.6741 ) - MEE (train | test/val ) = (0.4582 | 0.8949 )E: 0.8949 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1324 +- 0.05529 | 0.6912 +- 0.05988)- MEE (train | test/val ) = ( 0.3894 +-  0.07452 | 0.9005 +- 0.04897)\n",
      "Epoch 00919: reducing learning rate of group 0 to 1.6000e-04.4608 | Test - Epoch [919/5000], Loss: 0.7120 MEE: 0.9485       \n",
      "Epoch 01025: reducing learning rate of group 0 to 1.2800e-04.0.4073 | Test - Epoch [1025/5000], Loss: 0.6013 MEE: 0.9146 \n",
      "N. Epochs = 1029 - Loss (train | test/val )= (0.1608 | 0.5914 ) - MEE (train | test/val ) = (0.3811 | 0.8991 ) Loss: 0.5914 MEE: 0.8991 \n",
      "Epoch 00964: reducing learning rate of group 0 to 1.6000e-04.4295 | Test - Epoch [964/5000], Loss: 0.6124 MEE: 0.9007       \n",
      "N. Epochs = 968 - Loss (train | test/val )= (0.1773 | 0.609 ) - MEE (train | test/val ) = (0.4106 | 0.8988 )EE: 0.8988 \n",
      "Epoch 00949: reducing learning rate of group 0 to 1.6000e-04.4233 | Test - Epoch [949/5000], Loss: 0.6241 MEE: 0.9225       \n",
      "N. Epochs = 950 - Loss (train | test/val )= (0.1716 | 0.6161 ) - MEE (train | test/val ) = (0.3966 | 0.8966 )E: 0.8966 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1699 +- 0.006838 | 0.6055 +- 0.01034)- MEE (train | test/val ) = ( 0.3961 +-  0.01204 | 0.8982 +- 0.001141)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1533 +- 0.01562 | val_loss = 0.5314 +- 0.169train_mee = 0.3921 +- 0.002898 | val_mee = 0.8086 +- 0.1283\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.9;lr_patience=10\n",
      "Epoch 00898: reducing learning rate of group 0 to 1.8000e-04.5991 | Test - Epoch [898/5000], Loss: 0.4096 MEE: 0.7375       \n",
      "Epoch 00970: reducing learning rate of group 0 to 1.6200e-04..4978 | Test - Epoch [970/5000], Loss: 0.3544 MEE: 0.6907 \n",
      "Epoch 01075: reducing learning rate of group 0 to 1.4580e-04. 0.4147 | Test - Epoch [1075/5000], Loss: 0.3240 MEE: 0.6450 \n",
      "Epoch 01095: reducing learning rate of group 0 to 1.3122e-04.: 0.1662, MEE: 0.3917 | Test - Epoch [1095/5000], Loss: 0.3187 MEE: 0.6485 \n",
      "Epoch 01144: reducing learning rate of group 0 to 1.1810e-04.: 0.1343, MEE: 0.3578 | Test - Epoch [1144/5000], Loss: 0.3061 MEE: 0.6268 \n",
      "Epoch 01200: reducing learning rate of group 0 to 1.0629e-04.: 0.1122, MEE: 0.3405 | Test - Epoch [1200/5000], Loss: 0.3071 MEE: 0.6227 \n",
      "Epoch 01291: reducing learning rate of group 0 to 9.5659e-05.: 0.0870, MEE: 0.3096 | Test - Epoch [1291/5000], Loss: 0.2942 MEE: 0.6113 \n",
      "Epoch 01323: reducing learning rate of group 0 to 8.6093e-05. 0.0750, MEE: 0.2949 | Test - Epoch [1323/5000], Loss: 0.2889 MEE: 0.5920 \n",
      "Epoch 01339: reducing learning rate of group 0 to 7.7484e-05. 0.0721, MEE: 0.2894 | Test - Epoch [1339/5000], Loss: 0.2949 MEE: 0.5998 \n",
      "Epoch 01352: reducing learning rate of group 0 to 6.9736e-05. 0.0689, MEE: 0.2851 | Test - Epoch [1352/5000], Loss: 0.2938 MEE: 0.5907 \n",
      "Epoch 01383: reducing learning rate of group 0 to 6.2762e-05. 0.0630, MEE: 0.2807 | Test - Epoch [1383/5000], Loss: 0.2885 MEE: 0.5838 \n",
      "Epoch 01425: reducing learning rate of group 0 to 5.6486e-05. 0.0579, MEE: 0.2713 | Test - Epoch [1425/5000], Loss: 0.2926 MEE: 0.5910 \n",
      "N. Epochs = 1434 - Loss (train | test/val )= (0.05733 | 0.2891 ) - MEE (train | test/val ) = (0.2696 | 0.5888 )Loss: 0.2891 MEE: 0.5888 \n",
      "Epoch 00920: reducing learning rate of group 0 to 1.8000e-04.5271 | Test - Epoch [920/5000], Loss: 0.3101 MEE: 0.7080       \n",
      "Epoch 00990: reducing learning rate of group 0 to 1.6200e-04..4351 | Test - Epoch [990/5000], Loss: 0.2792 MEE: 0.6438 \n",
      "Epoch 01045: reducing learning rate of group 0 to 1.4580e-04. 0.4117 | Test - Epoch [1045/5000], Loss: 0.2844 MEE: 0.6405 \n",
      "N. Epochs = 1054 - Loss (train | test/val )= (0.1607 | 0.2899 ) - MEE (train | test/val ) = (0.4433 | 0.6653 ) Loss: 0.2899 MEE: 0.6653 \n",
      "Epoch 01010: reducing learning rate of group 0 to 1.8000e-04..4885 | Test - Epoch [1010/5000], Loss: 0.3524 MEE: 0.7012     \n",
      "Epoch 01042: reducing learning rate of group 0 to 1.6200e-04.0.4306 | Test - Epoch [1042/5000], Loss: 0.3398 MEE: 0.6715 \n",
      "Epoch 01114: reducing learning rate of group 0 to 1.4580e-04. 0.3926 | Test - Epoch [1114/5000], Loss: 0.3322 MEE: 0.6598 \n",
      "Epoch 01151: reducing learning rate of group 0 to 1.3122e-04.: 0.1214, MEE: 0.3556 | Test - Epoch [1151/5000], Loss: 0.3236 MEE: 0.6294 \n",
      "N. Epochs = 1160 - Loss (train | test/val )= (0.1193 | 0.3254 ) - MEE (train | test/val ) = (0.3513 | 0.6263 ) Loss: 0.3254 MEE: 0.6263 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1125 +- 0.04247 | 0.3015 +- 0.0169)- MEE (train | test/val ) = ( 0.3547 +-  0.07096 | 0.6268 +- 0.03123)\n",
      "Epoch 00771: reducing learning rate of group 0 to 1.8000e-04.7260 | Test - Epoch [771/5000], Loss: 1.4305 MEE: 1.1627       \n",
      "Epoch 00912: reducing learning rate of group 0 to 1.6200e-04..4949 | Test - Epoch [912/5000], Loss: 0.8898 MEE: 0.9576 \n",
      "Epoch 00983: reducing learning rate of group 0 to 1.4580e-04.0.4387 | Test - Epoch [983/5000], Loss: 0.7894 MEE: 0.9230 \n",
      "Epoch 01013: reducing learning rate of group 0 to 1.3122e-04.: 0.1760, MEE: 0.4103 | Test - Epoch [1013/5000], Loss: 0.7332 MEE: 0.8914 \n",
      "Epoch 01098: reducing learning rate of group 0 to 1.1810e-04.: 0.1356, MEE: 0.3651 | Test - Epoch [1098/5000], Loss: 0.6871 MEE: 0.8724 \n",
      "Epoch 01138: reducing learning rate of group 0 to 1.0629e-04.: 0.1102, MEE: 0.3465 | Test - Epoch [1138/5000], Loss: 0.6414 MEE: 0.8575 \n",
      "Epoch 01174: reducing learning rate of group 0 to 9.5659e-05.: 0.0980, MEE: 0.3224 | Test - Epoch [1174/5000], Loss: 0.6278 MEE: 0.8475 \n",
      "Epoch 01188: reducing learning rate of group 0 to 8.6093e-05. 0.0949, MEE: 0.3169 | Test - Epoch [1188/5000], Loss: 0.6166 MEE: 0.8304 \n",
      "Epoch 01218: reducing learning rate of group 0 to 7.7484e-05. 0.0899, MEE: 0.3107 | Test - Epoch [1218/5000], Loss: 0.6145 MEE: 0.8308 \n",
      "Epoch 01234: reducing learning rate of group 0 to 6.9736e-05. 0.0878, MEE: 0.3069 | Test - Epoch [1234/5000], Loss: 0.6119 MEE: 0.8250 \n",
      "Epoch 01257: reducing learning rate of group 0 to 6.2762e-05. 0.0847, MEE: 0.3026 | Test - Epoch [1257/5000], Loss: 0.6102 MEE: 0.8302 \n",
      "Epoch 01288: reducing learning rate of group 0 to 5.6486e-05. 0.0810, MEE: 0.2949 | Test - Epoch [1288/5000], Loss: 0.6127 MEE: 0.8305 \n",
      "N. Epochs = 1289 - Loss (train | test/val )= (0.08181 | 0.6119 ) - MEE (train | test/val ) = (0.3006 | 0.8282 )Loss: 0.6119 MEE: 0.8282 \n",
      "Epoch 00728: reducing learning rate of group 0 to 1.8000e-04.8109 | Test - Epoch [728/5000], Loss: 1.4308 MEE: 1.2551       \n",
      "Epoch 00895: reducing learning rate of group 0 to 1.6200e-04..5273 | Test - Epoch [895/5000], Loss: 1.0047 MEE: 1.0364 \n",
      "Epoch 00987: reducing learning rate of group 0 to 1.4580e-04.0.4402 | Test - Epoch [987/5000], Loss: 0.8656 MEE: 0.9520 \n",
      "Epoch 01014: reducing learning rate of group 0 to 1.3122e-04.: 0.1758, MEE: 0.4358 | Test - Epoch [1014/5000], Loss: 0.8568 MEE: 0.9636 \n",
      "Epoch 01072: reducing learning rate of group 0 to 1.1810e-04.: 0.1367, MEE: 0.3818 | Test - Epoch [1072/5000], Loss: 0.8003 MEE: 0.9102 \n",
      "Epoch 01116: reducing learning rate of group 0 to 1.0629e-04.: 0.1092, MEE: 0.3505 | Test - Epoch [1116/5000], Loss: 0.7498 MEE: 0.8936 \n",
      "Epoch 01158: reducing learning rate of group 0 to 9.5659e-05.: 0.0960, MEE: 0.3287 | Test - Epoch [1158/5000], Loss: 0.7213 MEE: 0.8701 \n",
      "Epoch 01188: reducing learning rate of group 0 to 8.6093e-05. 0.0896, MEE: 0.3244 | Test - Epoch [1188/5000], Loss: 0.7090 MEE: 0.8668 \n",
      "Epoch 01230: reducing learning rate of group 0 to 7.7484e-05. 0.0808, MEE: 0.3051 | Test - Epoch [1230/5000], Loss: 0.7017 MEE: 0.8545 \n",
      "Epoch 01250: reducing learning rate of group 0 to 6.9736e-05. 0.0792, MEE: 0.3089 | Test - Epoch [1250/5000], Loss: 0.7044 MEE: 0.8629 \n",
      "N. Epochs = 1259 - Loss (train | test/val )= (0.07781 | 0.7012 ) - MEE (train | test/val ) = (0.305 | 0.8517 )Loss: 0.7012 MEE: 0.8517 \n",
      "Epoch 00864: reducing learning rate of group 0 to 1.8000e-04.5813 | Test - Epoch [864/5000], Loss: 0.9278 MEE: 0.9900       \n",
      "Epoch 00939: reducing learning rate of group 0 to 1.6200e-04..4454 | Test - Epoch [939/5000], Loss: 0.7728 MEE: 0.8668 \n",
      "Epoch 01041: reducing learning rate of group 0 to 1.4580e-04. 0.3826 | Test - Epoch [1041/5000], Loss: 0.6356 MEE: 0.8299 \n",
      "Epoch 01054: reducing learning rate of group 0 to 1.3122e-04.: 0.1429, MEE: 0.3890 | Test - Epoch [1054/5000], Loss: 0.6098 MEE: 0.8082 \n",
      "Epoch 01113: reducing learning rate of group 0 to 1.1810e-04.: 0.1118, MEE: 0.3593 | Test - Epoch [1113/5000], Loss: 0.5787 MEE: 0.8010 \n",
      "Epoch 01163: reducing learning rate of group 0 to 1.0629e-04.: 0.0968, MEE: 0.3343 | Test - Epoch [1163/5000], Loss: 0.5536 MEE: 0.7793 \n",
      "Epoch 01179: reducing learning rate of group 0 to 9.5659e-05.: 0.0911, MEE: 0.3156 | Test - Epoch [1179/5000], Loss: 0.5575 MEE: 0.7798 \n",
      "Epoch 01197: reducing learning rate of group 0 to 8.6093e-05. 0.0881, MEE: 0.3138 | Test - Epoch [1197/5000], Loss: 0.5552 MEE: 0.7771 \n",
      "Epoch 01226: reducing learning rate of group 0 to 7.7484e-05. 0.0806, MEE: 0.3032 | Test - Epoch [1226/5000], Loss: 0.5414 MEE: 0.7655 \n",
      "Epoch 01249: reducing learning rate of group 0 to 6.9736e-05. 0.0758, MEE: 0.2916 | Test - Epoch [1249/5000], Loss: 0.5353 MEE: 0.7645 \n",
      "N. Epochs = 1270 - Loss (train | test/val )= (0.07422 | 0.5303 ) - MEE (train | test/val ) = (0.2907 | 0.7632 )oss: 0.5303 MEE: 0.7632 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.07795 +- 0.003101 | 0.6145 +- 0.06981)- MEE (train | test/val ) = ( 0.2987 +-  0.00598 | 0.8144 +- 0.03744)\n",
      "Epoch 00826: reducing learning rate of group 0 to 1.8000e-04.5381 | Test - Epoch [826/5000], Loss: 0.8863 MEE: 1.0501       \n",
      "Epoch 00948: reducing learning rate of group 0 to 1.6200e-04..4207 | Test - Epoch [948/5000], Loss: 0.6477 MEE: 0.9315 \n",
      "Epoch 01002: reducing learning rate of group 0 to 1.4580e-04. 0.4399 | Test - Epoch [1002/5000], Loss: 0.6024 MEE: 0.9384 \n",
      "Epoch 01029: reducing learning rate of group 0 to 1.3122e-04.: 0.1509, MEE: 0.3786 | Test - Epoch [1029/5000], Loss: 0.5565 MEE: 0.8911 \n",
      "Epoch 01090: reducing learning rate of group 0 to 1.1810e-04.: 0.1311, MEE: 0.3438 | Test - Epoch [1090/5000], Loss: 0.5174 MEE: 0.8522 \n",
      "Epoch 01105: reducing learning rate of group 0 to 1.0629e-04.: 0.1208, MEE: 0.3343 | Test - Epoch [1105/5000], Loss: 0.4992 MEE: 0.8458 \n",
      "Epoch 01158: reducing learning rate of group 0 to 9.5659e-05.: 0.1077, MEE: 0.3286 | Test - Epoch [1158/5000], Loss: 0.4759 MEE: 0.8382 \n",
      "Epoch 01196: reducing learning rate of group 0 to 8.6093e-05. 0.0999, MEE: 0.3178 | Test - Epoch [1196/5000], Loss: 0.4593 MEE: 0.8194 \n",
      "N. Epochs = 1205 - Loss (train | test/val )= (0.09398 | 0.4562 ) - MEE (train | test/val ) = (0.3032 | 0.8163 )oss: 0.4562 MEE: 0.8163 \n",
      "Epoch 00871: reducing learning rate of group 0 to 1.8000e-04.4819 | Test - Epoch [871/5000], Loss: 0.7180 MEE: 0.9610       \n",
      "Epoch 00912: reducing learning rate of group 0 to 1.6200e-04..4415 | Test - Epoch [912/5000], Loss: 0.6523 MEE: 0.9281 \n",
      "Epoch 00977: reducing learning rate of group 0 to 1.4580e-04.0.4022 | Test - Epoch [977/5000], Loss: 0.5667 MEE: 0.8847 \n",
      "Epoch 00996: reducing learning rate of group 0 to 1.3122e-04. 0.1726, MEE: 0.3882 | Test - Epoch [996/5000], Loss: 0.5455 MEE: 0.8657 \n",
      "Epoch 01050: reducing learning rate of group 0 to 1.1810e-04.: 0.1462, MEE: 0.3514 | Test - Epoch [1050/5000], Loss: 0.5100 MEE: 0.8417 \n",
      "Epoch 01075: reducing learning rate of group 0 to 1.0629e-04.: 0.1279, MEE: 0.3551 | Test - Epoch [1075/5000], Loss: 0.4803 MEE: 0.8259 \n",
      "Epoch 01095: reducing learning rate of group 0 to 9.5659e-05.: 0.1151, MEE: 0.3459 | Test - Epoch [1095/5000], Loss: 0.4747 MEE: 0.8323 \n",
      "Epoch 01164: reducing learning rate of group 0 to 8.6093e-05. 0.0877, MEE: 0.3299 | Test - Epoch [1164/5000], Loss: 0.4524 MEE: 0.8121 \n",
      "Epoch 01193: reducing learning rate of group 0 to 7.7484e-05. 0.0791, MEE: 0.3030 | Test - Epoch [1193/5000], Loss: 0.4407 MEE: 0.7983 \n",
      "Epoch 01214: reducing learning rate of group 0 to 6.9736e-05. 0.0762, MEE: 0.3069 | Test - Epoch [1214/5000], Loss: 0.4275 MEE: 0.7935 \n",
      "N. Epochs = 1215 - Loss (train | test/val )= (0.07496 | 0.4283 ) - MEE (train | test/val ) = (0.3005 | 0.789 )Loss: 0.4283 MEE: 0.7890 \n",
      "Epoch 00840: reducing learning rate of group 0 to 1.8000e-04.5318 | Test - Epoch [840/5000], Loss: 0.8579 MEE: 1.0322       \n",
      "Epoch 00893: reducing learning rate of group 0 to 1.6200e-04..4624 | Test - Epoch [893/5000], Loss: 0.7721 MEE: 0.9846 \n",
      "Epoch 00931: reducing learning rate of group 0 to 1.4580e-04.0.4312 | Test - Epoch [931/5000], Loss: 0.6933 MEE: 0.9356 \n",
      "Epoch 00967: reducing learning rate of group 0 to 1.3122e-04. 0.1957, MEE: 0.4119 | Test - Epoch [967/5000], Loss: 0.6455 MEE: 0.9214 \n",
      "Epoch 01033: reducing learning rate of group 0 to 1.1810e-04.: 0.1709, MEE: 0.3843 | Test - Epoch [1033/5000], Loss: 0.5945 MEE: 0.8907 \n",
      "Epoch 01084: reducing learning rate of group 0 to 1.0629e-04.: 0.1563, MEE: 0.3649 | Test - Epoch [1084/5000], Loss: 0.5616 MEE: 0.8692 \n",
      "Epoch 01140: reducing learning rate of group 0 to 9.5659e-05.: 0.1437, MEE: 0.3382 | Test - Epoch [1140/5000], Loss: 0.5407 MEE: 0.8534 \n",
      "Epoch 01169: reducing learning rate of group 0 to 8.6093e-05. 0.1361, MEE: 0.3267 | Test - Epoch [1169/5000], Loss: 0.5307 MEE: 0.8465 \n",
      "Epoch 01201: reducing learning rate of group 0 to 7.7484e-05. 0.1242, MEE: 0.3231 | Test - Epoch [1201/5000], Loss: 0.5075 MEE: 0.8284 \n",
      "Epoch 01218: reducing learning rate of group 0 to 6.9736e-05. 0.1208, MEE: 0.3149 | Test - Epoch [1218/5000], Loss: 0.5047 MEE: 0.8300 \n",
      "Epoch 01230: reducing learning rate of group 0 to 6.2762e-05. 0.1202, MEE: 0.3212 | Test - Epoch [1230/5000], Loss: 0.5009 MEE: 0.8308 \n",
      "Epoch 01253: reducing learning rate of group 0 to 5.6486e-05. 0.1161, MEE: 0.3062 | Test - Epoch [1253/5000], Loss: 0.4984 MEE: 0.8252 \n",
      "Epoch 01272: reducing learning rate of group 0 to 5.0837e-05.: 0.1144, MEE: 0.3066 | Test - Epoch [1272/5000], Loss: 0.4938 MEE: 0.8232 \n",
      "N. Epochs = 1278 - Loss (train | test/val )= (0.1135 | 0.4923 ) - MEE (train | test/val ) = (0.3034 | 0.8234 )Loss: 0.4923 MEE: 0.8234 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.09416 +- 0.01575 | 0.459 +- 0.02618)- MEE (train | test/val ) = ( 0.3024 +-  0.001307 | 0.8096 +- 0.01481)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.09485 +- 0.01409 | val_loss = 0.4583 +- 0.1278train_mee = 0.3186 +- 0.02557 | val_mee = 0.7503 +- 0.08729\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.9;lr_patience=15\n",
      "Epoch 01029: reducing learning rate of group 0 to 1.8000e-04..4382 | Test - Epoch [1029/5000], Loss: 0.2736 MEE: 0.6377     \n",
      "N. Epochs = 1065 - Loss (train | test/val )= (0.1313 | 0.2573 ) - MEE (train | test/val ) = (0.3929 | 0.5974 )EE: 0.5974 \n",
      "Epoch 01015: reducing learning rate of group 0 to 1.8000e-04..4277 | Test - Epoch [1015/5000], Loss: 0.3646 MEE: 0.7005     \n",
      "N. Epochs = 1019 - Loss (train | test/val )= (0.1422 | 0.3671 ) - MEE (train | test/val ) = (0.4286 | 0.6903 )EE: 0.6903 \n",
      "Epoch 00991: reducing learning rate of group 0 to 1.8000e-04.4614 | Test - Epoch [991/5000], Loss: 0.3681 MEE: 0.6977       \n",
      "Epoch 01165: reducing learning rate of group 0 to 1.6200e-04.0.3246 | Test - Epoch [1165/5000], Loss: 0.3176 MEE: 0.6289 \n",
      "N. Epochs = 1169 - Loss (train | test/val )= (0.07826 | 0.3236 ) - MEE (train | test/val ) = (0.3406 | 0.6363 )EE: 0.6363 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1172 +- 0.02792 | 0.316 +- 0.04514)- MEE (train | test/val ) = ( 0.3874 +-  0.03614 | 0.6413 +- 0.03811)\n",
      "Epoch 00907: reducing learning rate of group 0 to 1.8000e-04.5261 | Test - Epoch [907/5000], Loss: 0.8557 MEE: 1.0079       \n",
      "N. Epochs = 911 - Loss (train | test/val )= (0.221 | 0.8533 ) - MEE (train | test/val ) = (0.5149 | 0.9968 )EE: 0.9968 \n",
      "Epoch 00982: reducing learning rate of group 0 to 1.8000e-04.3908 | Test - Epoch [982/5000], Loss: 0.6535 MEE: 0.8884       \n",
      "Epoch 01007: reducing learning rate of group 0 to 1.6200e-04.0.3767 | Test - Epoch [1007/5000], Loss: 0.6422 MEE: 0.8806 \n",
      "Epoch 01123: reducing learning rate of group 0 to 1.4580e-04. 0.3327 | Test - Epoch [1123/5000], Loss: 0.5841 MEE: 0.8394 \n",
      "N. Epochs = 1125 - Loss (train | test/val )= (0.07404 | 0.5909 ) - MEE (train | test/val ) = (0.3182 | 0.8345 )Loss: 0.5909 MEE: 0.8345 \n",
      "Epoch 00969: reducing learning rate of group 0 to 1.8000e-04.3877 | Test - Epoch [969/5000], Loss: 0.6489 MEE: 0.8732       \n",
      "N. Epochs = 973 - Loss (train | test/val )= (0.1286 | 0.64 ) - MEE (train | test/val ) = (0.3875 | 0.8814 )MEE: 0.8814 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1412 +- 0.06064 | 0.6947 +- 0.1139)- MEE (train | test/val ) = ( 0.4069 +-  0.08147 | 0.9043 +- 0.06818)\n",
      "N. Epochs = 1015 - Loss (train | test/val )= (0.1904 | 0.6478 ) - MEE (train | test/val ) = (0.3823 | 0.9016 )E: 0.9016     \n",
      "Epoch 00883: reducing learning rate of group 0 to 1.8000e-04.4681 | Test - Epoch [883/5000], Loss: 0.7671 MEE: 0.9839       \n",
      "Epoch 01003: reducing learning rate of group 0 to 1.6200e-04.0.4027 | Test - Epoch [1003/5000], Loss: 0.6024 MEE: 0.9238 \n",
      "N. Epochs = 1007 - Loss (train | test/val )= (0.1396 | 0.5977 ) - MEE (train | test/val ) = (0.3783 | 0.9189 )MEE: 0.9189 \n",
      "N. Epochs = 927 - Loss (train | test/val )= (0.2348 | 0.7195 ) - MEE (train | test/val ) = (0.4689 | 0.9732 ): 0.9732       \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1883 +- 0.03889 | 0.655 +- 0.04997)- MEE (train | test/val ) = ( 0.4098 +-  0.04177 | 0.9312 +- 0.03048)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0002; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1489 +- 0.02949 | val_loss = 0.5552 +- 0.1699train_mee = 0.4014 +- 0.009952 | val_mee = 0.8256 +- 0.1308\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.5;lr_patience=10\n",
      "Epoch 00644: reducing learning rate of group 0 to 1.5000e-04.5762 | Test - Epoch [644/5000], Loss: 0.3408 MEE: 0.7342     \n",
      "Epoch 00849: reducing learning rate of group 0 to 7.5000e-05..3924 | Test - Epoch [849/5000], Loss: 0.2699 MEE: 0.6284 \n",
      "Epoch 00870: reducing learning rate of group 0 to 3.7500e-05..3811 | Test - Epoch [870/5000], Loss: 0.2667 MEE: 0.6253 \n",
      "Epoch 00920: reducing learning rate of group 0 to 1.8750e-05.0.3641 | Test - Epoch [920/5000], Loss: 0.2625 MEE: 0.6158 \n",
      "N. Epochs = 997 - Loss (train | test/val )= (0.1141 | 0.2597 ) - MEE (train | test/val ) = (0.3579 | 0.6129 )MEE: 0.6129 \n",
      "Epoch 00577: reducing learning rate of group 0 to 1.5000e-04.7372 | Test - Epoch [577/5000], Loss: 0.5624 MEE: 0.8670     \n",
      "Epoch 00760: reducing learning rate of group 0 to 7.5000e-05..5433 | Test - Epoch [760/5000], Loss: 0.4192 MEE: 0.7515 \n",
      "Epoch 00893: reducing learning rate of group 0 to 3.7500e-05..4885 | Test - Epoch [893/5000], Loss: 0.3758 MEE: 0.7073 \n",
      "Epoch 01127: reducing learning rate of group 0 to 1.8750e-05. 0.4082 | Test - Epoch [1127/5000], Loss: 0.3420 MEE: 0.6597 \n",
      "Epoch 01250: reducing learning rate of group 0 to 9.3750e-06.: 0.3882 | Test - Epoch [1250/5000], Loss: 0.3337 MEE: 0.6470 \n",
      "N. Epochs = 1294 - Loss (train | test/val )= (0.1861 | 0.332 ) - MEE (train | test/val ) = (0.383 | 0.6414 )20 MEE: 0.6414 \n",
      "Epoch 00622: reducing learning rate of group 0 to 1.5000e-04.6068 | Test - Epoch [622/5000], Loss: 0.3679 MEE: 0.7713     \n",
      "Epoch 00743: reducing learning rate of group 0 to 7.5000e-05..4573 | Test - Epoch [743/5000], Loss: 0.2968 MEE: 0.6590 \n",
      "Epoch 00944: reducing learning rate of group 0 to 3.7500e-05..3855 | Test - Epoch [944/5000], Loss: 0.2629 MEE: 0.6088 \n",
      "Epoch 01196: reducing learning rate of group 0 to 1.8750e-05. 0.3386 | Test - Epoch [1196/5000], Loss: 0.2419 MEE: 0.5785 \n",
      "N. Epochs = 1250 - Loss (train | test/val )= (0.08227 | 0.2425 ) - MEE (train | test/val ) = (0.3292 | 0.5731 )MEE: 0.5731 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1275 +- 0.04345 | 0.2781 +- 0.03881)- MEE (train | test/val ) = ( 0.3567 +-  0.02197 | 0.6091 +- 0.02804)\n",
      "Epoch 00650: reducing learning rate of group 0 to 1.5000e-04.5617 | Test - Epoch [650/5000], Loss: 0.8865 MEE: 0.9911     \n",
      "Epoch 00897: reducing learning rate of group 0 to 7.5000e-05..3524 | Test - Epoch [897/5000], Loss: 0.5919 MEE: 0.8092 \n",
      "Epoch 00968: reducing learning rate of group 0 to 3.7500e-05..3278 | Test - Epoch [968/5000], Loss: 0.5679 MEE: 0.8007 \n",
      "Epoch 01012: reducing learning rate of group 0 to 1.8750e-05. 0.3158 | Test - Epoch [1012/5000], Loss: 0.5603 MEE: 0.7903 \n",
      "Epoch 01070: reducing learning rate of group 0 to 9.3750e-06.: 0.3118 | Test - Epoch [1070/5000], Loss: 0.5586 MEE: 0.7895 \n",
      "N. Epochs = 1079 - Loss (train | test/val )= (0.1023 | 0.5585 ) - MEE (train | test/val ) = (0.3101 | 0.7888 ) MEE: 0.7888 \n",
      "Epoch 00552: reducing learning rate of group 0 to 1.5000e-04.8010 | Test - Epoch [552/5000], Loss: 1.8118 MEE: 1.2298     \n",
      "Epoch 00849: reducing learning rate of group 0 to 7.5000e-05..4393 | Test - Epoch [849/5000], Loss: 0.7058 MEE: 0.8656 \n",
      "Epoch 00921: reducing learning rate of group 0 to 3.7500e-05..3970 | Test - Epoch [921/5000], Loss: 0.6382 MEE: 0.8405 \n",
      "Epoch 01180: reducing learning rate of group 0 to 1.8750e-05. 0.3356 | Test - Epoch [1180/5000], Loss: 0.5660 MEE: 0.7912 \n",
      "Epoch 01302: reducing learning rate of group 0 to 9.3750e-06.: 0.3248 | Test - Epoch [1302/5000], Loss: 0.5587 MEE: 0.7827 \n",
      "N. Epochs = 1336 - Loss (train | test/val )= (0.1021 | 0.5559 ) - MEE (train | test/val ) = (0.3202 | 0.7808 ) MEE: 0.7808 \n",
      "Epoch 00641: reducing learning rate of group 0 to 1.5000e-04.6441 | Test - Epoch [641/5000], Loss: 1.0636 MEE: 1.0949     \n",
      "Epoch 00769: reducing learning rate of group 0 to 7.5000e-05..4204 | Test - Epoch [769/5000], Loss: 0.8600 MEE: 0.9399 \n",
      "Epoch 00895: reducing learning rate of group 0 to 3.7500e-05..3782 | Test - Epoch [895/5000], Loss: 0.8176 MEE: 0.9100 \n",
      "Epoch 00908: reducing learning rate of group 0 to 1.8750e-05.0.3703 | Test - Epoch [908/5000], Loss: 0.8143 MEE: 0.9092 \n",
      "N. Epochs = 954 - Loss (train | test/val )= (0.1541 | 0.8042 ) - MEE (train | test/val ) = (0.3659 | 0.9011 )MEE: 0.9011 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1195 +- 0.02448 | 0.6395 +- 0.1164)- MEE (train | test/val ) = ( 0.3321 +-  0.0243 | 0.8236 +- 0.05491)\n",
      "Epoch 00539: reducing learning rate of group 0 to 1.5000e-04.7258 | Test - Epoch [539/5000], Loss: 1.3965 MEE: 1.2334     \n",
      "Epoch 00836: reducing learning rate of group 0 to 7.5000e-05..4329 | Test - Epoch [836/5000], Loss: 0.8178 MEE: 0.9609 \n",
      "Epoch 01005: reducing learning rate of group 0 to 3.7500e-05.0.3705 | Test - Epoch [1005/5000], Loss: 0.7109 MEE: 0.9218 \n",
      "Epoch 01061: reducing learning rate of group 0 to 1.8750e-05. 0.3590 | Test - Epoch [1061/5000], Loss: 0.6966 MEE: 0.9084 \n",
      "Epoch 01089: reducing learning rate of group 0 to 9.3750e-06.: 0.3533 | Test - Epoch [1089/5000], Loss: 0.6939 MEE: 0.9057 \n",
      "Epoch 01124: reducing learning rate of group 0 to 4.6875e-06.: 0.3502 | Test - Epoch [1124/5000], Loss: 0.6905 MEE: 0.9048 \n",
      "N. Epochs = 1129 - Loss (train | test/val )= (0.212 | 0.6898 ) - MEE (train | test/val ) = (0.3496 | 0.9036 )98 MEE: 0.9036 \n",
      "Epoch 00549: reducing learning rate of group 0 to 1.5000e-04.7073 | Test - Epoch [549/5000], Loss: 1.2510 MEE: 1.2170     \n",
      "Epoch 00636: reducing learning rate of group 0 to 7.5000e-05..5789 | Test - Epoch [636/5000], Loss: 0.9990 MEE: 1.0934 \n",
      "Epoch 00775: reducing learning rate of group 0 to 3.7500e-05..4897 | Test - Epoch [775/5000], Loss: 0.8597 MEE: 1.0159 \n",
      "Epoch 01201: reducing learning rate of group 0 to 1.8750e-05. 0.3872 | Test - Epoch [1201/5000], Loss: 0.6853 MEE: 0.9264 \n",
      "Epoch 01291: reducing learning rate of group 0 to 9.3750e-06.: 0.3676 | Test - Epoch [1291/5000], Loss: 0.6668 MEE: 0.9164 \n",
      "Epoch 01309: reducing learning rate of group 0 to 4.6875e-06.: 0.3640 | Test - Epoch [1309/5000], Loss: 0.6628 MEE: 0.9103 \n",
      "N. Epochs = 1317 - Loss (train | test/val )= (0.1916 | 0.6619 ) - MEE (train | test/val ) = (0.3634 | 0.9103 )9 MEE: 0.9103 \n",
      "Epoch 00613: reducing learning rate of group 0 to 1.5000e-04.5423 | Test - Epoch [613/5000], Loss: 0.9822 MEE: 1.0792     \n",
      "Epoch 00683: reducing learning rate of group 0 to 7.5000e-05..4703 | Test - Epoch [683/5000], Loss: 0.8553 MEE: 1.0052 \n",
      "Epoch 00845: reducing learning rate of group 0 to 3.7500e-05..4080 | Test - Epoch [845/5000], Loss: 0.7409 MEE: 0.9481 \n",
      "Epoch 00914: reducing learning rate of group 0 to 1.8750e-05.0.3931 | Test - Epoch [914/5000], Loss: 0.7262 MEE: 0.9385 \n",
      "N. Epochs = 939 - Loss (train | test/val )= (0.2389 | 0.723 ) - MEE (train | test/val ) = (0.3885 | 0.9352 ) MEE: 0.9352 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.2142 +- 0.01936 | 0.6916 +- 0.02498)- MEE (train | test/val ) = ( 0.3672 +-  0.0161 | 0.9164 +- 0.01358)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1537 +- 0.04287 | val_loss = 0.5364 +- 0.1839train_mee = 0.352 +- 0.01471 | val_mee = 0.783 +- 0.1287\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.5;lr_patience=15\n",
      "Epoch 00602: reducing learning rate of group 0 to 1.5000e-04.6871 | Test - Epoch [602/5000], Loss: 0.4735 MEE: 0.8412     \n",
      "Epoch 00860: reducing learning rate of group 0 to 7.5000e-05..4306 | Test - Epoch [860/5000], Loss: 0.3222 MEE: 0.6528 \n",
      "N. Epochs = 1017 - Loss (train | test/val )= (0.1348 | 0.2989 ) - MEE (train | test/val ) = (0.3691 | 0.6236 )EE: 0.6236 \n",
      "Epoch 00808: reducing learning rate of group 0 to 1.5000e-04.4268 | Test - Epoch [808/5000], Loss: 0.3558 MEE: 0.6814     \n",
      "Epoch 00878: reducing learning rate of group 0 to 7.5000e-05..3232 | Test - Epoch [878/5000], Loss: 0.3359 MEE: 0.6197 \n",
      "N. Epochs = 932 - Loss (train | test/val )= (0.06682 | 0.322 ) - MEE (train | test/val ) = (0.3017 | 0.5967 )E: 0.5967 \n",
      "Epoch 00583: reducing learning rate of group 0 to 1.5000e-04.8334 | Test - Epoch [583/5000], Loss: 0.5380 MEE: 0.9356     \n",
      "Epoch 01023: reducing learning rate of group 0 to 7.5000e-05.0.4078 | Test - Epoch [1023/5000], Loss: 0.2887 MEE: 0.6249 \n",
      "N. Epochs = 1027 - Loss (train | test/val )= (0.1882 | 0.2828 ) - MEE (train | test/val ) = (0.3946 | 0.6215 )EE: 0.6215 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1299 +- 0.04968 | 0.3012 +- 0.0161)- MEE (train | test/val ) = ( 0.3552 +-  0.03921 | 0.6139 +- 0.01221)\n",
      "Epoch 00791: reducing learning rate of group 0 to 1.5000e-04.3988 | Test - Epoch [791/5000], Loss: 0.6753 MEE: 0.8727     \n",
      "N. Epochs = 969 - Loss (train | test/val )= (0.1049 | 0.6026 ) - MEE (train | test/val ) = (0.3325 | 0.8236 )E: 0.8236 \n",
      "Epoch 00759: reducing learning rate of group 0 to 1.5000e-04.4175 | Test - Epoch [759/5000], Loss: 0.6514 MEE: 0.8963   5 \n",
      "Epoch 00791: reducing learning rate of group 0 to 7.5000e-05..3587 | Test - Epoch [791/5000], Loss: 0.6393 MEE: 0.8724 \n",
      "Epoch 00902: reducing learning rate of group 0 to 3.7500e-05..3209 | Test - Epoch [902/5000], Loss: 0.6144 MEE: 0.8442 \n",
      "Epoch 00920: reducing learning rate of group 0 to 1.8750e-05.0.3124 | Test - Epoch [920/5000], Loss: 0.6179 MEE: 0.8431 \n",
      "N. Epochs = 924 - Loss (train | test/val )= (0.08442 | 0.6127 ) - MEE (train | test/val ) = (0.3073 | 0.837 )MEE: 0.8370 \n",
      "N. Epochs = 780 - Loss (train | test/val )= (0.1408 | 0.7412 ) - MEE (train | test/val ) = (0.4022 | 0.9005 ): 0.9005     \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.11 +- 0.02331 | 0.6522 +- 0.06312)- MEE (train | test/val ) = ( 0.3474 +-  0.04011 | 0.8537 +- 0.03354)\n",
      "Epoch 00687: reducing learning rate of group 0 to 1.5000e-04.5539 | Test - Epoch [687/5000], Loss: 0.8243 MEE: 1.0560     \n",
      "N. Epochs = 800 - Loss (train | test/val )= (0.2413 | 0.7054 ) - MEE (train | test/val ) = (0.4061 | 0.9344 )E: 0.9344 \n",
      "Epoch 00697: reducing learning rate of group 0 to 1.5000e-04.4540 | Test - Epoch [697/5000], Loss: 0.8076 MEE: 0.9818     \n",
      "Epoch 00829: reducing learning rate of group 0 to 7.5000e-05..3816 | Test - Epoch [829/5000], Loss: 0.6900 MEE: 0.9161 \n",
      "N. Epochs = 887 - Loss (train | test/val )= (0.1927 | 0.6616 ) - MEE (train | test/val ) = (0.3564 | 0.9026 )E: 0.9026 \n",
      "N. Epochs = 735 - Loss (train | test/val )= (0.1458 | 0.556 ) - MEE (train | test/val ) = (0.4192 | 0.8962 )E: 0.8962     \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1933 +- 0.039 | 0.641 +- 0.06271)- MEE (train | test/val ) = ( 0.3939 +-  0.02704 | 0.9111 +- 0.01668)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1444 +- 0.03548 | val_loss = 0.5315 +- 0.1629train_mee = 0.3655 +- 0.02035 | val_mee = 0.7929 +- 0.1287\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.6;lr_patience=10\n",
      "Epoch 00601: reducing learning rate of group 0 to 1.8000e-04.6618 | Test - Epoch [601/5000], Loss: 0.4763 MEE: 0.8364     \n",
      "Epoch 00731: reducing learning rate of group 0 to 1.0800e-04. 0.2886, MEE: 0.4973 | Test - Epoch [731/5000], Loss: 0.3673 MEE: 0.7080 \n",
      "Epoch 00843: reducing learning rate of group 0 to 6.4800e-05. 0.2051, MEE: 0.4295 | Test - Epoch [843/5000], Loss: 0.3295 MEE: 0.6621 \n",
      "Epoch 01001: reducing learning rate of group 0 to 3.8880e-05. 0.1572, MEE: 0.3911 | Test - Epoch [1001/5000], Loss: 0.3075 MEE: 0.6393 \n",
      "Epoch 01027: reducing learning rate of group 0 to 2.3328e-05.: 0.1525, MEE: 0.3821 | Test - Epoch [1027/5000], Loss: 0.3091 MEE: 0.6321 \n",
      "N. Epochs = 1140 - Loss (train | test/val )= (0.1402 | 0.2981 ) - MEE (train | test/val ) = (0.3629 | 0.6172 ) Loss: 0.2981 MEE: 0.6172 \n",
      "Epoch 00653: reducing learning rate of group 0 to 1.8000e-04.6177 | Test - Epoch [653/5000], Loss: 0.3649 MEE: 0.7720     \n",
      "Epoch 00727: reducing learning rate of group 0 to 1.0800e-04. 0.3146, MEE: 0.4939 | Test - Epoch [727/5000], Loss: 0.3126 MEE: 0.6787 \n",
      "N. Epochs = 912 - Loss (train | test/val )= (0.2298 | 0.2789 ) - MEE (train | test/val ) = (0.4135 | 0.6487 )Loss: 0.2789 MEE: 0.6487 \n",
      "Epoch 00655: reducing learning rate of group 0 to 1.8000e-04.6400 | Test - Epoch [655/5000], Loss: 0.3840 MEE: 0.8006     \n",
      "Epoch 00816: reducing learning rate of group 0 to 1.0800e-04. 0.2173, MEE: 0.4592 | Test - Epoch [816/5000], Loss: 0.3117 MEE: 0.6926 \n",
      "Epoch 00855: reducing learning rate of group 0 to 6.4800e-05. 0.1865, MEE: 0.4097 | Test - Epoch [855/5000], Loss: 0.2898 MEE: 0.6470 \n",
      "Epoch 01053: reducing learning rate of group 0 to 3.8880e-05. 0.1125, MEE: 0.3497 | Test - Epoch [1053/5000], Loss: 0.2664 MEE: 0.6121 \n",
      "Epoch 01085: reducing learning rate of group 0 to 2.3328e-05.: 0.1075, MEE: 0.3375 | Test - Epoch [1085/5000], Loss: 0.2657 MEE: 0.6064 \n",
      "N. Epochs = 1134 - Loss (train | test/val )= (0.1031 | 0.266 ) - MEE (train | test/val ) = (0.3319 | 0.6072 ), Loss: 0.2660 MEE: 0.6072 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1577 +- 0.05321 | 0.281 +- 0.01318)- MEE (train | test/val ) = ( 0.3694 +-  0.03364 | 0.6244 +- 0.01767)\n",
      "Epoch 00553: reducing learning rate of group 0 to 1.8000e-04.7665 | Test - Epoch [553/5000], Loss: 1.2916 MEE: 1.1823     \n",
      "Epoch 00735: reducing learning rate of group 0 to 1.0800e-04. 0.2225, MEE: 0.4580 | Test - Epoch [735/5000], Loss: 0.8180 MEE: 0.9458 \n",
      "Epoch 00840: reducing learning rate of group 0 to 6.4800e-05. 0.1619, MEE: 0.4090 | Test - Epoch [840/5000], Loss: 0.7204 MEE: 0.9088 \n",
      "Epoch 00907: reducing learning rate of group 0 to 3.8880e-05.0.1416, MEE: 0.3776 | Test - Epoch [907/5000], Loss: 0.7019 MEE: 0.8899 \n",
      "Epoch 01049: reducing learning rate of group 0 to 2.3328e-05.: 0.1205, MEE: 0.3515 | Test - Epoch [1049/5000], Loss: 0.6781 MEE: 0.8716 \n",
      "N. Epochs = 1090 - Loss (train | test/val )= (0.1157 | 0.6729 ) - MEE (train | test/val ) = (0.3425 | 0.8708 ) Loss: 0.6729 MEE: 0.8708 \n",
      "Epoch 00717: reducing learning rate of group 0 to 1.8000e-04.4674 | Test - Epoch [717/5000], Loss: 0.7198 MEE: 0.9016     \n",
      "Epoch 00770: reducing learning rate of group 0 to 1.0800e-04. 0.1409, MEE: 0.3778 | Test - Epoch [770/5000], Loss: 0.6708 MEE: 0.8801 \n",
      "Epoch 00794: reducing learning rate of group 0 to 6.4800e-05. 0.1295, MEE: 0.3584 | Test - Epoch [794/5000], Loss: 0.6419 MEE: 0.8602 \n",
      "Epoch 00893: reducing learning rate of group 0 to 3.8880e-05.0.1114, MEE: 0.3340 | Test - Epoch [893/5000], Loss: 0.6237 MEE: 0.8480 \n",
      "Epoch 01063: reducing learning rate of group 0 to 2.3328e-05.: 0.0935, MEE: 0.3139 | Test - Epoch [1063/5000], Loss: 0.5900 MEE: 0.8285 \n",
      "N. Epochs = 1100 - Loss (train | test/val )= (0.09066 | 0.5849 ) - MEE (train | test/val ) = (0.3086 | 0.8235 )Loss: 0.5849 MEE: 0.8235 \n",
      "Epoch 00632: reducing learning rate of group 0 to 1.8000e-04.6486 | Test - Epoch [632/5000], Loss: 1.1660 MEE: 1.1103     \n",
      "Epoch 00797: reducing learning rate of group 0 to 1.0800e-04. 0.1867, MEE: 0.4234 | Test - Epoch [797/5000], Loss: 0.8312 MEE: 0.9471 \n",
      "Epoch 00818: reducing learning rate of group 0 to 6.4800e-05. 0.1766, MEE: 0.4114 | Test - Epoch [818/5000], Loss: 0.8336 MEE: 0.9396 \n",
      "Epoch 00877: reducing learning rate of group 0 to 3.8880e-05.0.1587, MEE: 0.3831 | Test - Epoch [877/5000], Loss: 0.8065 MEE: 0.9178 \n",
      "Epoch 01070: reducing learning rate of group 0 to 2.3328e-05.: 0.1171, MEE: 0.3457 | Test - Epoch [1070/5000], Loss: 0.7214 MEE: 0.8877 \n",
      "Epoch 01145: reducing learning rate of group 0 to 1.3997e-05.: 0.1077, MEE: 0.3360 | Test - Epoch [1145/5000], Loss: 0.7127 MEE: 0.8853 \n",
      "N. Epochs = 1169 - Loss (train | test/val )= (0.1053 | 0.702 ) - MEE (train | test/val ) = (0.3336 | 0.8822 ), Loss: 0.7020 MEE: 0.8822 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1039 +- 0.01029 | 0.6533 +- 0.04975)- MEE (train | test/val ) = ( 0.3282 +-  0.01435 | 0.8588 +- 0.0254)\n",
      "Epoch 00561: reducing learning rate of group 0 to 1.8000e-04.6677 | Test - Epoch [561/5000], Loss: 1.0307 MEE: 1.1352     \n",
      "Epoch 00668: reducing learning rate of group 0 to 1.0800e-04. 0.3015, MEE: 0.5058 | Test - Epoch [668/5000], Loss: 0.8019 MEE: 0.9954 \n",
      "Epoch 00830: reducing learning rate of group 0 to 6.4800e-05. 0.2042, MEE: 0.4171 | Test - Epoch [830/5000], Loss: 0.6546 MEE: 0.9264 \n",
      "Epoch 00930: reducing learning rate of group 0 to 3.8880e-05.0.1641, MEE: 0.3884 | Test - Epoch [930/5000], Loss: 0.6005 MEE: 0.8989 \n",
      "Epoch 01039: reducing learning rate of group 0 to 2.3328e-05.: 0.1335, MEE: 0.3658 | Test - Epoch [1039/5000], Loss: 0.5565 MEE: 0.8762 \n",
      "Epoch 01067: reducing learning rate of group 0 to 1.3997e-05.: 0.1292, MEE: 0.3560 | Test - Epoch [1067/5000], Loss: 0.5533 MEE: 0.8740 \n",
      "Epoch 01181: reducing learning rate of group 0 to 8.3981e-06.: 0.1218, MEE: 0.3455 | Test - Epoch [1181/5000], Loss: 0.5396 MEE: 0.8623 \n",
      "N. Epochs = 1205 - Loss (train | test/val )= (0.1206 | 0.5375 ) - MEE (train | test/val ) = (0.3429 | 0.8615 )Loss: 0.5375 MEE: 0.8615 \n",
      "Epoch 00563: reducing learning rate of group 0 to 1.8000e-04.6273 | Test - Epoch [563/5000], Loss: 1.1984 MEE: 1.1711     \n",
      "Epoch 00704: reducing learning rate of group 0 to 1.0800e-04. 0.3420, MEE: 0.4636 | Test - Epoch [704/5000], Loss: 0.9403 MEE: 1.0148 \n",
      "Epoch 00962: reducing learning rate of group 0 to 6.4800e-05. 0.2180, MEE: 0.3620 | Test - Epoch [962/5000], Loss: 0.7115 MEE: 0.9226 \n",
      "Epoch 01024: reducing learning rate of group 0 to 3.8880e-05. 0.2040, MEE: 0.3457 | Test - Epoch [1024/5000], Loss: 0.6828 MEE: 0.9095 \n",
      "Epoch 01049: reducing learning rate of group 0 to 2.3328e-05.: 0.2002, MEE: 0.3353 | Test - Epoch [1049/5000], Loss: 0.6707 MEE: 0.8891 \n",
      "N. Epochs = 1054 - Loss (train | test/val )= (0.199 | 0.6716 ) - MEE (train | test/val ) = (0.3305 | 0.8907 ), Loss: 0.6716 MEE: 0.8907 \n",
      "Epoch 00527: reducing learning rate of group 0 to 1.8000e-04.7458 | Test - Epoch [527/5000], Loss: 1.3601 MEE: 1.2345     \n",
      "Epoch 00769: reducing learning rate of group 0 to 1.0800e-04. 0.3181, MEE: 0.4921 | Test - Epoch [769/5000], Loss: 0.8659 MEE: 0.9940 \n",
      "Epoch 00918: reducing learning rate of group 0 to 6.4800e-05. 0.2438, MEE: 0.3842 | Test - Epoch [918/5000], Loss: 0.7404 MEE: 0.9146 \n",
      "Epoch 00956: reducing learning rate of group 0 to 3.8880e-05.0.2306, MEE: 0.3691 | Test - Epoch [956/5000], Loss: 0.7159 MEE: 0.8983 \n",
      "Epoch 01032: reducing learning rate of group 0 to 2.3328e-05.: 0.2093, MEE: 0.3549 | Test - Epoch [1032/5000], Loss: 0.6831 MEE: 0.8868 \n",
      "Epoch 01064: reducing learning rate of group 0 to 1.3997e-05.: 0.2037, MEE: 0.3503 | Test - Epoch [1064/5000], Loss: 0.6740 MEE: 0.8833 \n",
      "N. Epochs = 1134 - Loss (train | test/val )= (0.1985 | 0.6639 ) - MEE (train | test/val ) = (0.3448 | 0.8801 ) Loss: 0.6639 MEE: 0.8801 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1727 +- 0.03686 | 0.6243 +- 0.06147)- MEE (train | test/val ) = ( 0.3394 +-  0.006328 | 0.8775 +- 0.01207)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1448 +- 0.02954 | val_loss = 0.5195 +- 0.1691train_mee = 0.3457 +- 0.01739 | val_mee = 0.7869 +- 0.1152\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.6;lr_patience=15\n",
      "Epoch 00659: reducing learning rate of group 0 to 1.8000e-04.6050 | Test - Epoch [659/5000], Loss: 0.4546 MEE: 0.7554     \n",
      "Epoch 00915: reducing learning rate of group 0 to 1.0800e-04. 0.2246, MEE: 0.4045 | Test - Epoch [915/5000], Loss: 0.3756 MEE: 0.6223 \n",
      "N. Epochs = 919 - Loss (train | test/val )= (0.2178 | 0.3774 ) - MEE (train | test/val ) = (0.3878 | 0.6165 )Loss: 0.3774 MEE: 0.6165 \n",
      "Epoch 00751: reducing learning rate of group 0 to 1.8000e-04.4397 | Test - Epoch [751/5000], Loss: 0.2837 MEE: 0.6423     \n",
      "N. Epochs = 816 - Loss (train | test/val )= (0.1238 | 0.2828 ) - MEE (train | test/val ) = (0.377 | 0.6107 ) Loss: 0.2828 MEE: 0.6107 \n",
      "Epoch 00662: reducing learning rate of group 0 to 1.8000e-04.5881 | Test - Epoch [662/5000], Loss: 0.4073 MEE: 0.7704     \n",
      "Epoch 00924: reducing learning rate of group 0 to 1.0800e-04. 0.1284, MEE: 0.3866 | Test - Epoch [924/5000], Loss: 0.2942 MEE: 0.6242 \n",
      "Epoch 00964: reducing learning rate of group 0 to 6.4800e-05. 0.1065, MEE: 0.3400 | Test - Epoch [964/5000], Loss: 0.2921 MEE: 0.6104 \n",
      "N. Epochs = 968 - Loss (train | test/val )= (0.1056 | 0.2896 ) - MEE (train | test/val ) = (0.3387 | 0.614 )Loss: 0.2896 MEE: 0.6140 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1491 +- 0.04915 | 0.3166 +- 0.04309)- MEE (train | test/val ) = ( 0.3678 +-  0.02108 | 0.6137 +- 0.002371)\n",
      "Epoch 00684: reducing learning rate of group 0 to 1.8000e-04.4750 | Test - Epoch [684/5000], Loss: 0.7429 MEE: 0.9145     \n",
      "Epoch 00848: reducing learning rate of group 0 to 1.0800e-04. 0.1113, MEE: 0.3656 | Test - Epoch [848/5000], Loss: 0.6268 MEE: 0.8442 \n",
      "N. Epochs = 852 - Loss (train | test/val )= (0.1058 | 0.6195 ) - MEE (train | test/val ) = (0.3534 | 0.8361 )Loss: 0.6195 MEE: 0.8361 \n",
      "Epoch 00668: reducing learning rate of group 0 to 1.8000e-04.5106 | Test - Epoch [668/5000], Loss: 0.8168 MEE: 0.9921     \n",
      "Epoch 00881: reducing learning rate of group 0 to 1.0800e-04. 0.0967, MEE: 0.3415 | Test - Epoch [881/5000], Loss: 0.6519 MEE: 0.8744 \n",
      "Epoch 00953: reducing learning rate of group 0 to 6.4800e-05. 0.0826, MEE: 0.3220 | Test - Epoch [953/5000], Loss: 0.6431 MEE: 0.8742 \n",
      "N. Epochs = 1085 - Loss (train | test/val )= (0.07056 | 0.6406 ) - MEE (train | test/val ) = (0.3031 | 0.8615 )oss: 0.6406 MEE: 0.8615 \n",
      "Epoch 00748: reducing learning rate of group 0 to 1.8000e-04.4353 | Test - Epoch [748/5000], Loss: 0.5917 MEE: 0.8654   7 \n",
      "Epoch 00798: reducing learning rate of group 0 to 1.0800e-04. 0.1017, MEE: 0.3507 | Test - Epoch [798/5000], Loss: 0.5494 MEE: 0.8295 \n",
      "N. Epochs = 802 - Loss (train | test/val )= (0.1004 | 0.5474 ) - MEE (train | test/val ) = (0.3439 | 0.8207 )Loss: 0.5474 MEE: 0.8207 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.09223 +- 0.01549 | 0.6025 +- 0.0399)- MEE (train | test/val ) = ( 0.3335 +-  0.02183 | 0.8394 +- 0.01686)\n",
      "Epoch 00624: reducing learning rate of group 0 to 1.8000e-04.5398 | Test - Epoch [624/5000], Loss: 0.8434 MEE: 1.0213     \n",
      "Epoch 00859: reducing learning rate of group 0 to 1.0800e-04. 0.1708, MEE: 0.3941 | Test - Epoch [859/5000], Loss: 0.5724 MEE: 0.9047 \n",
      "Epoch 01018: reducing learning rate of group 0 to 6.4800e-05.: 0.1174, MEE: 0.3095 | Test - Epoch [1018/5000], Loss: 0.4844 MEE: 0.8221 \n",
      "N. Epochs = 1054 - Loss (train | test/val )= (0.1128 | 0.4738 ) - MEE (train | test/val ) = (0.3057 | 0.8135 )Loss: 0.4738 MEE: 0.8135 \n",
      "Epoch 00639: reducing learning rate of group 0 to 1.8000e-04.5263 | Test - Epoch [639/5000], Loss: 0.7794 MEE: 0.9957     \n",
      "Epoch 00741: reducing learning rate of group 0 to 1.0800e-04. 0.1789, MEE: 0.4106 | Test - Epoch [741/5000], Loss: 0.6472 MEE: 0.9298 \n",
      "N. Epochs = 846 - Loss (train | test/val )= (0.1469 | 0.5879 ) - MEE (train | test/val ) = (0.3556 | 0.8759 )Loss: 0.5879 MEE: 0.8759 \n",
      "Epoch 00678: reducing learning rate of group 0 to 1.8000e-04.4826 | Test - Epoch [678/5000], Loss: 0.8479 MEE: 1.0082     \n",
      "Epoch 00744: reducing learning rate of group 0 to 1.0800e-04. 0.2280, MEE: 0.4093 | Test - Epoch [744/5000], Loss: 0.7601 MEE: 0.9419 \n",
      "Epoch 01060: reducing learning rate of group 0 to 6.4800e-05.: 0.1006, MEE: 0.3193 | Test - Epoch [1060/5000], Loss: 0.5408 MEE: 0.8402 \n",
      "N. Epochs = 1104 - Loss (train | test/val )= (0.08985 | 0.5212 ) - MEE (train | test/val ) = (0.3027 | 0.8329 )oss: 0.5212 MEE: 0.8329 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1165 +- 0.02343 | 0.5276 +- 0.04678)- MEE (train | test/val ) = ( 0.3213 +-  0.02428 | 0.8408 +- 0.02607)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1193 +- 0.02329 | val_loss = 0.4823 +- 0.121train_mee = 0.3409 +- 0.01969 | val_mee = 0.7646 +- 0.1067\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.7;lr_patience=10\n",
      "Epoch 00611: reducing learning rate of group 0 to 2.1000e-04.6913 | Test - Epoch [611/5000], Loss: 0.4166 MEE: 0.7831     \n",
      "Epoch 00724: reducing learning rate of group 0 to 1.4700e-04. 0.4399, MEE: 0.5541 | Test - Epoch [724/5000], Loss: 0.3412 MEE: 0.7203 \n",
      "Epoch 00782: reducing learning rate of group 0 to 1.0290e-04. 0.3584, MEE: 0.4988 | Test - Epoch [782/5000], Loss: 0.3252 MEE: 0.6835 \n",
      "Epoch 00812: reducing learning rate of group 0 to 7.2030e-05. 0.3415, MEE: 0.4760 | Test - Epoch [812/5000], Loss: 0.3031 MEE: 0.6575 \n",
      "Epoch 00885: reducing learning rate of group 0 to 5.0421e-05.0.3173, MEE: 0.4544 | Test - Epoch [885/5000], Loss: 0.2883 MEE: 0.6324 \n",
      "N. Epochs = 974 - Loss (train | test/val )= (0.2964 | 0.2897 ) - MEE (train | test/val ) = (0.4427 | 0.6404 )oss: 0.2897 MEE: 0.6404 \n",
      "Epoch 00614: reducing learning rate of group 0 to 2.1000e-04.6131 | Test - Epoch [614/5000], Loss: 0.3882 MEE: 0.7861     \n",
      "Epoch 00842: reducing learning rate of group 0 to 1.4700e-04. 0.1180, MEE: 0.4108 | Test - Epoch [842/5000], Loss: 0.2641 MEE: 0.6558 \n",
      "N. Epochs = 851 - Loss (train | test/val )= (0.1051 | 0.2639 ) - MEE (train | test/val ) = (0.3674 | 0.6228 )Loss: 0.2639 MEE: 0.6228 \n",
      "Epoch 00637: reducing learning rate of group 0 to 2.1000e-04.6095 | Test - Epoch [637/5000], Loss: 0.4441 MEE: 0.7712     \n",
      "Epoch 00700: reducing learning rate of group 0 to 1.4700e-04. 0.3922, MEE: 0.5308 | Test - Epoch [700/5000], Loss: 0.4144 MEE: 0.7300 \n",
      "Epoch 00801: reducing learning rate of group 0 to 1.0290e-04. 0.2705, MEE: 0.4542 | Test - Epoch [801/5000], Loss: 0.3676 MEE: 0.6772 \n",
      "Epoch 00851: reducing learning rate of group 0 to 7.2030e-05. 0.2358, MEE: 0.4222 | Test - Epoch [851/5000], Loss: 0.3560 MEE: 0.6626 \n",
      "Epoch 00884: reducing learning rate of group 0 to 5.0421e-05.0.2254, MEE: 0.4078 | Test - Epoch [884/5000], Loss: 0.3585 MEE: 0.6590 \n",
      "N. Epochs = 970 - Loss (train | test/val )= (0.1967 | 0.3555 ) - MEE (train | test/val ) = (0.388 | 0.6551 )Loss: 0.3555 MEE: 0.6551 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1994 +- 0.07812 | 0.303 +- 0.03855)- MEE (train | test/val ) = ( 0.3994 +-  0.03177 | 0.6394 +- 0.01322)\n",
      "Epoch 00692: reducing learning rate of group 0 to 2.1000e-04.5169 | Test - Epoch [692/5000], Loss: 0.7406 MEE: 0.9387     \n",
      "Epoch 00740: reducing learning rate of group 0 to 1.4700e-04. 0.1745, MEE: 0.4263 | Test - Epoch [740/5000], Loss: 0.6781 MEE: 0.8669 \n",
      "Epoch 00811: reducing learning rate of group 0 to 1.0290e-04. 0.1347, MEE: 0.3781 | Test - Epoch [811/5000], Loss: 0.6302 MEE: 0.8417 \n",
      "Epoch 00832: reducing learning rate of group 0 to 7.2030e-05. 0.1268, MEE: 0.3573 | Test - Epoch [832/5000], Loss: 0.6307 MEE: 0.8368 \n",
      "Epoch 00938: reducing learning rate of group 0 to 5.0421e-05.0.1057, MEE: 0.3355 | Test - Epoch [938/5000], Loss: 0.6031 MEE: 0.8301 \n",
      "Epoch 00966: reducing learning rate of group 0 to 3.5295e-05.0.1014, MEE: 0.3257 | Test - Epoch [966/5000], Loss: 0.6016 MEE: 0.8220 \n",
      "N. Epochs = 1019 - Loss (train | test/val )= (0.09727 | 0.5973 ) - MEE (train | test/val ) = (0.319 | 0.8146 )Loss: 0.5973 MEE: 0.8146 \n",
      "Epoch 00592: reducing learning rate of group 0 to 2.1000e-04.6602 | Test - Epoch [592/5000], Loss: 1.1341 MEE: 1.1255     \n",
      "Epoch 00678: reducing learning rate of group 0 to 1.4700e-04. 0.2672, MEE: 0.5244 | Test - Epoch [678/5000], Loss: 0.8740 MEE: 0.9844 \n",
      "Epoch 00766: reducing learning rate of group 0 to 1.0290e-04. 0.1744, MEE: 0.4396 | Test - Epoch [766/5000], Loss: 0.7270 MEE: 0.9362 \n",
      "Epoch 00804: reducing learning rate of group 0 to 7.2030e-05. 0.1526, MEE: 0.3915 | Test - Epoch [804/5000], Loss: 0.6994 MEE: 0.8954 \n",
      "Epoch 00875: reducing learning rate of group 0 to 5.0421e-05.0.1339, MEE: 0.3756 | Test - Epoch [875/5000], Loss: 0.6760 MEE: 0.8825 \n",
      "Epoch 00971: reducing learning rate of group 0 to 3.5295e-05.0.1129, MEE: 0.3510 | Test - Epoch [971/5000], Loss: 0.6415 MEE: 0.8677 \n",
      "Epoch 01161: reducing learning rate of group 0 to 2.4706e-05. 0.0879, MEE: 0.3242 | Test - Epoch [1161/5000], Loss: 0.6073 MEE: 0.8520 \n",
      "Epoch 01210: reducing learning rate of group 0 to 1.7294e-05.: 0.0826, MEE: 0.3133 | Test - Epoch [1210/5000], Loss: 0.5978 MEE: 0.8449 \n",
      "Epoch 01246: reducing learning rate of group 0 to 1.2106e-05.: 0.0806, MEE: 0.3098 | Test - Epoch [1246/5000], Loss: 0.5955 MEE: 0.8464 \n",
      "Epoch 01276: reducing learning rate of group 0 to 8.4743e-06. 0.0789, MEE: 0.3076 | Test - Epoch [1276/5000], Loss: 0.5918 MEE: 0.8415 \n",
      "N. Epochs = 1280 - Loss (train | test/val )= (0.07874 | 0.5929 ) - MEE (train | test/val ) = (0.3069 | 0.8401 )oss: 0.5929 MEE: 0.8401 \n",
      "Epoch 00571: reducing learning rate of group 0 to 2.1000e-04.7369 | Test - Epoch [571/5000], Loss: 1.4062 MEE: 1.2055     \n",
      "Epoch 00741: reducing learning rate of group 0 to 1.4700e-04. 0.3001, MEE: 0.5376 | Test - Epoch [741/5000], Loss: 1.0320 MEE: 1.0088 \n",
      "Epoch 00862: reducing learning rate of group 0 to 1.0290e-04. 0.1460, MEE: 0.4015 | Test - Epoch [862/5000], Loss: 0.7554 MEE: 0.9007 \n",
      "Epoch 00995: reducing learning rate of group 0 to 7.2030e-05. 0.0991, MEE: 0.3348 | Test - Epoch [995/5000], Loss: 0.7016 MEE: 0.8584 \n",
      "Epoch 01034: reducing learning rate of group 0 to 5.0421e-05. 0.0909, MEE: 0.3262 | Test - Epoch [1034/5000], Loss: 0.6843 MEE: 0.8532 \n",
      "Epoch 01115: reducing learning rate of group 0 to 3.5295e-05. 0.0821, MEE: 0.3159 | Test - Epoch [1115/5000], Loss: 0.6684 MEE: 0.8423 \n",
      "Epoch 01200: reducing learning rate of group 0 to 2.4706e-05. 0.0749, MEE: 0.2968 | Test - Epoch [1200/5000], Loss: 0.6576 MEE: 0.8317 \n",
      "N. Epochs = 1224 - Loss (train | test/val )= (0.07356 | 0.6592 ) - MEE (train | test/val ) = (0.2953 | 0.8334 )Loss: 0.6592 MEE: 0.8334 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.08319 +- 0.01018 | 0.6165 +- 0.03027)- MEE (train | test/val ) = ( 0.3071 +-  0.009694 | 0.8293 +- 0.01078)\n",
      "Epoch 00569: reducing learning rate of group 0 to 2.1000e-04.6725 | Test - Epoch [569/5000], Loss: 1.0268 MEE: 1.1762     \n",
      "Epoch 00705: reducing learning rate of group 0 to 1.4700e-04. 0.2328, MEE: 0.4601 | Test - Epoch [705/5000], Loss: 0.7388 MEE: 0.9768 \n",
      "Epoch 00730: reducing learning rate of group 0 to 1.0290e-04. 0.2135, MEE: 0.4234 | Test - Epoch [730/5000], Loss: 0.7120 MEE: 0.9632 \n",
      "Epoch 00781: reducing learning rate of group 0 to 7.2030e-05. 0.1944, MEE: 0.4018 | Test - Epoch [781/5000], Loss: 0.6826 MEE: 0.9416 \n",
      "Epoch 00854: reducing learning rate of group 0 to 5.0421e-05.0.1695, MEE: 0.3796 | Test - Epoch [854/5000], Loss: 0.6401 MEE: 0.9207 \n",
      "Epoch 00956: reducing learning rate of group 0 to 3.5295e-05.0.1494, MEE: 0.3644 | Test - Epoch [956/5000], Loss: 0.6011 MEE: 0.9014 \n",
      "Epoch 01107: reducing learning rate of group 0 to 2.4706e-05. 0.1186, MEE: 0.3376 | Test - Epoch [1107/5000], Loss: 0.5504 MEE: 0.8689 \n",
      "Epoch 01337: reducing learning rate of group 0 to 1.7294e-05.: 0.0932, MEE: 0.3103 | Test - Epoch [1337/5000], Loss: 0.5017 MEE: 0.8421 \n",
      "N. Epochs = 1375 - Loss (train | test/val )= (0.08995 | 0.4926 ) - MEE (train | test/val ) = (0.3072 | 0.8314 )Loss: 0.4926 MEE: 0.8314 \n",
      "Epoch 00621: reducing learning rate of group 0 to 2.1000e-04.5597 | Test - Epoch [621/5000], Loss: 0.9384 MEE: 1.0698     \n",
      "Epoch 00695: reducing learning rate of group 0 to 1.4700e-04. 0.2716, MEE: 0.4689 | Test - Epoch [695/5000], Loss: 0.7858 MEE: 0.9952 \n",
      "Epoch 00784: reducing learning rate of group 0 to 1.0290e-04. 0.1927, MEE: 0.3929 | Test - Epoch [784/5000], Loss: 0.6526 MEE: 0.9128 \n",
      "Epoch 00821: reducing learning rate of group 0 to 7.2030e-05. 0.1719, MEE: 0.3742 | Test - Epoch [821/5000], Loss: 0.6197 MEE: 0.8935 \n",
      "Epoch 00870: reducing learning rate of group 0 to 5.0421e-05.0.1519, MEE: 0.3631 | Test - Epoch [870/5000], Loss: 0.5863 MEE: 0.8825 \n",
      "Epoch 00964: reducing learning rate of group 0 to 3.5295e-05.0.1243, MEE: 0.3381 | Test - Epoch [964/5000], Loss: 0.5457 MEE: 0.8608 \n",
      "Epoch 01044: reducing learning rate of group 0 to 2.4706e-05. 0.1153, MEE: 0.3257 | Test - Epoch [1044/5000], Loss: 0.5258 MEE: 0.8461 \n",
      "Epoch 01084: reducing learning rate of group 0 to 1.7294e-05.: 0.1120, MEE: 0.3218 | Test - Epoch [1084/5000], Loss: 0.5191 MEE: 0.8432 \n",
      "N. Epochs = 1114 - Loss (train | test/val )= (0.1097 | 0.5173 ) - MEE (train | test/val ) = (0.3176 | 0.8403 ) Loss: 0.5173 MEE: 0.8403 \n",
      "Epoch 00626: reducing learning rate of group 0 to 2.1000e-04.5438 | Test - Epoch [626/5000], Loss: 0.8466 MEE: 1.0280     \n",
      "Epoch 00644: reducing learning rate of group 0 to 1.4700e-04. 0.3121, MEE: 0.4999 | Test - Epoch [644/5000], Loss: 0.8173 MEE: 1.0078 \n",
      "Epoch 00675: reducing learning rate of group 0 to 1.0290e-04. 0.2754, MEE: 0.4578 | Test - Epoch [675/5000], Loss: 0.7549 MEE: 0.9642 \n",
      "Epoch 00835: reducing learning rate of group 0 to 7.2030e-05. 0.2130, MEE: 0.3934 | Test - Epoch [835/5000], Loss: 0.6515 MEE: 0.9116 \n",
      "Epoch 00906: reducing learning rate of group 0 to 5.0421e-05.0.1923, MEE: 0.3759 | Test - Epoch [906/5000], Loss: 0.6211 MEE: 0.8957 \n",
      "Epoch 00939: reducing learning rate of group 0 to 3.5295e-05.0.1859, MEE: 0.3682 | Test - Epoch [939/5000], Loss: 0.6088 MEE: 0.8887 \n",
      "Epoch 00969: reducing learning rate of group 0 to 2.4706e-05.0.1820, MEE: 0.3628 | Test - Epoch [969/5000], Loss: 0.6050 MEE: 0.8836 \n",
      "Epoch 00995: reducing learning rate of group 0 to 1.7294e-05. 0.1795, MEE: 0.3602 | Test - Epoch [995/5000], Loss: 0.6011 MEE: 0.8801 \n",
      "Epoch 01033: reducing learning rate of group 0 to 1.2106e-05.: 0.1768, MEE: 0.3548 | Test - Epoch [1033/5000], Loss: 0.5976 MEE: 0.8798 \n",
      "N. Epochs = 1067 - Loss (train | test/val )= (0.1752 | 0.5944 ) - MEE (train | test/val ) = (0.3514 | 0.8786 )Loss: 0.5944 MEE: 0.8786 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.125 +- 0.03645 | 0.5347 +- 0.04335)- MEE (train | test/val ) = ( 0.3254 +-  0.01886 | 0.8501 +- 0.02047)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1358 +- 0.04806 | val_loss = 0.4847 +- 0.1328train_mee = 0.344 +- 0.03991 | val_mee = 0.773 +- 0.0948\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.7;lr_patience=15\n",
      "Epoch 00749: reducing learning rate of group 0 to 2.1000e-04.5022 | Test - Epoch [749/5000], Loss: 0.2748 MEE: 0.6813     \n",
      "Epoch 00830: reducing learning rate of group 0 to 1.4700e-04. 0.1432, MEE: 0.3978 | Test - Epoch [830/5000], Loss: 0.2410 MEE: 0.6153 \n",
      "N. Epochs = 834 - Loss (train | test/val )= (0.1384 | 0.2411 ) - MEE (train | test/val ) = (0.3854 | 0.6084 )Loss: 0.2411 MEE: 0.6084 \n",
      "Epoch 00740: reducing learning rate of group 0 to 2.1000e-04.4678 | Test - Epoch [740/5000], Loss: 0.2879 MEE: 0.6747     \n",
      "Epoch 00883: reducing learning rate of group 0 to 1.4700e-04. 0.1115, MEE: 0.3871 | Test - Epoch [883/5000], Loss: 0.2681 MEE: 0.6360 \n",
      "Epoch 00909: reducing learning rate of group 0 to 1.0290e-04. 0.0942, MEE: 0.3327 | Test - Epoch [909/5000], Loss: 0.2528 MEE: 0.6031 \n",
      "Epoch 00971: reducing learning rate of group 0 to 7.2030e-05. 0.0837, MEE: 0.3146 | Test - Epoch [971/5000], Loss: 0.2471 MEE: 0.5884 \n",
      "Epoch 01032: reducing learning rate of group 0 to 5.0421e-05. 0.0787, MEE: 0.3145 | Test - Epoch [1032/5000], Loss: 0.2485 MEE: 0.5898 \n",
      "N. Epochs = 1055 - Loss (train | test/val )= (0.07403 | 0.2434 ) - MEE (train | test/val ) = (0.2971 | 0.5803 )oss: 0.2434 MEE: 0.5803 \n",
      "Epoch 00716: reducing learning rate of group 0 to 2.1000e-04.5335 | Test - Epoch [716/5000], Loss: 0.3368 MEE: 0.7128     \n",
      "N. Epochs = 847 - Loss (train | test/val )= (0.1585 | 0.2976 ) - MEE (train | test/val ) = (0.3979 | 0.6198 )Loss: 0.2976 MEE: 0.6198 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1236 +- 0.03603 | 0.2607 +- 0.0261)- MEE (train | test/val ) = ( 0.3601 +-  0.04487 | 0.6028 +- 0.01659)\n",
      "Epoch 00720: reducing learning rate of group 0 to 2.1000e-04.4718 | Test - Epoch [720/5000], Loss: 0.7136 MEE: 0.9043     \n",
      "Epoch 00759: reducing learning rate of group 0 to 1.4700e-04. 0.1642, MEE: 0.4097 | Test - Epoch [759/5000], Loss: 0.6821 MEE: 0.8799 \n",
      "N. Epochs = 786 - Loss (train | test/val )= (0.1487 | 0.6704 ) - MEE (train | test/val ) = (0.3714 | 0.8513 )Loss: 0.6704 MEE: 0.8513 \n",
      "Epoch 00747: reducing learning rate of group 0 to 2.1000e-04.4497 | Test - Epoch [747/5000], Loss: 0.6878 MEE: 0.9257     \n",
      "Epoch 00804: reducing learning rate of group 0 to 1.4700e-04. 0.1053, MEE: 0.3581 | Test - Epoch [804/5000], Loss: 0.6366 MEE: 0.8628 \n",
      "Epoch 00830: reducing learning rate of group 0 to 1.0290e-04. 0.0924, MEE: 0.3257 | Test - Epoch [830/5000], Loss: 0.6259 MEE: 0.8498 \n",
      "N. Epochs = 834 - Loss (train | test/val )= (0.09132 | 0.6174 ) - MEE (train | test/val ) = (0.3253 | 0.8304 )oss: 0.6174 MEE: 0.8304 \n",
      "Epoch 00570: reducing learning rate of group 0 to 2.1000e-04.7293 | Test - Epoch [570/5000], Loss: 1.2588 MEE: 1.1861     \n",
      "Epoch 00745: reducing learning rate of group 0 to 1.4700e-04. 0.1832, MEE: 0.4380 | Test - Epoch [745/5000], Loss: 0.7103 MEE: 0.9042 \n",
      "Epoch 00957: reducing learning rate of group 0 to 1.0290e-04. 0.0874, MEE: 0.3263 | Test - Epoch [957/5000], Loss: 0.5646 MEE: 0.8270 \n",
      "N. Epochs = 961 - Loss (train | test/val )= (0.08633 | 0.5594 ) - MEE (train | test/val ) = (0.3274 | 0.8182 )oss: 0.5594 MEE: 0.8182 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1088 +- 0.02829 | 0.6157 +- 0.04533)- MEE (train | test/val ) = ( 0.3414 +-  0.02124 | 0.8333 +- 0.01367)\n",
      "Epoch 00718: reducing learning rate of group 0 to 2.1000e-04.4726 | Test - Epoch [718/5000], Loss: 0.8126 MEE: 0.9950     \n",
      "Epoch 00842: reducing learning rate of group 0 to 1.4700e-04. 0.2201, MEE: 0.3682 | Test - Epoch [842/5000], Loss: 0.6989 MEE: 0.9086 \n",
      "Epoch 00863: reducing learning rate of group 0 to 1.0290e-04. 0.2059, MEE: 0.3572 | Test - Epoch [863/5000], Loss: 0.6710 MEE: 0.8893 \n",
      "Epoch 00927: reducing learning rate of group 0 to 7.2030e-05. 0.1918, MEE: 0.3390 | Test - Epoch [927/5000], Loss: 0.6489 MEE: 0.8779 \n",
      "Epoch 00963: reducing learning rate of group 0 to 5.0421e-05.0.1850, MEE: 0.3180 | Test - Epoch [963/5000], Loss: 0.6392 MEE: 0.8666 \n",
      "Epoch 01012: reducing learning rate of group 0 to 3.5295e-05. 0.1804, MEE: 0.3114 | Test - Epoch [1012/5000], Loss: 0.6302 MEE: 0.8616 \n",
      "N. Epochs = 1016 - Loss (train | test/val )= (0.1803 | 0.6305 ) - MEE (train | test/val ) = (0.3115 | 0.8621 )Loss: 0.6305 MEE: 0.8621 \n",
      "Epoch 00583: reducing learning rate of group 0 to 2.1000e-04.6354 | Test - Epoch [583/5000], Loss: 1.1310 MEE: 1.1352     \n",
      "Epoch 00834: reducing learning rate of group 0 to 1.4700e-04. 0.2266, MEE: 0.3914 | Test - Epoch [834/5000], Loss: 0.6884 MEE: 0.9107 \n",
      "N. Epochs = 838 - Loss (train | test/val )= (0.2211 | 0.6838 ) - MEE (train | test/val ) = (0.3775 | 0.9038 )Loss: 0.6838 MEE: 0.9038 \n",
      "N. Epochs = 661 - Loss (train | test/val )= (0.3317 | 0.9021 ) - MEE (train | test/val ) = (0.505 | 1.037 )EE: 1.0366     \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.2444 +- 0.06396 | 0.7388 +- 0.1175)- MEE (train | test/val ) = ( 0.398 +-  0.0803 | 0.9342 +- 0.07439)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1589 +- 0.06071 | val_loss = 0.5384 +- 0.2027train_mee = 0.3665 +- 0.02356 | val_mee = 0.7901 +- 0.1387\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.8;lr_patience=10\n",
      "Epoch 00623: reducing learning rate of group 0 to 2.4000e-04.6495 | Test - Epoch [623/5000], Loss: 0.4075 MEE: 0.7727     \n",
      "Epoch 00788: reducing learning rate of group 0 to 1.9200e-04. 0.2508, MEE: 0.4647 | Test - Epoch [788/5000], Loss: 0.3207 MEE: 0.6859 \n",
      "Epoch 00855: reducing learning rate of group 0 to 1.5360e-04.0.4007 | Test - Epoch [855/5000], Loss: 0.3082 MEE: 0.6423 \n",
      "Epoch 00912: reducing learning rate of group 0 to 1.2288e-04. 0.1222, MEE: 0.3657 | Test - Epoch [912/5000], Loss: 0.3088 MEE: 0.6301 \n",
      "Epoch 00975: reducing learning rate of group 0 to 9.8304e-05. 0.0961, MEE: 0.3398 | Test - Epoch [975/5000], Loss: 0.3143 MEE: 0.6246 \n",
      "Epoch 01044: reducing learning rate of group 0 to 7.8643e-05. 0.0833, MEE: 0.3262 | Test - Epoch [1044/5000], Loss: 0.3128 MEE: 0.6222 \n",
      "Epoch 01068: reducing learning rate of group 0 to 6.2915e-05. 0.0779, MEE: 0.3069 | Test - Epoch [1068/5000], Loss: 0.3068 MEE: 0.6069 \n",
      "Epoch 01115: reducing learning rate of group 0 to 5.0332e-05. 0.0729, MEE: 0.2960 | Test - Epoch [1115/5000], Loss: 0.3051 MEE: 0.6053 \n",
      "Epoch 01135: reducing learning rate of group 0 to 4.0265e-05.: 0.0707, MEE: 0.2918 | Test - Epoch [1135/5000], Loss: 0.3069 MEE: 0.6019 \n",
      "N. Epochs = 1141 - Loss (train | test/val )= (0.07018 | 0.301 ) - MEE (train | test/val ) = (0.2902 | 0.5943 )Loss: 0.3010 MEE: 0.5943 \n",
      "Epoch 00649: reducing learning rate of group 0 to 2.4000e-04.6135 | Test - Epoch [649/5000], Loss: 0.3485 MEE: 0.7379     \n",
      "Epoch 00763: reducing learning rate of group 0 to 1.9200e-04. 0.1805, MEE: 0.4351 | Test - Epoch [763/5000], Loss: 0.3104 MEE: 0.6582 \n",
      "Epoch 00814: reducing learning rate of group 0 to 1.5360e-04.0.3869 | Test - Epoch [814/5000], Loss: 0.2770 MEE: 0.6176 \n",
      "Epoch 00886: reducing learning rate of group 0 to 1.2288e-04. 0.1130, MEE: 0.3605 | Test - Epoch [886/5000], Loss: 0.2793 MEE: 0.6103 \n",
      "Epoch 00904: reducing learning rate of group 0 to 9.8304e-05. 0.1076, MEE: 0.3522 | Test - Epoch [904/5000], Loss: 0.2820 MEE: 0.6137 \n",
      "Epoch 00964: reducing learning rate of group 0 to 7.8643e-05.0.0943, MEE: 0.3439 | Test - Epoch [964/5000], Loss: 0.2862 MEE: 0.6185 \n",
      "Epoch 00980: reducing learning rate of group 0 to 6.2915e-05.0.0894, MEE: 0.3316 | Test - Epoch [980/5000], Loss: 0.2857 MEE: 0.6066 \n",
      "N. Epochs = 1006 - Loss (train | test/val )= (0.0848 | 0.2849 ) - MEE (train | test/val ) = (0.3185 | 0.5945 )Loss: 0.2849 MEE: 0.5945 \n",
      "Epoch 00659: reducing learning rate of group 0 to 2.4000e-04.6072 | Test - Epoch [659/5000], Loss: 0.4068 MEE: 0.7626     \n",
      "Epoch 00709: reducing learning rate of group 0 to 1.9200e-04. 0.4826, MEE: 0.5321 | Test - Epoch [709/5000], Loss: 0.3642 MEE: 0.7114 \n",
      "Epoch 00843: reducing learning rate of group 0 to 1.5360e-04.0.4269 | Test - Epoch [843/5000], Loss: 0.3102 MEE: 0.6589 \n",
      "Epoch 00936: reducing learning rate of group 0 to 1.2288e-04. 0.1822, MEE: 0.3662 | Test - Epoch [936/5000], Loss: 0.2962 MEE: 0.6285 \n",
      "Epoch 00984: reducing learning rate of group 0 to 9.8304e-05. 0.1667, MEE: 0.3522 | Test - Epoch [984/5000], Loss: 0.2930 MEE: 0.6240 \n",
      "Epoch 01004: reducing learning rate of group 0 to 7.8643e-05. 0.1535, MEE: 0.3506 | Test - Epoch [1004/5000], Loss: 0.2998 MEE: 0.6360 \n",
      "Epoch 01052: reducing learning rate of group 0 to 6.2915e-05. 0.1277, MEE: 0.3265 | Test - Epoch [1052/5000], Loss: 0.2832 MEE: 0.6050 \n",
      "Epoch 01110: reducing learning rate of group 0 to 5.0332e-05. 0.1094, MEE: 0.3133 | Test - Epoch [1110/5000], Loss: 0.2839 MEE: 0.5990 \n",
      "Epoch 01140: reducing learning rate of group 0 to 4.0265e-05.: 0.1057, MEE: 0.3105 | Test - Epoch [1140/5000], Loss: 0.2814 MEE: 0.5957 \n",
      "Epoch 01158: reducing learning rate of group 0 to 3.2212e-05. 0.1040, MEE: 0.3069 | Test - Epoch [1158/5000], Loss: 0.2815 MEE: 0.5965 \n",
      "Epoch 01188: reducing learning rate of group 0 to 2.5770e-05. 0.1011, MEE: 0.3012 | Test - Epoch [1188/5000], Loss: 0.2765 MEE: 0.5906 \n",
      "Epoch 01247: reducing learning rate of group 0 to 2.0616e-05.: 0.0973, MEE: 0.2950 | Test - Epoch [1247/5000], Loss: 0.2778 MEE: 0.5883 \n",
      "N. Epochs = 1249 - Loss (train | test/val )= (0.09716 | 0.2792 ) - MEE (train | test/val ) = (0.2949 | 0.5919 )Loss: 0.2792 MEE: 0.5919 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.08405 +- 0.01103 | 0.2883 +- 0.009219)- MEE (train | test/val ) = ( 0.3012 +-  0.0124 | 0.5936 +- 0.001148)\n",
      "Epoch 00653: reducing learning rate of group 0 to 2.4000e-04.5278 | Test - Epoch [653/5000], Loss: 0.9838 MEE: 1.0171     \n",
      "Epoch 00764: reducing learning rate of group 0 to 1.9200e-04. 0.1667, MEE: 0.4179 | Test - Epoch [764/5000], Loss: 0.8507 MEE: 0.9379 \n",
      "Epoch 00781: reducing learning rate of group 0 to 1.5360e-04.0.3877 | Test - Epoch [781/5000], Loss: 0.8178 MEE: 0.9162 \n",
      "Epoch 00793: reducing learning rate of group 0 to 1.2288e-04. 0.1407, MEE: 0.3794 | Test - Epoch [793/5000], Loss: 0.7973 MEE: 0.9015 \n",
      "Epoch 00847: reducing learning rate of group 0 to 9.8304e-05. 0.1179, MEE: 0.3472 | Test - Epoch [847/5000], Loss: 0.7523 MEE: 0.8800 \n",
      "N. Epochs = 913 - Loss (train | test/val )= (0.1002 | 0.704 ) - MEE (train | test/val ) = (0.3323 | 0.8571 )Loss: 0.7040 MEE: 0.8571 \n",
      "Epoch 00546: reducing learning rate of group 0 to 2.4000e-04.7386 | Test - Epoch [546/5000], Loss: 1.3895 MEE: 1.1722     \n",
      "Epoch 00674: reducing learning rate of group 0 to 1.9200e-04. 0.3027, MEE: 0.5424 | Test - Epoch [674/5000], Loss: 0.9129 MEE: 0.9910 \n",
      "Epoch 00750: reducing learning rate of group 0 to 1.5360e-04.0.4307 | Test - Epoch [750/5000], Loss: 0.6824 MEE: 0.8866 \n",
      "Epoch 00782: reducing learning rate of group 0 to 1.2288e-04. 0.1575, MEE: 0.4140 | Test - Epoch [782/5000], Loss: 0.6600 MEE: 0.8798 \n",
      "Epoch 00872: reducing learning rate of group 0 to 9.8304e-05. 0.1106, MEE: 0.3547 | Test - Epoch [872/5000], Loss: 0.5755 MEE: 0.8323 \n",
      "Epoch 00900: reducing learning rate of group 0 to 7.8643e-05.0.1039, MEE: 0.3465 | Test - Epoch [900/5000], Loss: 0.5600 MEE: 0.8276 \n",
      "Epoch 00938: reducing learning rate of group 0 to 6.2915e-05.0.0973, MEE: 0.3375 | Test - Epoch [938/5000], Loss: 0.5486 MEE: 0.8167 \n",
      "Epoch 00959: reducing learning rate of group 0 to 5.0332e-05.0.0935, MEE: 0.3278 | Test - Epoch [959/5000], Loss: 0.5508 MEE: 0.8140 \n",
      "Epoch 01004: reducing learning rate of group 0 to 4.0265e-05.: 0.0884, MEE: 0.3183 | Test - Epoch [1004/5000], Loss: 0.5460 MEE: 0.8124 \n",
      "N. Epochs = 1062 - Loss (train | test/val )= (0.08309 | 0.54 ) - MEE (train | test/val ) = (0.3094 | 0.8035 ) Loss: 0.5400 MEE: 0.8035 \n",
      "Epoch 00714: reducing learning rate of group 0 to 2.4000e-04.4706 | Test - Epoch [714/5000], Loss: 0.7277 MEE: 0.9244     \n",
      "Epoch 00729: reducing learning rate of group 0 to 1.9200e-04. 0.1817, MEE: 0.4423 | Test - Epoch [729/5000], Loss: 0.7175 MEE: 0.9183 \n",
      "Epoch 00759: reducing learning rate of group 0 to 1.5360e-04.0.3979 | Test - Epoch [759/5000], Loss: 0.6350 MEE: 0.8746 \n",
      "Epoch 00800: reducing learning rate of group 0 to 1.2288e-04. 0.1271, MEE: 0.3666 | Test - Epoch [800/5000], Loss: 0.5942 MEE: 0.8448 \n",
      "Epoch 00842: reducing learning rate of group 0 to 9.8304e-05. 0.1169, MEE: 0.3602 | Test - Epoch [842/5000], Loss: 0.5819 MEE: 0.8339 \n",
      "Epoch 00883: reducing learning rate of group 0 to 7.8643e-05.0.1041, MEE: 0.3413 | Test - Epoch [883/5000], Loss: 0.5668 MEE: 0.8313 \n",
      "Epoch 00916: reducing learning rate of group 0 to 6.2915e-05.0.0963, MEE: 0.3291 | Test - Epoch [916/5000], Loss: 0.5514 MEE: 0.8196 \n",
      "Epoch 00944: reducing learning rate of group 0 to 5.0332e-05.0.0917, MEE: 0.3223 | Test - Epoch [944/5000], Loss: 0.5433 MEE: 0.8115 \n",
      "N. Epochs = 1015 - Loss (train | test/val )= (0.08382 | 0.5335 ) - MEE (train | test/val ) = (0.3124 | 0.8076 )Loss: 0.5335 MEE: 0.8076 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.08903 +- 0.007896 | 0.5925 +- 0.0789)- MEE (train | test/val ) = ( 0.318 +-  0.01019 | 0.8227 +- 0.02438)\n",
      "Epoch 00596: reducing learning rate of group 0 to 2.4000e-04.5671 | Test - Epoch [596/5000], Loss: 0.9989 MEE: 1.0752     \n",
      "Epoch 00634: reducing learning rate of group 0 to 1.9200e-04. 0.3558, MEE: 0.5177 | Test - Epoch [634/5000], Loss: 0.8833 MEE: 1.0254 \n",
      "Epoch 00654: reducing learning rate of group 0 to 1.5360e-04.0.5020 | Test - Epoch [654/5000], Loss: 0.8381 MEE: 0.9960 \n",
      "Epoch 00709: reducing learning rate of group 0 to 1.2288e-04. 0.3003, MEE: 0.4538 | Test - Epoch [709/5000], Loss: 0.7790 MEE: 0.9514 \n",
      "Epoch 00730: reducing learning rate of group 0 to 9.8304e-05. 0.2718, MEE: 0.4490 | Test - Epoch [730/5000], Loss: 0.7289 MEE: 0.9541 \n",
      "Epoch 00744: reducing learning rate of group 0 to 7.8643e-05.0.2564, MEE: 0.4330 | Test - Epoch [744/5000], Loss: 0.7066 MEE: 0.9378 \n",
      "Epoch 00861: reducing learning rate of group 0 to 6.2915e-05.0.2146, MEE: 0.3907 | Test - Epoch [861/5000], Loss: 0.6344 MEE: 0.9060 \n",
      "Epoch 00987: reducing learning rate of group 0 to 5.0332e-05.0.1650, MEE: 0.3579 | Test - Epoch [987/5000], Loss: 0.5626 MEE: 0.8733 \n",
      "Epoch 01110: reducing learning rate of group 0 to 4.0265e-05.: 0.1200, MEE: 0.3273 | Test - Epoch [1110/5000], Loss: 0.4948 MEE: 0.8379 \n",
      "Epoch 01133: reducing learning rate of group 0 to 3.2212e-05. 0.1114, MEE: 0.3222 | Test - Epoch [1133/5000], Loss: 0.4853 MEE: 0.8327 \n",
      "Epoch 01177: reducing learning rate of group 0 to 2.5770e-05. 0.1058, MEE: 0.3193 | Test - Epoch [1177/5000], Loss: 0.4776 MEE: 0.8283 \n",
      "N. Epochs = 1186 - Loss (train | test/val )= (0.1043 | 0.4743 ) - MEE (train | test/val ) = (0.313 | 0.8226 ), Loss: 0.4743 MEE: 0.8226 \n",
      "Epoch 00578: reducing learning rate of group 0 to 2.4000e-04.6023 | Test - Epoch [578/5000], Loss: 1.1489 MEE: 1.1073     \n",
      "Epoch 00629: reducing learning rate of group 0 to 1.9200e-04. 0.4000, MEE: 0.5264 | Test - Epoch [629/5000], Loss: 0.9945 MEE: 1.0434 \n",
      "Epoch 00659: reducing learning rate of group 0 to 1.5360e-04.0.4937 | Test - Epoch [659/5000], Loss: 0.9485 MEE: 1.0197 \n",
      "Epoch 00723: reducing learning rate of group 0 to 1.2288e-04. 0.3124, MEE: 0.4499 | Test - Epoch [723/5000], Loss: 0.8384 MEE: 0.9651 \n",
      "Epoch 00780: reducing learning rate of group 0 to 9.8304e-05. 0.2847, MEE: 0.4203 | Test - Epoch [780/5000], Loss: 0.7939 MEE: 0.9422 \n",
      "Epoch 00797: reducing learning rate of group 0 to 7.8643e-05.0.2793, MEE: 0.4134 | Test - Epoch [797/5000], Loss: 0.7846 MEE: 0.9298 \n",
      "Epoch 00811: reducing learning rate of group 0 to 6.2915e-05.0.2779, MEE: 0.4240 | Test - Epoch [811/5000], Loss: 0.7823 MEE: 0.9438 \n",
      "Epoch 00862: reducing learning rate of group 0 to 5.0332e-05.0.2551, MEE: 0.4007 | Test - Epoch [862/5000], Loss: 0.7479 MEE: 0.9264 \n",
      "Epoch 00996: reducing learning rate of group 0 to 4.0265e-05. 0.2162, MEE: 0.3626 | Test - Epoch [996/5000], Loss: 0.6736 MEE: 0.8849 \n",
      "Epoch 01019: reducing learning rate of group 0 to 3.2212e-05. 0.2081, MEE: 0.3582 | Test - Epoch [1019/5000], Loss: 0.6589 MEE: 0.8777 \n",
      "Epoch 01036: reducing learning rate of group 0 to 2.5770e-05. 0.2028, MEE: 0.3560 | Test - Epoch [1036/5000], Loss: 0.6513 MEE: 0.8775 \n",
      "Epoch 01221: reducing learning rate of group 0 to 2.0616e-05.: 0.1718, MEE: 0.3332 | Test - Epoch [1221/5000], Loss: 0.5940 MEE: 0.8486 \n",
      "Epoch 01260: reducing learning rate of group 0 to 1.6493e-05.: 0.1620, MEE: 0.3305 | Test - Epoch [1260/5000], Loss: 0.5776 MEE: 0.8426 \n",
      "N. Epochs = 1261 - Loss (train | test/val )= (0.162 | 0.5783 ) - MEE (train | test/val ) = (0.3296 | 0.844 ), Loss: 0.5783 MEE: 0.8440 \n",
      "Epoch 00594: reducing learning rate of group 0 to 2.4000e-04.5792 | Test - Epoch [594/5000], Loss: 0.8903 MEE: 1.0410     \n",
      "Epoch 00616: reducing learning rate of group 0 to 1.9200e-04. 0.3264, MEE: 0.5127 | Test - Epoch [616/5000], Loss: 0.8492 MEE: 1.0050 \n",
      "Epoch 00698: reducing learning rate of group 0 to 1.5360e-04.0.4439 | Test - Epoch [698/5000], Loss: 0.7239 MEE: 0.9391 \n",
      "Epoch 00772: reducing learning rate of group 0 to 1.2288e-04. 0.2044, MEE: 0.4299 | Test - Epoch [772/5000], Loss: 0.6444 MEE: 0.9185 \n",
      "Epoch 00800: reducing learning rate of group 0 to 9.8304e-05. 0.1871, MEE: 0.3862 | Test - Epoch [800/5000], Loss: 0.6166 MEE: 0.8837 \n",
      "Epoch 00823: reducing learning rate of group 0 to 7.8643e-05.0.1794, MEE: 0.3743 | Test - Epoch [823/5000], Loss: 0.6096 MEE: 0.8810 \n",
      "Epoch 00854: reducing learning rate of group 0 to 6.2915e-05.0.1718, MEE: 0.3657 | Test - Epoch [854/5000], Loss: 0.5964 MEE: 0.8739 \n",
      "Epoch 00874: reducing learning rate of group 0 to 5.0332e-05.0.1636, MEE: 0.3576 | Test - Epoch [874/5000], Loss: 0.5833 MEE: 0.8663 \n",
      "Epoch 00896: reducing learning rate of group 0 to 4.0265e-05. 0.1591, MEE: 0.3530 | Test - Epoch [896/5000], Loss: 0.5774 MEE: 0.8629 \n",
      "Epoch 00925: reducing learning rate of group 0 to 3.2212e-05.0.1559, MEE: 0.3478 | Test - Epoch [925/5000], Loss: 0.5718 MEE: 0.8605 \n",
      "N. Epochs = 983 - Loss (train | test/val )= (0.1507 | 0.5614 ) - MEE (train | test/val ) = (0.3411 | 0.8567 )oss: 0.5614 MEE: 0.8567 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.139 +- 0.02495 | 0.538 +- 0.04556)- MEE (train | test/val ) = ( 0.3279 +-  0.01154 | 0.8411 +- 0.01407)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.104 +- 0.02481 | val_loss = 0.4729 +- 0.1324train_mee = 0.3157 +- 0.01102 | val_mee = 0.7525 +- 0.1126\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.8;lr_patience=15\n",
      "Epoch 00644: reducing learning rate of group 0 to 2.4000e-04.6233 | Test - Epoch [644/5000], Loss: 0.4200 MEE: 0.7865     \n",
      "N. Epochs = 810 - Loss (train | test/val )= (0.1631 | 0.3493 ) - MEE (train | test/val ) = (0.4262 | 0.6841 )Loss: 0.3493 MEE: 0.6841 \n",
      "Epoch 00681: reducing learning rate of group 0 to 2.4000e-04.5769 | Test - Epoch [681/5000], Loss: 0.3370 MEE: 0.7185     \n",
      "Epoch 00732: reducing learning rate of group 0 to 1.9200e-04. 0.3726, MEE: 0.5243 | Test - Epoch [732/5000], Loss: 0.3210 MEE: 0.7105 \n",
      "Epoch 00809: reducing learning rate of group 0 to 1.5360e-04.0.4553 | Test - Epoch [809/5000], Loss: 0.2852 MEE: 0.6612 \n",
      "Epoch 00962: reducing learning rate of group 0 to 1.2288e-04. 0.1172, MEE: 0.3609 | Test - Epoch [962/5000], Loss: 0.2575 MEE: 0.6003 \n",
      "N. Epochs = 966 - Loss (train | test/val )= (0.1161 | 0.2574 ) - MEE (train | test/val ) = (0.3623 | 0.6105 )Loss: 0.2574 MEE: 0.6105 \n",
      "Epoch 00735: reducing learning rate of group 0 to 2.4000e-04.5755 | Test - Epoch [735/5000], Loss: 0.3868 MEE: 0.7402     \n",
      "N. Epochs = 738 - Loss (train | test/val )= (0.3265 | 0.3441 ) - MEE (train | test/val ) = (0.5113 | 0.705 ) Loss: 0.3441 MEE: 0.7050 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.2019 +- 0.09014 | 0.3169 +- 0.04216)- MEE (train | test/val ) = ( 0.4333 +-  0.06101 | 0.6665 +- 0.04054)\n",
      "Epoch 00699: reducing learning rate of group 0 to 2.4000e-04.4934 | Test - Epoch [699/5000], Loss: 0.7341 MEE: 0.9662     \n",
      "Epoch 00758: reducing learning rate of group 0 to 1.9200e-04. 0.1387, MEE: 0.3999 | Test - Epoch [758/5000], Loss: 0.6601 MEE: 0.8949 \n",
      "Epoch 00831: reducing learning rate of group 0 to 1.5360e-04.0.3483 | Test - Epoch [831/5000], Loss: 0.6173 MEE: 0.8456 \n",
      "N. Epochs = 870 - Loss (train | test/val )= (0.09712 | 0.598 ) - MEE (train | test/val ) = (0.341 | 0.8464 ) Loss: 0.5980 MEE: 0.8464 \n",
      "Epoch 00727: reducing learning rate of group 0 to 2.4000e-04.5272 | Test - Epoch [727/5000], Loss: 0.8576 MEE: 0.9526     \n",
      "Epoch 00775: reducing learning rate of group 0 to 1.9200e-04. 0.1911, MEE: 0.4219 | Test - Epoch [775/5000], Loss: 0.7946 MEE: 0.9078 \n",
      "N. Epochs = 777 - Loss (train | test/val )= (0.187 | 0.7902 ) - MEE (train | test/val ) = (0.4114 | 0.9018 )MEE: 0.9018 \n",
      "Epoch 00669: reducing learning rate of group 0 to 2.4000e-04.4928 | Test - Epoch [669/5000], Loss: 0.9166 MEE: 0.9989     \n",
      "Epoch 00691: reducing learning rate of group 0 to 1.9200e-04. 0.2207, MEE: 0.4787 | Test - Epoch [691/5000], Loss: 0.8703 MEE: 0.9660 \n",
      "Epoch 00739: reducing learning rate of group 0 to 1.5360e-04.0.4217 | Test - Epoch [739/5000], Loss: 0.8314 MEE: 0.9455 \n",
      "N. Epochs = 911 - Loss (train | test/val )= (0.09298 | 0.6464 ) - MEE (train | test/val ) = (0.3567 | 0.8707 )oss: 0.6464 MEE: 0.8707 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1257 +- 0.04337 | 0.6782 +- 0.08159)- MEE (train | test/val ) = ( 0.3697 +-  0.03019 | 0.873 +- 0.02269)\n",
      "Epoch 00669: reducing learning rate of group 0 to 2.4000e-04.4834 | Test - Epoch [669/5000], Loss: 0.8138 MEE: 1.0076     \n",
      "Epoch 00700: reducing learning rate of group 0 to 1.9200e-04. 0.2690, MEE: 0.4739 | Test - Epoch [700/5000], Loss: 0.7457 MEE: 0.9707 \n",
      "Epoch 00722: reducing learning rate of group 0 to 1.5360e-04.0.4236 | Test - Epoch [722/5000], Loss: 0.7221 MEE: 0.9419 \n",
      "Epoch 00786: reducing learning rate of group 0 to 1.2288e-04. 0.1923, MEE: 0.3889 | Test - Epoch [786/5000], Loss: 0.6378 MEE: 0.9063 \n",
      "Epoch 01057: reducing learning rate of group 0 to 9.8304e-05.: 0.0905, MEE: 0.3162 | Test - Epoch [1057/5000], Loss: 0.4729 MEE: 0.8210 \n",
      "N. Epochs = 1061 - Loss (train | test/val )= (0.08671 | 0.4722 ) - MEE (train | test/val ) = (0.3059 | 0.8214 )oss: 0.4722 MEE: 0.8214 \n",
      "Epoch 00761: reducing learning rate of group 0 to 2.4000e-04.4163 | Test - Epoch [761/5000], Loss: 0.7840 MEE: 0.9303     \n",
      "Epoch 00783: reducing learning rate of group 0 to 1.9200e-04. 0.2372, MEE: 0.4163 | Test - Epoch [783/5000], Loss: 0.7367 MEE: 0.9287 \n",
      "N. Epochs = 787 - Loss (train | test/val )= (0.2288 | 0.7211 ) - MEE (train | test/val ) = (0.3853 | 0.9084 )EE: 0.9084 \n",
      "Epoch 00704: reducing learning rate of group 0 to 2.4000e-04.4766 | Test - Epoch [704/5000], Loss: 0.7182 MEE: 0.9757     \n",
      "Epoch 00796: reducing learning rate of group 0 to 1.9200e-04. 0.1545, MEE: 0.3902 | Test - Epoch [796/5000], Loss: 0.5588 MEE: 0.8704 \n",
      "Epoch 00877: reducing learning rate of group 0 to 1.5360e-04.0.3434 | Test - Epoch [877/5000], Loss: 0.4997 MEE: 0.8364 \n",
      "N. Epochs = 881 - Loss (train | test/val )= (0.1025 | 0.4903 ) - MEE (train | test/val ) = (0.3294 | 0.8278 )Loss: 0.4903 MEE: 0.8278 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1393 +- 0.0636 | 0.5612 +- 0.1133)- MEE (train | test/val ) = ( 0.3402 +-  0.0333 | 0.8526 +- 0.03956)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1556 +- 0.03318 | val_loss = 0.5188 +- 0.1505train_mee = 0.3811 +- 0.03884 | val_mee = 0.7973 +- 0.09289\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.9;lr_patience=10\n",
      "Epoch 00555: reducing learning rate of group 0 to 2.7000e-04.7595 | Test - Epoch [555/5000], Loss: 0.4741 MEE: 0.8763     \n",
      "Epoch 00693: reducing learning rate of group 0 to 2.4300e-04..5314 | Test - Epoch [693/5000], Loss: 0.3289 MEE: 0.7021 \n",
      "Epoch 00734: reducing learning rate of group 0 to 2.1870e-04.0.4474 | Test - Epoch [734/5000], Loss: 0.3249 MEE: 0.6580 \n",
      "Epoch 00833: reducing learning rate of group 0 to 1.9683e-04. 0.4386 | Test - Epoch [833/5000], Loss: 0.3084 MEE: 0.6721 \n",
      "N. Epochs = 835 - Loss (train | test/val )= (0.1038 | 0.2911 ) - MEE (train | test/val ) = (0.3758 | 0.611 )1 MEE: 0.6110 \n",
      "Epoch 00670: reducing learning rate of group 0 to 2.7000e-04.5795 | Test - Epoch [670/5000], Loss: 0.3755 MEE: 0.7716     \n",
      "Epoch 00695: reducing learning rate of group 0 to 2.4300e-04..5077 | Test - Epoch [695/5000], Loss: 0.3376 MEE: 0.7057 \n",
      "Epoch 00778: reducing learning rate of group 0 to 2.1870e-04.0.4307 | Test - Epoch [778/5000], Loss: 0.3329 MEE: 0.6740 \n",
      "Epoch 00796: reducing learning rate of group 0 to 1.9683e-04. 0.3907 | Test - Epoch [796/5000], Loss: 0.3054 MEE: 0.6387 \n",
      "Epoch 00841: reducing learning rate of group 0 to 1.7715e-04.: 0.3829 | Test - Epoch [841/5000], Loss: 0.3041 MEE: 0.6412 \n",
      "N. Epochs = 850 - Loss (train | test/val )= (0.1019 | 0.3017 ) - MEE (train | test/val ) = (0.3735 | 0.639 )17 MEE: 0.6390 \n",
      "Epoch 00724: reducing learning rate of group 0 to 2.7000e-04.4779 | Test - Epoch [724/5000], Loss: 0.3372 MEE: 0.7063     \n",
      "Epoch 00742: reducing learning rate of group 0 to 2.4300e-04..4429 | Test - Epoch [742/5000], Loss: 0.3274 MEE: 0.6644 \n",
      "Epoch 00783: reducing learning rate of group 0 to 2.1870e-04.0.3908 | Test - Epoch [783/5000], Loss: 0.3110 MEE: 0.6523 \n",
      "Epoch 00809: reducing learning rate of group 0 to 1.9683e-04. 0.3747 | Test - Epoch [809/5000], Loss: 0.3030 MEE: 0.6364 \n",
      "Epoch 00863: reducing learning rate of group 0 to 1.7715e-04.: 0.3641 | Test - Epoch [863/5000], Loss: 0.2979 MEE: 0.6361 \n",
      "Epoch 00875: reducing learning rate of group 0 to 1.5943e-04.E: 0.3395 | Test - Epoch [875/5000], Loss: 0.2894 MEE: 0.6156 \n",
      "Epoch 00908: reducing learning rate of group 0 to 1.4349e-04.EE: 0.3205 | Test - Epoch [908/5000], Loss: 0.2829 MEE: 0.6087 \n",
      "Epoch 00930: reducing learning rate of group 0 to 1.2914e-04.MEE: 0.3081 | Test - Epoch [930/5000], Loss: 0.2802 MEE: 0.6010 \n",
      "Epoch 00959: reducing learning rate of group 0 to 1.1623e-04. MEE: 0.3029 | Test - Epoch [959/5000], Loss: 0.2763 MEE: 0.5960 \n",
      "N. Epochs = 968 - Loss (train | test/val )= (0.06584 | 0.2735 ) - MEE (train | test/val ) = (0.2947 | 0.5921 )2735 MEE: 0.5921 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.09054 +- 0.01748 | 0.2888 +- 0.01166)- MEE (train | test/val ) = ( 0.348 +-  0.03769 | 0.6141 +- 0.01927)\n",
      "Epoch 00654: reducing learning rate of group 0 to 2.7000e-04.5292 | Test - Epoch [654/5000], Loss: 0.8358 MEE: 1.0106     \n",
      "Epoch 00685: reducing learning rate of group 0 to 2.4300e-04..4536 | Test - Epoch [685/5000], Loss: 0.7494 MEE: 0.9366 \n",
      "Epoch 00740: reducing learning rate of group 0 to 2.1870e-04.0.4433 | Test - Epoch [740/5000], Loss: 0.7129 MEE: 0.9275 \n",
      "N. Epochs = 749 - Loss (train | test/val )= (0.1441 | 0.7162 ) - MEE (train | test/val ) = (0.4172 | 0.9282 )MEE: 0.9282 \n",
      "Epoch 00566: reducing learning rate of group 0 to 2.7000e-04.7440 | Test - Epoch [566/5000], Loss: 1.6328 MEE: 1.1999     \n",
      "Epoch 00626: reducing learning rate of group 0 to 2.4300e-04..6230 | Test - Epoch [626/5000], Loss: 1.3645 MEE: 1.0955 \n",
      "Epoch 00704: reducing learning rate of group 0 to 2.1870e-04.0.5439 | Test - Epoch [704/5000], Loss: 0.9952 MEE: 1.0031 \n",
      "Epoch 00729: reducing learning rate of group 0 to 1.9683e-04. 0.4894 | Test - Epoch [729/5000], Loss: 0.8438 MEE: 0.9531 \n",
      "Epoch 00805: reducing learning rate of group 0 to 1.7715e-04.: 0.3916 | Test - Epoch [805/5000], Loss: 0.7056 MEE: 0.8826 \n",
      "Epoch 00825: reducing learning rate of group 0 to 1.5943e-04.E: 0.3913 | Test - Epoch [825/5000], Loss: 0.7010 MEE: 0.8904 \n",
      "Epoch 00880: reducing learning rate of group 0 to 1.4349e-04.EE: 0.3603 | Test - Epoch [880/5000], Loss: 0.6368 MEE: 0.8638 \n",
      "Epoch 00923: reducing learning rate of group 0 to 1.2914e-04.MEE: 0.3328 | Test - Epoch [923/5000], Loss: 0.6075 MEE: 0.8436 \n",
      "Epoch 00949: reducing learning rate of group 0 to 1.1623e-04. MEE: 0.3356 | Test - Epoch [949/5000], Loss: 0.6039 MEE: 0.8338 \n",
      "N. Epochs = 958 - Loss (train | test/val )= (0.08798 | 0.6061 ) - MEE (train | test/val ) = (0.3305 | 0.8485 )6061 MEE: 0.8485 \n",
      "Epoch 00679: reducing learning rate of group 0 to 2.7000e-04.5216 | Test - Epoch [679/5000], Loss: 0.9827 MEE: 1.0225     \n",
      "Epoch 00726: reducing learning rate of group 0 to 2.4300e-04..4339 | Test - Epoch [726/5000], Loss: 0.8565 MEE: 0.9350 \n",
      "Epoch 00756: reducing learning rate of group 0 to 2.1870e-04.0.4012 | Test - Epoch [756/5000], Loss: 0.8344 MEE: 0.9309 \n",
      "Epoch 00788: reducing learning rate of group 0 to 1.9683e-04. 0.3773 | Test - Epoch [788/5000], Loss: 0.7938 MEE: 0.9122 \n",
      "Epoch 00817: reducing learning rate of group 0 to 1.7715e-04.: 0.3568 | Test - Epoch [817/5000], Loss: 0.7268 MEE: 0.8924 \n",
      "Epoch 00854: reducing learning rate of group 0 to 1.5943e-04.E: 0.3410 | Test - Epoch [854/5000], Loss: 0.7161 MEE: 0.8814 \n",
      "N. Epochs = 863 - Loss (train | test/val )= (0.101 | 0.7099 ) - MEE (train | test/val ) = (0.3282 | 0.8751 )099 MEE: 0.8751 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.111 +- 0.024 | 0.6774 +- 0.05047)- MEE (train | test/val ) = ( 0.3586 +-  0.04144 | 0.8839 +- 0.03314)\n",
      "Epoch 00628: reducing learning rate of group 0 to 2.7000e-04.5542 | Test - Epoch [628/5000], Loss: 0.8337 MEE: 1.0162     \n",
      "N. Epochs = 637 - Loss (train | test/val )= (0.34 | 0.8147 ) - MEE (train | test/val ) = (0.5375 | 1.004 ) MEE: 1.0041 \n",
      "Epoch 00580: reducing learning rate of group 0 to 2.7000e-04.6288 | Test - Epoch [580/5000], Loss: 1.0481 MEE: 1.1152     \n",
      "Epoch 00678: reducing learning rate of group 0 to 2.4300e-04..4799 | Test - Epoch [678/5000], Loss: 0.8408 MEE: 1.0089 \n",
      "Epoch 00716: reducing learning rate of group 0 to 2.1870e-04.0.4348 | Test - Epoch [716/5000], Loss: 0.7776 MEE: 0.9578 \n",
      "Epoch 00728: reducing learning rate of group 0 to 1.9683e-04. 0.4493 | Test - Epoch [728/5000], Loss: 0.7401 MEE: 0.9435 \n",
      "Epoch 00776: reducing learning rate of group 0 to 1.7715e-04.: 0.4225 | Test - Epoch [776/5000], Loss: 0.6919 MEE: 0.9305 \n",
      "Epoch 00793: reducing learning rate of group 0 to 1.5943e-04.E: 0.3989 | Test - Epoch [793/5000], Loss: 0.6746 MEE: 0.9061 \n",
      "Epoch 00852: reducing learning rate of group 0 to 1.4349e-04.EE: 0.3985 | Test - Epoch [852/5000], Loss: 0.6114 MEE: 0.8995 \n",
      "Epoch 00908: reducing learning rate of group 0 to 1.2914e-04.MEE: 0.3394 | Test - Epoch [908/5000], Loss: 0.5628 MEE: 0.8568 \n",
      "N. Epochs = 911 - Loss (train | test/val )= (0.1358 | 0.5652 ) - MEE (train | test/val ) = (0.3425 | 0.8589 )5652 MEE: 0.8589 \n",
      "Epoch 00611: reducing learning rate of group 0 to 2.7000e-04.5548 | Test - Epoch [611/5000], Loss: 0.8776 MEE: 1.0607     \n",
      "Epoch 00698: reducing learning rate of group 0 to 2.4300e-04..4410 | Test - Epoch [698/5000], Loss: 0.7194 MEE: 0.9482 \n",
      "Epoch 00774: reducing learning rate of group 0 to 2.1870e-04.0.4138 | Test - Epoch [774/5000], Loss: 0.6380 MEE: 0.9146 \n",
      "Epoch 00811: reducing learning rate of group 0 to 1.9683e-04. 0.3805 | Test - Epoch [811/5000], Loss: 0.6021 MEE: 0.8926 \n",
      "Epoch 00841: reducing learning rate of group 0 to 1.7715e-04.: 0.3596 | Test - Epoch [841/5000], Loss: 0.5709 MEE: 0.8562 \n",
      "Epoch 00863: reducing learning rate of group 0 to 1.5943e-04.E: 0.3689 | Test - Epoch [863/5000], Loss: 0.5562 MEE: 0.8528 \n",
      "N. Epochs = 909 - Loss (train | test/val )= (0.1175 | 0.5224 ) - MEE (train | test/val ) = (0.3193 | 0.8298 )24 MEE: 0.8298 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1978 +- 0.1008 | 0.6341 +- 0.1289)- MEE (train | test/val ) = ( 0.3998 +-  0.09784 | 0.8976 +- 0.07622)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.1331 +- 0.04647 | val_loss = 0.5334 +- 0.1739train_mee = 0.3688 +- 0.02234 | val_mee = 0.7985 +- 0.1306\n",
      "activation=Tanh(); layers=3; neuron_number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; factor=0.9;lr_patience=15\n",
      "Epoch 00782: reducing learning rate of group 0 to 2.7000e-04.4909 | Test - Epoch [782/5000], Loss: 0.4534 MEE: 0.7583     \n",
      "N. Epochs = 786 - Loss (train | test/val )= (0.145 | 0.4405 ) - MEE (train | test/val ) = (0.4416 | 0.7039 )EE: 0.7039 \n",
      "Epoch 00700: reducing learning rate of group 0 to 2.7000e-04.5809 | Test - Epoch [700/5000], Loss: 0.3299 MEE: 0.7447     \n",
      "N. Epochs = 704 - Loss (train | test/val )= (0.384 | 0.3099 ) - MEE (train | test/val ) = (0.564 | 0.7033 )MEE: 0.7033 \n",
      "Epoch 00670: reducing learning rate of group 0 to 2.7000e-04.5575 | Test - Epoch [670/5000], Loss: 0.3700 MEE: 0.7338     \n",
      "Epoch 00709: reducing learning rate of group 0 to 2.4300e-04..5604 | Test - Epoch [709/5000], Loss: 0.3767 MEE: 0.7393 \n",
      "N. Epochs = 713 - Loss (train | test/val )= (0.2524 | 0.3451 ) - MEE (train | test/val ) = (0.5083 | 0.6972 )EE: 0.6972 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.2605 +- 0.09773 | 0.3652 +- 0.05517)- MEE (train | test/val ) = ( 0.5046 +-  0.05002 | 0.7015 +- 0.003017)\n",
      "Epoch 00712: reducing learning rate of group 0 to 2.7000e-04.4647 | Test - Epoch [712/5000], Loss: 0.6625 MEE: 0.8647     \n",
      "N. Epochs = 716 - Loss (train | test/val )= (0.2017 | 0.6546 ) - MEE (train | test/val ) = (0.4715 | 0.8639 )E: 0.8639 \n",
      "N. Epochs = 571 - Loss (train | test/val )= (0.9829 | 1.951 ) - MEE (train | test/val ) = (0.8271 | 1.274 )EE: 1.2736     \n",
      "Epoch 00644: reducing learning rate of group 0 to 2.7000e-04.5868 | Test - Epoch [644/5000], Loss: 1.0077 MEE: 1.0486     \n",
      "Epoch 00786: reducing learning rate of group 0 to 2.4300e-04..4118 | Test - Epoch [786/5000], Loss: 0.6690 MEE: 0.9085 \n",
      "N. Epochs = 790 - Loss (train | test/val )= (0.124 | 0.6597 ) - MEE (train | test/val ) = (0.3913 | 0.8881 )MEE: 0.8881 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.4362 +- 0.3879 | 1.088 +- 0.6099)- MEE (train | test/val ) = ( 0.5633 +-  0.1894 | 1.009 +- 0.1877)\n",
      "Epoch 00694: reducing learning rate of group 0 to 2.7000e-04.4680 | Test - Epoch [694/5000], Loss: 0.7478 MEE: 0.9762     \n",
      "N. Epochs = 698 - Loss (train | test/val )= (0.2452 | 0.7383 ) - MEE (train | test/val ) = (0.4695 | 0.9821 )E: 0.9821 \n",
      "Epoch 00659: reducing learning rate of group 0 to 2.7000e-04.4862 | Test - Epoch [659/5000], Loss: 0.7243 MEE: 0.9635     \n",
      "Epoch 00726: reducing learning rate of group 0 to 2.4300e-04..5414 | Test - Epoch [726/5000], Loss: 0.6655 MEE: 1.0074 \n",
      "N. Epochs = 730 - Loss (train | test/val )= (0.1849 | 0.6283 ) - MEE (train | test/val ) = (0.4297 | 0.933 )MEE: 0.9330 \n",
      "Epoch 00652: reducing learning rate of group 0 to 2.7000e-04.5320 | Test - Epoch [652/5000], Loss: 0.7993 MEE: 1.0227     \n",
      "N. Epochs = 656 - Loss (train | test/val )= (0.319 | 0.8135 ) - MEE (train | test/val ) = (0.5889 | 1.066 )MEE: 1.0657 \n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.2497 +- 0.05484 | 0.7267 +- 0.07606)- MEE (train | test/val ) = ( 0.496 +-  0.06764 | 0.9936 +- 0.05476)\n",
      "Final Results: activation=Tanh(); layers=3; neuron number=1000; lr=0.0003; alpha = 0; batch size = 128; lambda = 0.001 --> train_loss = 0.3155 +- 0.08549 | val_loss = 0.7268 +- 0.2952train_mee = 0.5213 +- 0.0299 | val_mee = 0.9012 +- 0.1414\n",
      "[1000, 0.0002, 0, 128, 0.001, Tanh(), 3, 0, 0.8, 10] n Best val MEE: 0.7436501847373114\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#study of the effect of activation function\n",
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [1e-4]\n",
    "momentums = [0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh(),nn.ReLU()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "optimiz = 'SGD' #either 'SGD' or 'Adam'\n",
    "'''\n",
    "\n",
    "'''\n",
    "#study of the effect of regularization\n",
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [1e-4]\n",
    "momentums = [0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-2,1e-3,1e-4,1e-5]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "optimiz = 'SGD' #either 'SGD' or 'Adam'\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "hidden_neurons = [100,1000] #total number of neurons\n",
    "learning_rates = [1e-4,1e-5]\n",
    "momentums = [0.5,0.8] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [1,2,3]\n",
    "dropouts = [0]\n",
    "optimiz = 'SGD' #either 'SGD' or 'Adam'\n",
    "'''\n",
    "\n",
    "'''\n",
    "hidden_neurons = [1000,2000,3000] #total number of neurons\n",
    "learning_rates = [1e-4]\n",
    "momentums = [0.7,0.8,0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "optimiz = 'SGD' #either 'SGD' or 'Adam'\n",
    "'''\n",
    "\n",
    "'''\n",
    "hidden_neurons = [100,1000] #total number of neurons\n",
    "learning_rates = [1e-4,1e-5]\n",
    "momentums = [0] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [1,2,3]\n",
    "dropouts = [0]\n",
    "optimiz = 'Adam' #either 'SGD' or 'Adam'\n",
    "'''\n",
    "\n",
    "'''\n",
    "hidden_neurons = [1000,2000,3000] #total number of neurons\n",
    "learning_rates = [1e-4]\n",
    "momentums = [0] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "optimiz = 'Adam' #either 'SGD' or 'Adam'\n",
    "'''\n",
    "\n",
    "\n",
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [2e-4,3e-4]\n",
    "momentums = [0] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "optimiz = 'Adam' #either 'SGD' or 'Adam'\n",
    "factors = [0.5,0.6,0.7,0.8,0.9]\n",
    "lr_patiences = [10,15]\n",
    "\n",
    "#[1000, 0.0001, 0, 128, 0.001, Tanh(), 3, 0, 0.7, 20] n Best val MEE: 0.7188452879587809\n",
    "\n",
    "\n",
    "\n",
    "best_hp = perform_grid_search_kfold(hidden_neurons,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    activations,\n",
    "                                    layerss,\n",
    "                                    dropouts,\n",
    "                                    factors,\n",
    "                                    lr_patiences,\n",
    "                                    optimiz,\n",
    "                                    k_folds=3,\n",
    "                                    x=X_train,\n",
    "                                    y=y_train,\n",
    "                                    num_epochs=5000,\n",
    "                                    number_of_init=3,\n",
    "                                    plot_curves=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN THE FINAL MODEL ON ALL THE TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_with_different_initializations() missing 2 required positional arguments: 'factor' and 'lr_patience'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[253], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m models, train_losses_hist, test_losses_hist, train_mees_hist, test_mees_hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_different_initializations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m               \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m               \u001b[49m\u001b[43mx_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m               \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m               \u001b[49m\u001b[43mneuron_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_hp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m               \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_hp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m               \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_hp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m               \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbest_hp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m               \u001b[49m\u001b[43mreg_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbest_hp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m               \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbest_hp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m               \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbest_hp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m               \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbest_hp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m               \u001b[49m\u001b[43moptimiz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimiz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m               \u001b[49m\u001b[43mmax_num_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m               \u001b[49m\u001b[43mplot_curves\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m               \u001b[49m\u001b[43mreturn_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m               \u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#save models to file\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models):\n",
      "\u001b[0;31mTypeError\u001b[0m: train_with_different_initializations() missing 2 required positional arguments: 'factor' and 'lr_patience'"
     ]
    }
   ],
   "source": [
    "models, train_losses_hist, test_losses_hist, train_mees_hist, test_mees_hist = train_with_different_initializations(x_train=X_train,\n",
    "               y_train=y_train,\n",
    "               x_test=X_test,\n",
    "               y_test=y_test,\n",
    "               neuron_number=best_hp[0],\n",
    "               learning_rate=best_hp[1],\n",
    "               momentum=best_hp[2],\n",
    "               bs = best_hp[3],\n",
    "               reg_coeff= best_hp[4],\n",
    "               activation= best_hp[5],\n",
    "               layers= best_hp[6],\n",
    "               dropout= best_hp[7],\n",
    "               optimiz=optimiz,\n",
    "               max_num_epochs=5000,\n",
    "               plot_curves=True,\n",
    "               return_history=True,\n",
    "               N=5)\n",
    "\n",
    "#save models to file\n",
    "for index, model in enumerate(models):\n",
    "    torch.save(model,f'saved_models/trained_nn{index}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_mean_std(N=5,train_hist=train_losses_hist,test_hist=test_losses_hist,label='MSE')\n",
    "plot_mean_std(N=5,train_hist=train_mees_hist,test_hist=test_mees_hist,label='MEE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMITTEE RESPONSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save models to file\n",
    "for index, model in enumerate(models):\n",
    "    torch.save(model,f'saved_models/trained_nn{index}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 3])\n",
      "0.7542089819908142\n"
     ]
    }
   ],
   "source": [
    "def committee(model_name, N):\n",
    "    model0 = torch.load(f'saved_models/{model_name}0.pth')\n",
    "    model0.eval()\n",
    "    output_sum = model0(X_test)\n",
    "    #print(output_sum[0])\n",
    "\n",
    "    for i in range(1,N):\n",
    "        model = torch.load(f'saved_models/{model_name}{i}.pth')\n",
    "        model.eval()\n",
    "        test_outputs = model(X_test)\n",
    "        #print(test_outputs[0])\n",
    "        output_sum += test_outputs\n",
    "        #print(output_sum[0])\n",
    "\n",
    "    return output_sum/N\n",
    "\n",
    "\n",
    "output_commitee = committee('trained_nn', N=5)\n",
    "print(output_commitee.shape)\n",
    "# Calculate test mee\n",
    "test_mee = mean_euclidean_error(output_commitee,y_test)\n",
    "print(test_mee)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cmepda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
