{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML CUP - Neural Networks\n",
    "In this notebook we will study the application of neural networks to the ML CUP.\n",
    "Firs we will study the effect of different optimizers (SGD, Adam, RMSProp).\n",
    "Then we will introduce a decaying lr using ReduceLROnpPlateau, and study the effects of this callback.\n",
    "A more detailed explanation of our pipeline is in the presentation slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)   # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)   # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)    # fontsize of the figure suptitle\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(train_losses, test_losses, train_mees, test_mees, hyperparams):\n",
    "    \"\"\"\n",
    "    Plot training and test curves for loss and Mean Euclidean Error (MEE).\n",
    "\n",
    "    Parameters:\n",
    "    - epoch (int): The total number of training epochs.\n",
    "    - train_losses (list): List of training losses for each epoch.\n",
    "    - test_losses (list): List of test losses for each epoch.\n",
    "    - train_mees (list): List of training MEE values for each epoch.\n",
    "    - test_mees (list): List of test MEE values for each epoch.\n",
    "    - hyperparams (list): List of hyperparameters used for the plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    Plots four subplots:\n",
    "    1. Training and test loss curves.\n",
    "    2. Training and test MEE curves.\n",
    "    3. Zoomed-in training and test loss curves with y-axis limit [0, 10].\n",
    "    4. Zoomed-in training and test MEE curves with y-axis limit [0, 10].\n",
    "\n",
    "    The hyperparameters are used in the plot title to provide additional context.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.suptitle(f'Batch Size={hyperparams[3]},Activation Function={hyperparams[5]}, Layers={hyperparams[6]} Hidden Units={hyperparams[0]}, Eta={hyperparams[1]}, Alpha={hyperparams[2]}, Lambda={hyperparams[4]}, dropout = {hyperparams[7]}')\n",
    "    # Loss plots\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(train_losses, label='Training MSE', color = 'red')\n",
    "    plt.plot(test_losses, label='Validation MSE', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "\n",
    "    # MEE plots\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(train_mees, label='Training MEE', color='red')\n",
    "    plt.plot(test_mees, label='Validation MEE', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss plots\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(train_losses, label='Training MSE', color = 'red')\n",
    "    plt.plot(test_losses, label='Validation MSE', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.ylim(0,10)\n",
    "    plt.legend()\n",
    "\n",
    "    # MEE plots\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(train_mees, label='Training MEE', color='red')\n",
    "    plt.plot(test_mees, label='Validation MEE', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.ylim(0,10)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_euclidean_error(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Compute the mean Euclidean error between two sets of 3D vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor1: PyTorch tensor of size (N, 3) representing the first set of 3D vectors\n",
    "    - tensor2: PyTorch tensor of size (N, 3) representing the second set of 3D vectors\n",
    "\n",
    "    Returns:\n",
    "    - mean_error: Mean Euclidean error between the two sets of vectors\n",
    "    \"\"\"\n",
    "    # Check if the tensors have the correct shape\n",
    "    if tensor1.shape[1] != 3 or tensor2.shape[1] != 3 or tensor1.shape[0] != tensor2.shape[0]:\n",
    "        raise ValueError(\"Input tensors must be of size (N, 3)\")\n",
    "\n",
    "\n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = torch.norm(tensor1 - tensor2, dim=1)\n",
    "\n",
    "    # Calculate the mean Euclidean error\n",
    "    mean_error = torch.mean(euclidean_distance)\n",
    "\n",
    "    return mean_error.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regression eural network\n",
    "\n",
    "class RegressorNN(nn.Module):\n",
    "    def __init__(self, hidden_size, activation_function, num_layers, dropout_prob, input_size=10, output_size=3):\n",
    "        super(RegressorNN, self).__init__()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers = [nn.Linear(input_size, hidden_size)]\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(activation_function)\n",
    "            self.layers.append(nn.Dropout(p=dropout_prob))  # Add dropout layer\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        # Create a Sequential container for the layers\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def training_model(x_train, y_train, x_test, y_test, neuron_number,learning_rate, momentum, bs, reg_coeff, activation, layers, dropout, factor, lr_patience, optimiz, num_epochs=1000, plot_curves=False, return_history=False):\n",
    "    \"\"\"\n",
    "    Train the regression model and evaluate it on a test (can also be validation, depending on the context).\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    - x_train (torch.Tensor): Training input data.\n",
    "    - y_train (torch.Tensor): Training target data.\n",
    "    - x_test (torch.Tensor): Test/Validation input data.\n",
    "    - y_test (torch.Tensor): Test/Validation target data.\n",
    "    - neuron_number (int): Total number of neurons in the network.\n",
    "    - learning_rate (float): Initial learning rate for the optimizer.\n",
    "    - momentum (float): Momentum factor for optimizers that use it.\n",
    "    - bs (int): Batch size for training.\n",
    "    - reg_coeff (float): Regularization coefficient for weight decay.\n",
    "    - activation (torch.nn.Module): Activation function used in the model.\n",
    "    - layers (int): Number of hidden layers in the model.\n",
    "    - dropout (float): Dropout rate for regularization.\n",
    "    - factor (float): Factor by which the learning rate will be reduced.\n",
    "    - lr_patience (int): Number of epochs with no improvement after which learning rate will be reduced.\n",
    "    - optimiz (str): Optimizer to use ('SGD', 'Adam', or 'RMS').\n",
    "    - num_epochs (int, optional): Total number of training epochs (default: 1000).\n",
    "    - plot_curves (bool, optional): If True, plots training and test(validation) loss and MEE curves (default: False).\n",
    "    - return_history (bool, optional): If True, returns the training history (default: False).\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Depending on 'return_history', returns either the final model and its performance metrics\n",
    "      (model, number of epochs, final training loss, final test loss, final training MEE, final test MEE),\n",
    "      or the model, performance metrics, and the training history\n",
    "      (model, number of epochs, final training loss, final test loss, final training MEE, final test MEE,\n",
    "      array of training losses, array of test losses, array of training MEEs, array of test MEEs).\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_size = int(neuron_number/layers)\n",
    "    # Create an instance of the model\n",
    "    model = RegressorNN(hidden_size=hidden_size, dropout_prob=dropout, activation_function=activation, num_layers=layers)\n",
    "    model.to(device)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    if optimiz == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=reg_coeff)\n",
    "    if optimiz == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg_coeff)\n",
    "    if optimiz == 'RMS':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=reg_coeff)\n",
    "\n",
    "    # Set the initial learning rate and create a learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=lr_patience, verbose=True)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "    # Lists to store training and test losses for plotting\n",
    "    train_losses, test_losses, train_mees, test_mees = [], [], [], []\n",
    "\n",
    "    # parameters to stop at training convergence\n",
    "    min_mee = float('inf')\n",
    "    patience_counter, patience = 0, 20\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        for inputs, labels in train_dataloader:\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels) #Compute the loss\n",
    "\n",
    "            optimizer.zero_grad()   # Zero the gradients\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        #calculate loss\n",
    "        y_pred = model(x_train)\n",
    "        train_loss = criterion(y_pred, y_train)\n",
    "        # Calculate mee\n",
    "        train_mee = mean_euclidean_error(y_pred,y_train)\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_mees.append(train_mee)\n",
    "\n",
    "        # Evaluation on the test/validation set\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(x_test)\n",
    "            test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "            # Calculate test mee\n",
    "            test_mee = mean_euclidean_error(test_outputs,y_test)\n",
    "            test_mees.append(test_mee)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}],Learning Rate: {optimizer.param_groups[0][\"lr\"]}, Loss: {train_loss.item():.4f}, '\n",
    "            f'MEE: {train_mee:.4f} | Test - Epoch [{epoch+1}/{num_epochs}], '\n",
    "            f'Loss: {test_loss.item():.4f} MEE: {test_mee:.4f} ', end='\\r')\n",
    "\n",
    "        # Check for convergence\n",
    "        if train_mee < min_mee and abs(train_mee-min_mee)>1e-3:\n",
    "            patience_counter = 0\n",
    "            min_mee = train_mee\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter == patience:\n",
    "        #    print(f\"Convergence reached. at epoch {epoch+1} Stopping training.\")\n",
    "            break\n",
    "\n",
    "        # Update the learning rate using the scheduler\n",
    "        scheduler.step(train_mee)\n",
    "\n",
    "    print(f'N. Epochs = {epoch+1} - Loss (train | test/val )= ({train_loss.item():.4} | {test_loss.item():.4} ) - MEE (train | test/val ) = ({train_mee:.4} | {test_mee:.4} )')\n",
    "\n",
    "    if plot_curves:\n",
    "        hyperparams = [hidden_size,learning_rate, momentum, bs, reg_coeff, activation, layers, dropout, factor,lr_patience, num_epochs]\n",
    "        plot_training_curves(train_losses, test_losses, train_mees, test_mees, hyperparams)\n",
    "\n",
    "    if return_history:\n",
    "        return model, epoch+1, train_loss.item(), test_loss.item(), train_mee, test_mee, np.array(train_losses), np.array(test_losses), np.array(train_mees), np.array(test_mees)\n",
    "    else:\n",
    "        return model, epoch+1, train_loss.item(), test_loss.item(), train_mee, test_mee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_history(N,hist):\n",
    "    \"\"\"\n",
    "    Truncate each history in a list to the length of the shortest history.\n",
    "\n",
    "    Parameters:\n",
    "    - N (int): Number of histories in the list.\n",
    "    - hist (list of lists): A list where each element is a history (list of values like losses or accuracies).\n",
    "\n",
    "    Returns:\n",
    "    - list of lists: A list of histories, each truncated to the length of the shortest history in the original list.\n",
    "\n",
    "    Each history in the list 'hist' is truncated to the length of the shortest history. This is needed because\n",
    "    histories of different lengths are collected and they need to be averaged.\n",
    "    \"\"\"\n",
    "    min_len = float('inf')\n",
    "    for i in range(0,N):\n",
    "        if len(hist[i]) < min_len:\n",
    "            min_len = len(hist[i])\n",
    "\n",
    "    for i in range(0,N):\n",
    "        hist[i]=hist[i][:min_len]\n",
    "\n",
    "    return hist\n",
    "\n",
    "def train_with_different_initializations(x_train, y_train, x_test, y_test,\n",
    "                                         neuron_number,learning_rate, momentum,\n",
    "                                         bs, reg_coeff, activation,layers, dropout, factor, lr_patience,optimiz, max_num_epochs=1000,\n",
    "                                         plot_curves=False, N = 5, return_history=False):\n",
    "    \"\"\"\n",
    "    Train the model multiple times with different weight initializations to estimate performance mean and variance.\n",
    "\n",
    "    Parameters:\n",
    "    - x_train (torch.Tensor): Training input data.\n",
    "    - y_train (torch.Tensor): Training target data.\n",
    "    - x_test (torch.Tensor): Test input data.\n",
    "    - y_test (torch.Tensor): Test target data.\n",
    "    - neuron_number (int): Total number of neurons across all layers.\n",
    "    - learning_rate (float): Learning rate for the optimizer.\n",
    "    - momentum (float): Momentum for the optimizer.\n",
    "    - bs (int): Batch size for training.\n",
    "    - reg_coeff (float): Regularization coefficient for weight decay.\n",
    "    - activation (torch.nn.Module): Activation function for the model.\n",
    "    - layers (int): Number of hidden layers in the model.\n",
    "    - dropout (float): Dropout rate for regularization.\n",
    "    - factor (float): Factor by which the learning rate will be reduced.\n",
    "    - lr_patience (int): Number of epochs with no improvement after which learning rate will be reduced.\n",
    "    - optimiz (str): Optimizer to use ('SGD', 'Adam', 'RMSprop').\n",
    "    - max_num_epochs (int): Maximum number of training epochs.\n",
    "    - plot_curves (bool): Flag to plot training and validation loss and MEE curves.\n",
    "    - N (int): Number of times to train the model with different initializations.\n",
    "    - return_history (bool): Flag to return the history of losses and MEEs.\n",
    "\n",
    "    Returns:\n",
    "    If return_history is True:\n",
    "    - models (list): List of trained models for each initialization.\n",
    "    - train_losses_hist (list): List of training loss histories for each model.\n",
    "    - test_losses_hist (list): List of test loss histories for each model.\n",
    "    - train_mees_hist (list): List of training MEE histories for each model.\n",
    "    - test_mees_hist (list): List of test MEE histories for each model.\n",
    "\n",
    "    If return_history is False:\n",
    "    - Mean of the final training loss across all initializations.\n",
    "    - Mean of the final test loss across all initializations.\n",
    "    - Mean of the final training MEE across all initializations.\n",
    "    - Mean of the final test MEE across all initializations.\n",
    "\n",
    "    Prints the mean and standard deviation of training and test loss, as well as training and test Mean Euclidean Error (MEE).\n",
    "    \"\"\"\n",
    "    final_train_loss = []\n",
    "    final_test_loss = []\n",
    "    final_train_mee = []\n",
    "    final_test_mee = []\n",
    "    train_losses_hist, test_losses_hist, train_mees_hist, test_mees_hist = [[] for _ in range(N)],  [[] for _ in range(N)],  [[] for _ in range(N)],  [[] for _ in range(N)]\n",
    "    models = []\n",
    "    for i in range (0,N):\n",
    "        model , num_epochs,train_loss, test_loss, train_mee, test_mee, train_losses_hist[i], test_losses_hist[i], train_mees_hist[i], test_mees_hist[i] = training_model(x_train,\n",
    "                        y_train,\n",
    "                        x_test,\n",
    "                        y_test,\n",
    "                        neuron_number,\n",
    "                        learning_rate,\n",
    "                        momentum,\n",
    "                        bs,\n",
    "                        reg_coeff,\n",
    "                        activation,\n",
    "                        layers,\n",
    "                        dropout,\n",
    "                        factor,\n",
    "                        lr_patience,\n",
    "                        optimiz,\n",
    "                        plot_curves=False,\n",
    "                        num_epochs=max_num_epochs,\n",
    "                        return_history=True)\n",
    "\n",
    "\n",
    "        final_train_loss.append(train_loss)\n",
    "        final_test_loss.append(test_loss)\n",
    "        final_train_mee.append(train_mee)\n",
    "        final_test_mee.append(test_mee)\n",
    "        models.append(model)\n",
    "\n",
    "    if plot_curves:\n",
    "        hyperparams = [int(neuron_number/layers),learning_rate, momentum, bs, reg_coeff, activation, layers, dropout, num_epochs]\n",
    "        plot_training_curves(train_losses=train_losses_hist[i],test_losses=test_losses_hist[i],train_mees=train_mees_hist[i],test_mees=test_mees_hist[i],hyperparams=hyperparams)\n",
    "\n",
    "    plt.show()\n",
    "    print(f'Avg of {N} initializations: Loss (train | test/val )= ({np.mean(final_train_loss):.4} +- {np.std(final_train_loss):.4} | {np.mean(final_test_loss):.4} +- {np.std(final_test_loss):.4})'\n",
    "          f'- MEE (train | test/val ) = ( {np.mean(final_train_mee):.4} +-  {np.std(final_train_mee):.4} | {np.mean(final_test_mee):.4} +- {np.std(final_test_mee):.4})')\n",
    "\n",
    "    if return_history:\n",
    "        train_losses_hist = reshape_history(N,train_losses_hist)\n",
    "        test_losses_hist = reshape_history(N,test_losses_hist)\n",
    "        train_mees_hist = reshape_history(N,train_mees_hist)\n",
    "        test_mees_hist = reshape_history(N,test_mees_hist)\n",
    "        return models, train_losses_hist, test_losses_hist, train_mees_hist, test_mees_hist\n",
    "    else:\n",
    "        return np.mean(final_train_loss), np.mean(final_test_loss), np.mean(final_train_mee), np.mean(final_test_mee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search_kfold(neuron_numbers, learning_rates, momentums, batch_sizes, reg_coeffs, activations, layerss, dropouts,factors, lr_patiences, optimiz, k_folds, x, y, plot_curves=False, num_epochs=1000, number_of_init=5):\n",
    "    \"\"\"\n",
    "    Perform grid search with k-fold cross-validation for hyperparameters.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    - neuron_numbers (list): List of total neuron numbers to try in the network.\n",
    "    - learning_rates (list): List of learning rates to try.\n",
    "    - momentums (list): List of momentum values to try for optimizers that use it.\n",
    "    - batch_sizes (list): List of batch sizes to try.\n",
    "    - reg_coeffs (list): List of regularization coefficients to try for weight decay.\n",
    "    - activations (list): List of activation functions to try.\n",
    "    - layerss (list): List of numbers of layers to try.\n",
    "    - dropouts (list): List of dropout rates to try for regularization.\n",
    "    - factors (list): List of factors for reducing learning rate on plateau.\n",
    "    - lr_patiences (list): List of patience values for reducing learning rate on plateau.\n",
    "    - optimiz (str): Type of optimizer to use.\n",
    "    - k_folds (int): Number of folds to use in k-fold cross-validation.\n",
    "    - x (numpy.ndarray): Input feature data.\n",
    "    - y (numpy.ndarray): Target data.\n",
    "    - plot_curves (bool, optional): Whether to plot training and validation loss/MEE curves (default: False).\n",
    "    - num_epochs (int, optional): Maximum number of training epochs for each model (default: 1000).\n",
    "    - number_of_init (int): Number of different initializations to try for each hyperparameter set.\n",
    "\n",
    "    Returns:\n",
    "    - list: Best hyperparameters found through grid search and k-fold cross-validation, based on validation MEE.\n",
    "\n",
    "\n",
    "    The function performs grid search with k-fold cross-validation for Monk classifier hyperparameters and returns the best hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    best_mee = float('inf')\n",
    "    best_hyperparams = []\n",
    "\n",
    "    for neuron_number, learning_rate, momentum, bs, reg_coeff, activation, layers, dropout, factor,lr_patience in product(neuron_numbers,learning_rates,momentums,batch_sizes, reg_coeffs, activations, layerss, dropouts, factors, lr_patiences):\n",
    "        print(f'activation={activation}; layers={layers}; neuron_number={neuron_number}; lr={learning_rate}; alpha = {momentum}; batch size = {bs}; lambda = {reg_coeff}; optim = {optimiz}; factor={factor};lr_patience={lr_patience}')\n",
    "\n",
    "        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        # Lists to store training and validation losses and MEEs for each fold\n",
    "        train_losses, val_losses, train_mees, val_mees = [], [], [], []\n",
    "\n",
    "        # Perform K-fold cross-validation\n",
    "        for fold, (train_indices, val_indices) in enumerate(kf.split(x,y)):\n",
    "\n",
    "            # Split the data into training and validation (or test) sets\n",
    "            X_train, X_val = x[train_indices], x[val_indices]\n",
    "            Y_train, Y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "            train_loss, val_loss, train_mee, val_mee = train_with_different_initializations(\n",
    "                x_train=X_train, y_train=Y_train, x_test=X_val, y_test=Y_val,\n",
    "                neuron_number=neuron_number, learning_rate=learning_rate, momentum=momentum,\n",
    "                bs=bs, reg_coeff=reg_coeff, activation=activation, layers=layers,dropout=dropout,factor=factor, lr_patience=lr_patience, optimiz=optimiz ,plot_curves=plot_curves, max_num_epochs=num_epochs, N=number_of_init)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_mees.append(train_mee)\n",
    "            val_mees.append(val_mee)\n",
    "\n",
    "        print(f'Final Results: activation={activation}; layers={layers}; neuron number={neuron_number}; lr={learning_rate}; alpha = {momentum}; batch size = {bs}; lambda = {reg_coeff} --> '\n",
    "            f'train_loss = {np.mean(train_losses):.4} +- {np.std(train_losses):.4} | '\n",
    "            f'val_loss = {np.mean(val_losses):.4} +- {np.std(val_losses):.4}'\n",
    "            f'train_mee = {np.mean(train_mees):.4} +- {np.std(train_mees):.4} | '\n",
    "            f'val_mee = {np.mean(val_mees):.4} +- {np.std(val_mees):.4}')\n",
    "\n",
    "        if np.mean(val_mees) < best_mee:\n",
    "\n",
    "            best_mee = np.mean(val_mees)\n",
    "            best_hyperparams = [neuron_number, learning_rate, momentum, bs, reg_coeff, activation, layers, dropout, factor, lr_patience]\n",
    "\n",
    "    print(best_hyperparams,'n Best val MEE:',best_mee)\n",
    "    return best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_std(N,train_hist, test_hist,label):\n",
    "    \"\"\"\n",
    "    Plot the mean and standard deviation of training and testing metrics over epochs.\n",
    "\n",
    "    Parameters:\n",
    "    - N (int): The number of different initializations or runs.\n",
    "    - train_hist (list of lists): A list containing N lists of training metrics, each corresponding to a different run.\n",
    "    - test_hist (list of lists): A list containing N lists of testing metrics, each corresponding to a different run.\n",
    "    - label (str): A label for the metric being plotted (e.g., 'Loss' or 'Accuracy').\n",
    "\n",
    "    The function calculates the mean and standard deviation across runs for both training and testing metrics at each epoch.\n",
    "    It then plots these as two subplots: one showing the entire range of the metric, and another zoomed in on the lower values to better visualize differences when metrics are small.\n",
    "\n",
    "    The plotted lines show the mean value, and the shaded areas represent one standard deviation above and below the mean.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    mean_tr = np.array(train_hist).mean(axis=0)\n",
    "    std_tr = np.array(train_hist).std(axis=0)\n",
    "    mean_te = np.array(test_hist).mean(axis=0)\n",
    "    std_te = np.array(test_hist).std(axis=0)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(mean_tr, label=f'Training {label} (mean $\\pm$ std)', color = 'maroon', linewidth=1)\n",
    "    plt.fill_between(range(0,len(train_hist[0])),mean_tr-std_tr, mean_tr+std_tr,  color='lightcoral', alpha=0.5)\n",
    "\n",
    "    plt.plot(mean_te, label=f'Test {label} (mean $\\pm$ std)', color = 'blue', linestyle='--', linewidth=1)\n",
    "    plt.fill_between(range(0,len(test_hist[0])),mean_te-std_te, mean_te+std_te, color='cadetblue', alpha=0.3)\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(label)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "\n",
    "    plt.plot(mean_tr, label=f'Training {label} (mean $\\pm$ std)', color = 'maroon', linewidth=1)\n",
    "    plt.fill_between(range(0,len(train_hist[0])),mean_tr-std_tr, mean_tr+std_tr, color='lightcoral', alpha=0.5)\n",
    "\n",
    "    plt.plot(mean_te, label=f'Test {label} (mean $\\pm$ std)', color = 'blue', linestyle='--', linewidth=1)\n",
    "    plt.fill_between(range(0,len(test_hist[0])),mean_te-std_te, mean_te+std_te, color='cadetblue', alpha=0.3)\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(label)\n",
    "    plt.ylim(0,10)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('ML-CUP23-TR.csv', delimiter=',')\n",
    "X = dataset[:,1:11]\n",
    "y = dataset[:,11:14]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "#to use GPU\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# Split the data into training and testing sets (80%/20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERFORM GRID SEARCH TO FIND BEST HYPERPARAMETERS\n",
    "- We compare different optimizers\n",
    "- First we keep things simple and don't implement any decay in the lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#study of the effect of activation functions (compare tanh and relu)\n",
    "'''\n",
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [1e-4]\n",
    "momentums = [0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh(),nn.ReLU()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "optimiz = 'SGD' #either 'SGD' or 'Adam'or 'RMS'\n",
    "'''\n",
    "\n",
    "#study of the effect of regularization\n",
    "'''\n",
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [1e-4]\n",
    "momentums = [0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-2,1e-3,1e-4,1e-5]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "optimiz = 'SGD' #either 'SGD' or 'Adam' or 'RMS'\n",
    "'''\n",
    "\n",
    "#coarse grid search SGD\n",
    "'''\n",
    "hidden_neurons = [100,1000] #total number of neurons\n",
    "learning_rates = [1e-3,1e-4,1e-5]\n",
    "momentums = [0.5,0.8] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [1,2,3]\n",
    "dropouts = [0]\n",
    "optimiz = 'SGD' #either 'SGD' or 'Adam' or 'RMS'\n",
    "'''\n",
    "\n",
    "#refined grid search SGD\n",
    "'''\n",
    "hidden_neurons = [1000,2000,3000] #total number of neurons\n",
    "learning_rates = [1e-4]\n",
    "momentums = [0.7,0.8,0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "optimiz = 'SGD' #either 'SGD' or 'Adam' or 'RMS'\n",
    "'''\n",
    "\n",
    "#coarse grid search Adam\n",
    "'''\n",
    "hidden_neurons = [100,1000] #total number of neurons\n",
    "learning_rates = [1e-4,1e-5]\n",
    "momentums = [0] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [1,2,3]\n",
    "dropouts = [0]\n",
    "optimiz = 'Adam' #either 'SGD' or 'Adam' or 'RMS'\n",
    "'''\n",
    "\n",
    "#finer grid search Adam\n",
    "'''\n",
    "hidden_neurons = [1000,2000,3000] #total number of neurons\n",
    "learning_rates = [1e-4,2e-4,3e-4]\n",
    "momentums = [0] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "optimiz = 'Adam' #either 'SGD' or 'Adam' or 'RMS\n",
    "'''\n",
    "\n",
    "#coarse grid search RMSProp\n",
    "'''\n",
    "hidden_neurons = [100,1000] #total number of neurons\n",
    "learning_rates = [1e-4,1e-5]\n",
    "momentums = [0.7,0.8,0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [1,2,3]\n",
    "dropouts = [0]\n",
    "optimiz = 'RMS' #either 'SGD' or 'Adam' or 'RMS'\n",
    "'''\n",
    "\n",
    "#refined grid search RMSProp\n",
    "'''\n",
    "hidden_neurons = [1000,2000,3000] #total number of neurons\n",
    "learning_rates = [1e-5,2e-5,3e-5]\n",
    "momentums = [0.85,0.90,0.95] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [1,2,3]\n",
    "dropouts = [0]\n",
    "optimiz = 'RMS' #either 'SGD' or 'Adam' or 'RMS'\n",
    "'''\n",
    "\n",
    "\n",
    "#final best hyperparameters found (without reducelronplateau)\n",
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [1e-5]\n",
    "momentums = [0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "optimiz = 'RMS' #either 'SGD' or 'Adam' or 'RMS'\n",
    "\n",
    "\n",
    "best_hp = perform_grid_search_kfold(hidden_neurons,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    activations,\n",
    "                                    layerss,\n",
    "                                    dropouts,\n",
    "                                    [0], #this value doesn't matter because:\n",
    "                                    [1e10], #we don't want reducelronplateau to activate\n",
    "                                    optimiz,\n",
    "                                    k_folds=3,\n",
    "                                    x=X_train,\n",
    "                                    y=y_train,\n",
    "                                    num_epochs=5000,\n",
    "                                    number_of_init=3,\n",
    "                                    plot_curves=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReduceLROnPlateau\n",
    "Try to implement it and perform some grid searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD\n",
    "'''\n",
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [1e-4,2e-4,3e-4]\n",
    "momentums = [0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "optimiz = 'SGD' #either 'SGD' or 'Adam'or 'RMS'\n",
    "#hyperparameters for reducelronplateau:\n",
    "factors=[0.5,0.6,0.7,0.8,0.9]\n",
    "lr_patiences=[5,10,15]\n",
    "'''\n",
    "\n",
    "#Adam\n",
    "'''\n",
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [1e-4,2e-4,3e-4]\n",
    "momentums = [0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "optimiz = 'Adam' #either 'SGD' or 'Adam'or 'RMS'\n",
    "#hyperparameters for reducelronplateau:\n",
    "factors=[0.5,0.6,0.7,0.8,0.9]\n",
    "lr_patiences=[5,10,15]\n",
    "'''\n",
    "\n",
    "#RMSProp\n",
    "'''\n",
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [1e-5,2e-5,3e-5]\n",
    "momentums = [0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "optimiz = 'RMS' #either 'SGD' or 'Adam'or 'RMS'\n",
    "#hyperparameters for reducelronplateau:\n",
    "factors=[0.5,0.6,0.7,0.8,0.9]\n",
    "lr_patiences=[5,10,15]\n",
    "'''\n",
    "\n",
    "#Best configuration so far:\n",
    "#RMSProp\n",
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [1e-5]\n",
    "momentums = [0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "optimiz = 'RMS' #either 'SGD' or 'Adam'or 'RMS'\n",
    "#hyperparameters for reducelronplateau:\n",
    "factors=[0.5]\n",
    "lr_patiences=[15]\n",
    "\n",
    "\n",
    "best_hp = perform_grid_search_kfold(hidden_neurons,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    activations,\n",
    "                                    layerss,\n",
    "                                    dropouts,\n",
    "                                    factors,\n",
    "                                    lr_patiences,\n",
    "                                    optimiz,\n",
    "                                    k_folds=3,\n",
    "                                    x=X_train,\n",
    "                                    y=y_train,\n",
    "                                    num_epochs=5000,\n",
    "                                    number_of_init=3,\n",
    "                                    plot_curves=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN THE FINAL MODEL ON ALL THE TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = 1000 #total number of neurons\n",
    "learning_rate = 1e-5\n",
    "momentum = 0.9 #if optimiz = 'Adam' it doesn't matter\n",
    "batch_size = 128\n",
    "reg_coeff = 1e-3\n",
    "activation = nn.Tanh()\n",
    "layers = 3\n",
    "dropout = 0\n",
    "optimiz = 'RMS' #either 'SGD' or 'Adam' or 'RMS\n",
    "factor = 0.5\n",
    "lr_patience = 15\n",
    "\n",
    "models, train_losses_hist, test_losses_hist, train_mees_hist, test_mees_hist = train_with_different_initializations(x_train=X_train,\n",
    "               y_train=y_train,\n",
    "               x_test=X_test,\n",
    "               y_test=y_test,\n",
    "               neuron_number=hidden_neurons,\n",
    "               learning_rate=learning_rate,\n",
    "               momentum=momentum,\n",
    "               bs = batch_size,\n",
    "               reg_coeff= reg_coeff,\n",
    "               activation= activation,\n",
    "               layers= layers,\n",
    "               dropout= dropout,\n",
    "               factor= factor,\n",
    "               lr_patience=lr_patience,\n",
    "               optimiz=optimiz,\n",
    "               max_num_epochs=5000,\n",
    "               plot_curves=False,\n",
    "               return_history=True,\n",
    "               N=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "# Loss plots\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train_losses_hist[0], label='Training MSE', color = 'red')\n",
    "plt.plot(test_losses_hist[0], label='Test MSE', color = 'blue', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "# MEE plots\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train_mees_hist[0], label='Training MEE', color='red')\n",
    "plt.plot(test_mees_hist[0], label='Test MEE', color = 'blue', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MEE')\n",
    "plt.legend()\n",
    "\n",
    "# Loss plots\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(train_losses_hist[0], label='Training MSE', color = 'red')\n",
    "plt.plot(test_losses_hist[0], label='Test MSE', color = 'blue', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.ylim(0,10)\n",
    "plt.legend()\n",
    "\n",
    "# MEE plots\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(train_mees_hist[0], label='Training MEE', color='red')\n",
    "plt.plot(test_mees_hist[0], label='Test MEE', color = 'blue', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MEE')\n",
    "plt.ylim(0,10)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance from different initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = 1000 #total number of neurons\n",
    "learning_rate = 1e-5\n",
    "momentum = 0.9 #if optimiz = 'Adam' it doesn't matter\n",
    "batch_size = 128\n",
    "reg_coeff = 1e-3\n",
    "activation = nn.Tanh()\n",
    "layers = 3\n",
    "dropout = 0\n",
    "optimiz = 'RMS' #either 'SGD' or 'Adam' or 'RMS\n",
    "factor = 0.5\n",
    "lr_patience = 15\n",
    "\n",
    "models, train_losses_hist, test_losses_hist, train_mees_hist, test_mees_hist = train_with_different_initializations(x_train=X_train,\n",
    "               y_train=y_train,\n",
    "               x_test=X_test,\n",
    "               y_test=y_test,\n",
    "               neuron_number=hidden_neurons,\n",
    "               learning_rate=learning_rate,\n",
    "               momentum=momentum,\n",
    "               bs = batch_size,\n",
    "               reg_coeff= reg_coeff,\n",
    "               activation= activation,\n",
    "               layers= layers,\n",
    "               dropout= dropout,\n",
    "               factor= factor,\n",
    "               lr_patience=lr_patience,\n",
    "               optimiz=optimiz,\n",
    "               max_num_epochs=5000,\n",
    "               plot_curves=False,\n",
    "               return_history=True,\n",
    "               N=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_std(N=5,train_hist=train_losses_hist,test_hist=test_losses_hist,label='MSE')\n",
    "plot_mean_std(N=5,train_hist=train_mees_hist,test_hist=test_mees_hist,label='MEE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
