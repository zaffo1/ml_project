{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_monk(file_name):\n",
    "    '''\n",
    "    function to load data from the monk dataset\n",
    "    and preproces using one-hot encoding\n",
    "    '''\n",
    "\n",
    "    # load the dataset, split into input (X) and output (y) variables\n",
    "    df = pd.read_csv(file_name, delimiter=' ', header=None, names=['remove_this_column','target', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'data_number'], index_col=False)\n",
    "    df.drop(columns='remove_this_column')\n",
    "\n",
    "    x1 = np.array(df['x1'])\n",
    "    x2 = np.array(df['x2'])\n",
    "    x3 = np.array(df['x3'])\n",
    "    x4 = np.array(df['x4'])\n",
    "    x5 = np.array(df['x5'])\n",
    "    x6 = np.array(df['x6'])\n",
    "    target = np.array(df['target'])\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    x1_ =x1.reshape(-1, 1)\n",
    "\n",
    "    # Fit and transform the data to one-hot encoding\n",
    "    input_one_hot = encoder.fit_transform(x1_)\n",
    "    inputs = [x2,x3,x4,x5,x6]\n",
    "\n",
    "    for x in inputs:\n",
    "        data =x.reshape(-1, 1)\n",
    "        # Fit and transform the data to one-hot encoding\n",
    "        one_hot_encoded = encoder.fit_transform(data)\n",
    "        input_one_hot = np.hstack((input_one_hot, one_hot_encoded))\n",
    "\n",
    "    x = torch.tensor(input_one_hot, dtype=torch.float32)#.cuda()\n",
    "    y = torch.tensor(target, dtype=torch.float32).reshape(-1,1)#.cuda()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = self.sigmoid(x)\n",
    "        return output\n",
    "\n",
    "# Set hyperparameters\n",
    "input_size = 17\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(x_train, y_train, x_test, y_test, hidden_size,learning_rate, momentum, bs, reg_coeff, num_epochs=1000, plot_curves=False):\n",
    "    '''\n",
    "    function that performs training of the model\n",
    "    '''\n",
    "\n",
    "    # Create an instance of the model\n",
    "    model = SimpleClassifier(input_size, hidden_size, output_size)\n",
    "    #model.to(device)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=reg_coeff)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "    if plot_curves:\n",
    "        # Lists to store training and test losses for plotting\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_accuracies = []\n",
    "        test_accuracies = []\n",
    "\n",
    "    # parameters to stop at training convergence\n",
    "    prev_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        # Mini-batch training\n",
    "\n",
    "        model.train()  # Set the model to training mode\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()   # Zero the gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)   # Compute the loss\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step()    # Update weights\n",
    "\n",
    "\n",
    "            prev_loss = loss.item()\n",
    "            # Accumulate the total loss for this epoch\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            predicted_labels = torch.round(outputs)\n",
    "            correct_predictions = (predicted_labels == labels).sum().item()\n",
    "            total_samples = labels.size(0)\n",
    "            total_accuracy += correct_predictions / total_samples\n",
    "\n",
    "        # Print average training loss and accuracy for the epoch\n",
    "        average_loss = total_loss / len(train_dataloader)\n",
    "        average_accuracy = total_accuracy / len(train_dataloader)\n",
    "\n",
    "        #print(f'Training - Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}, '\n",
    "        #      f'Accuracy: {average_accuracy:.4f}')\n",
    "\n",
    "        if plot_curves:\n",
    "            train_losses.append(average_loss)\n",
    "            train_accuracies.append(average_accuracy)\n",
    "\n",
    "        # Evaluation on the test set\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            total_test_loss = 0.0\n",
    "            total_test_accuracy = 0.0\n",
    "            for test_inputs, test_labels in test_dataloader:\n",
    "                test_outputs = model(test_inputs)\n",
    "                test_loss = criterion(test_outputs, test_labels)\n",
    "                total_test_loss += test_loss.item()\n",
    "\n",
    "                # Calculate test accuracy\n",
    "                predicted_labels = torch.round(test_outputs)\n",
    "                correct_predictions = (predicted_labels == test_labels).sum().item()\n",
    "                total_samples = test_labels.size(0)\n",
    "                total_test_accuracy += correct_predictions / total_samples\n",
    "\n",
    "            average_test_loss = total_test_loss / len(test_dataloader)\n",
    "            average_test_accuracy = total_test_accuracy / len(test_dataloader)\n",
    "\n",
    "            #print(f'Test - Epoch [{epoch+1}/{num_epochs}], Loss: {average_test_loss:.4f},'\n",
    "            #       f'Accuracy: {average_test_accuracy:.4f}')\n",
    "\n",
    "            if plot_curves:\n",
    "                test_accuracies.append(average_test_accuracy)\n",
    "                test_losses.append(average_test_loss)\n",
    "\n",
    "        # Check for convergence\n",
    "        if abs(prev_loss - average_loss) < 1e-3:\n",
    "            patience_counter += 1\n",
    "        else:\n",
    "            patience_counter = 0\n",
    "\n",
    "        if patience_counter == patience:\n",
    "        #    print(f\"Convergence reached. at epoch {epoch+1} Stopping training.\")\n",
    "            break\n",
    "        #epochs_reached = epoch+2\n",
    "        prev_loss = average_loss\n",
    "\n",
    "    print(f'N. Epochs = {epoch+1} - Loss (train | test)= ({average_loss:.4} | {average_test_loss:.4}) - Accuracy (train | test) = ({average_accuracy} | {average_test_accuracy})')\n",
    "\n",
    "    if plot_curves:\n",
    "        # Plot the training and test losses\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        # Loss plots\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(1, epoch + 2), train_losses, label='Training Loss')\n",
    "        plt.plot(range(1, epoch + 2), test_losses, label='Test Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Accuracy plots\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(1, epoch + 2), train_accuracies, label='Training Accuracy')\n",
    "        plt.plot(range(1, epoch + 2), test_accuracies, label='Test Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return epoch+1, average_loss, average_test_loss, average_accuracy, average_test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monk 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([124, 17])\n",
      "torch.Size([124, 1])\n",
      "torch.Size([432, 17])\n",
      "torch.Size([432, 1])\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = preprocess_monk(file_name='monk_data/monks-1.train')\n",
    "x_test, y_test = preprocess_monk(file_name='monk_data/monks-1.test')\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 5\n",
    "learning_rate = 0.2\n",
    "momentum = 0.5\n",
    "bs = 8\n",
    "reg_coeff = 0\n",
    "\n",
    "training_model(x_train=x_train,\n",
    "               y_train=y_train,\n",
    "               x_test=x_test,\n",
    "               y_test=y_test,\n",
    "               hidden_size=hidden_size,\n",
    "               learning_rate=learning_rate,\n",
    "               momentum=momentum,\n",
    "               bs = bs,\n",
    "               reg_coeff= reg_coeff,\n",
    "               plot_curves=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size=2; lr=0.2; alpha = 0.5; batch size = 8; lambda = 0\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.04822 | 0.003015) - Accuracy (train | test) = (0.9519230769230769 | 1.0)\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.07645 | 0.1465) - Accuracy (train | test) = (0.9230769230769231 | 0.84375)\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.05592 | 0.01495) - Accuracy (train | test) = (0.9230769230769231 | 1.0)\n",
      "N. Epochs = 143 - Loss (train | test)= (0.001532 | 0.0018) - Accuracy (train | test) = (1.0 | 1.0)\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.0482 | 0.1251) - Accuracy (train | test) = (0.9519230769230769 | 0.875)\n",
      "hidden_size=2; lr=0.2; alpha = 0.5; batch size = 8; lambda = 0 --> train_loss = 0.04606 +- 0.02455 | val_loss = 0.05827 +- 0.06383\n",
      "hidden_size=2; lr=0.2; alpha = 0.8; batch size = 8; lambda = 0\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.04811 | 0.09379) - Accuracy (train | test) = (0.9519230769230769 | 0.90625)\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.08567 | 0.2115) - Accuracy (train | test) = (0.9134615384615384 | 0.78125)\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.04491 | 0.06253) - Accuracy (train | test) = (0.9551282051282051 | 0.9375)\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.05122 | 0.03272) - Accuracy (train | test) = (0.9326923076923077 | 0.96875)\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.04811 | 0.125) - Accuracy (train | test) = (0.9519230769230769 | 0.875)\n",
      "hidden_size=2; lr=0.2; alpha = 0.8; batch size = 8; lambda = 0 --> train_loss = 0.05561 +- 0.01517 | val_loss = 0.1051 +- 0.06148\n",
      "hidden_size=2; lr=0.1; alpha = 0.5; batch size = 8; lambda = 0\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.04826 | 0.09395) - Accuracy (train | test) = (0.9519230769230769 | 0.90625)\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.05895 | 0.1708) - Accuracy (train | test) = (0.9230769230769231 | 0.78125)\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.04515 | 0.06278) - Accuracy (train | test) = (0.9551282051282051 | 0.9375)\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.0548 | 0.03189) - Accuracy (train | test) = (0.9455128205128205 | 0.96875)\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.04827 | 0.1252) - Accuracy (train | test) = (0.9519230769230769 | 0.875)\n",
      "hidden_size=2; lr=0.1; alpha = 0.5; batch size = 8; lambda = 0 --> train_loss = 0.05109 +- 0.005037 | val_loss = 0.09692 +- 0.0483\n",
      "hidden_size=2; lr=0.1; alpha = 0.8; batch size = 8; lambda = 0\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.009777 | 0.53) - Accuracy (train | test) = (0.9903846153846154 | 0.46875)\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.01007 | 0.1252) - Accuracy (train | test) = (0.9903846153846154 | 0.875)\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.1039 | 0.05706) - Accuracy (train | test) = (0.8942307692307693 | 0.9375)\n",
      "N. Epochs = 1000 - Loss (train | test)= (0.05061 | 0.03299) - Accuracy (train | test) = (0.9326923076923077 | 0.96875)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m X_train, X_val \u001b[38;5;241m=\u001b[39m x_train[train_indices], x_train[val_indices]\n\u001b[1;32m     31\u001b[0m Y_train, Y_val \u001b[38;5;241m=\u001b[39m y_train[train_indices], y_train[val_indices]\n\u001b[0;32m---> 33\u001b[0m max_epoch, average_loss, average_val_loss, average_accuracy, average_val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                                                                                                        \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                                                                                                        \u001b[49m\u001b[43mx_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                                                                                        \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                                                                                        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                                                                                        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                                                                                        \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                                                                                        \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                                                                                                        \u001b[49m\u001b[43mreg_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_coeff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                                                                                                        \u001b[49m\u001b[43mplot_curves\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(average_loss)\n\u001b[1;32m     45\u001b[0m val_losses\u001b[38;5;241m.\u001b[39mappend(average_val_loss)\n",
      "Cell \u001b[0;32mIn[104], line 37\u001b[0m, in \u001b[0;36mtraining_model\u001b[0;34m(x_train, y_train, x_test, y_test, hidden_size, learning_rate, momentum, bs, reg_coeff, num_epochs, plot_curves)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Mini-batch training\u001b[39;00m\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     38\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()   \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Magistrale/ML/env_ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/Magistrale/ML/env_ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m~/Desktop/Magistrale/ML/env_ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:620\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Magistrale/ML/env_ml/lib/python3.10/site-packages/torch/utils/data/sampler.py:282\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m    281\u001b[0m idx_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler:\n\u001b[1;32m    283\u001b[0m     batch[idx_in_batch] \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m    284\u001b[0m     idx_in_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Magistrale/ML/env_ml/lib/python3.10/site-packages/torch/utils/data/sampler.py:153\u001b[0m, in \u001b[0;36mRandomSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mempty((), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\u001b[38;5;241m.\u001b[39mrandom_()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m--> 153\u001b[0m     generator \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_sizes = [2,5]\n",
    "learning_rates = [0.2, 0.1]\n",
    "momentums = [0.5,0.8]\n",
    "batch_sizes = [8]\n",
    "reg_coeffs = [0]\n",
    "\n",
    "\n",
    "best_acc = 0\n",
    "best_n_epochs = 10000  #initialize to high number\n",
    "best_hyperparams = []\n",
    "\n",
    "for hidden_size, learning_rate, momentum, bs, reg_coeff in product(hidden_sizes,learning_rates,momentums,batch_sizes, reg_coeffs):\n",
    "    print(f'hidden_size={hidden_size}; lr={learning_rate}; alpha = {momentum}; batch size = {bs}; lambda = {reg_coeff}')\n",
    "\n",
    "    # Define K-fold cross-validation\n",
    "    k_folds = 5\n",
    "    kf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Lists to store training and validation losses and accuracies for each epoch\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "\n",
    "    # Perform K-fold cross-validation\n",
    "    for fold, (train_indices, val_indices) in enumerate(kf.split(x_train,y_train)):\n",
    "        #print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "\n",
    "        # Split the data into training and validation sets\n",
    "        X_train, X_val = x_train[train_indices], x_train[val_indices]\n",
    "        Y_train, Y_val = y_train[train_indices], y_train[val_indices]\n",
    "\n",
    "        max_epoch, average_loss, average_val_loss, average_accuracy, average_val_accuracy = training_model(x_train=X_train,\n",
    "                                                                                                                y_train=Y_train,\n",
    "                                                                                                                x_test=X_val,\n",
    "                                                                                                                y_test=Y_val,\n",
    "                                                                                                                hidden_size=hidden_size,\n",
    "                                                                                                                learning_rate=learning_rate,\n",
    "                                                                                                                momentum=momentum,\n",
    "                                                                                                                bs = bs,\n",
    "                                                                                                                reg_coeff=reg_coeff,\n",
    "                                                                                                                plot_curves=False)\n",
    "\n",
    "        train_losses.append(average_loss)\n",
    "        val_losses.append(average_val_loss)\n",
    "        train_acc.append(average_accuracy)\n",
    "        val_acc.append(average_val_accuracy)\n",
    "\n",
    "    print(f'Final Results: hidden_size={hidden_size}; lr={learning_rate}; alpha = {momentum}; batch size = {bs}; lambda = {reg_coeff} --> '\n",
    "          f'train_loss = {np.mean(train_losses):.4} +- {np.std(train_losses):.4} | '\n",
    "          f'val_loss = {np.mean(val_losses):.4} +- {np.std(val_losses):.4}'\n",
    "          f'train_acc = {np.mean(train_acc):.4} +- {np.std(train_acc):.4} | '\n",
    "          f'val_acc = {np.mean(val_acc):.4} +- {np.std(val_acc):.4}')\n",
    "\n",
    "    if np.mean(val_acc) >= best_acc:\n",
    "        best_acc = np.mean(val_acc)\n",
    "        best_hyperparams = [hidden_size, learning_rate, momentum, bs, reg_coeff]\n",
    "\n",
    "print(best_hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
