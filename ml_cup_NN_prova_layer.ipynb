{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(epoch, train_losses, test_losses, train_mees, test_mees, hyperparams):\n",
    "    \"\"\"\n",
    "    Plot training and test curves for loss and Mean Euclidean Error (MEE).\n",
    "\n",
    "    Parameters:\n",
    "    - epoch (int): The total number of training epochs.\n",
    "    - train_losses (list): List of training losses for each epoch.\n",
    "    - test_losses (list): List of test losses for each epoch.\n",
    "    - train_mees (list): List of training MEE values for each epoch.\n",
    "    - test_mees (list): List of test MEE values for each epoch.\n",
    "    - hyperparams (list): List of hyperparameters used for the plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    Plots four subplots:\n",
    "    1. Training and test loss curves.\n",
    "    2. Training and test MEE curves.\n",
    "    3. Zoomed-in training and test loss curves with y-axis limit [0, 10].\n",
    "    4. Zoomed-in training and test MEE curves with y-axis limit [0, 10].\n",
    "\n",
    "    The hyperparameters are used in the plot title to provide additional context.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.suptitle(f'Batch Size={hyperparams[3]},Activation Function={hyperparams[5]}, Layers={hyperparams[6]} Hidden Units={hyperparams[0]}, Eta={hyperparams[1]}, Alpha={hyperparams[2]}, Lambda={hyperparams[4]}')\n",
    "    # Loss plots\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, epoch + 2), train_losses, label='Training Loss', color = 'red')\n",
    "    plt.plot(range(1, epoch + 2), test_losses, label='Test Loss', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # MEE plots\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, epoch + 2), train_mees, label='Training MEE', color='red')\n",
    "    plt.plot(range(1, epoch + 2), test_mees, label='Test MEE', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss plots\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(1, epoch + 2), train_losses, label='Training Loss', color = 'red')\n",
    "    plt.plot(range(1, epoch + 2), test_losses, label='Test Loss', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "\n",
    "    # MEE plots\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(1, epoch + 2), train_mees, label='Training MEE', color='red')\n",
    "    plt.plot(range(1, epoch + 2), test_mees, label='Test MEE', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_euclidean_error(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Compute the mean Euclidean error between two sets of 3D vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor1: PyTorch tensor of size (N, 3) representing the first set of 3D vectors\n",
    "    - tensor2: PyTorch tensor of size (N, 3) representing the second set of 3D vectors\n",
    "\n",
    "    Returns:\n",
    "    - mean_error: Mean Euclidean error between the two sets of vectors\n",
    "    \"\"\"\n",
    "    # Check if the tensors have the correct shape\n",
    "    if tensor1.shape[1] != 3 or tensor2.shape[1] != 3 or tensor1.shape[0] != tensor2.shape[0]:\n",
    "        raise ValueError(\"Input tensors must be of size (N, 3)\")\n",
    "\n",
    "\n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = torch.norm(tensor1 - tensor2, dim=1)\n",
    "\n",
    "    # Calculate the mean Euclidean error\n",
    "    mean_error = torch.mean(euclidean_distance)\n",
    "\n",
    "    return mean_error.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regression eural network\n",
    "\n",
    "class RegressorNN(nn.Module):\n",
    "    def __init__(self, hidden_sizes, activation_function, input_size=10, output_size=3):\n",
    "        super(RegressorNN, self).__init__()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers = [nn.Linear(input_size, hidden_sizes[0])]\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_sizes[0], hidden_sizes[1]))\n",
    "        self.layers.append(activation_function)\n",
    "        self.layers.append(nn.Linear(hidden_sizes[1], hidden_sizes[2]))\n",
    "        self.layers.append(activation_function)\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[2], output_size))\n",
    "\n",
    "        # Create a Sequential container for the layers\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(x_train, y_train, x_test, y_test, neuron_number,learning_rate, momentum, bs, reg_coeff, activation, optimiz, proportions, num_epochs=1000, plot_curves=False, return_history=False):\n",
    "    \"\"\"\n",
    "    Train the regression model and evaluate it on a test (can also be validation, depending on the context).\n",
    "\n",
    "    Parameters:\n",
    "    - x_train (torch.Tensor): Training input data.\n",
    "    - y_train (torch.Tensor): Training target data.\n",
    "    - x_test (torch.Tensor): Test input data.\n",
    "    - y_test (torch.Tensor): Test target data.\n",
    "    - neuron_number (int): Total number of neurons across all layers.\n",
    "    - learning_rate (float): Learning rate for the optimizer.\n",
    "    - momentum (float): Momentum for the optimizer.\n",
    "    - bs (int): Batch size for training.\n",
    "    - reg_coeff (float): Regularization coefficient for weight decay.\n",
    "    - activation (torch.nn.Module): Activation function for the model.\n",
    "    - layers (int): Number of hidden layers in the model.\n",
    "    - num_epochs (int, optional): Number of training epochs (default: 1000).\n",
    "    - plot_curves (bool, optional): Whether to plot training curves (default: False).\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Tuple containing the number of epochs, final training loss, final test loss, final training MEE, and final test MEE.\n",
    "\n",
    "    The function trains a neural network regression model using the specified hyperparameters and evaluates its performance on the test set.\n",
    "    \"\"\"\n",
    "    hidden_sizes = [0,0,0]\n",
    "\n",
    "    for i in range(len(proportions)):\n",
    "        hidden_sizes[i] = int(proportions[i]*neuron_number)\n",
    "        #print(hidden_sizes)\n",
    "    #print(hidden_size)\n",
    "    # Create an instance of the model\n",
    "    model = RegressorNN(hidden_sizes=hidden_sizes, activation_function=activation)\n",
    "    #model.to(device)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    if optimiz == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=reg_coeff)\n",
    "    if optimiz == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg_coeff)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "    # Lists to store training and test losses for plotting\n",
    "    train_losses, test_losses, train_mees, test_mees = [], [], [], []\n",
    "\n",
    "    # parameters to stop at training convergence\n",
    "    min_mee = float('inf')\n",
    "    patience_counter, patience = 0, 20\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        for inputs, labels in train_dataloader:\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels) #Compute the loss\n",
    "\n",
    "            optimizer.zero_grad()   # Zero the gradients\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        #calculate loss\n",
    "        y_pred = model(x_train)\n",
    "        train_loss = criterion(y_pred, y_train)\n",
    "        # Calculate mee\n",
    "        train_mee = mean_euclidean_error(y_pred,y_train)\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_mees.append(train_mee)\n",
    "\n",
    "        # Evaluation on the test/validation set\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(x_test)\n",
    "            test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "            # Calculate test mee\n",
    "            test_mee = mean_euclidean_error(test_outputs,y_test)\n",
    "            test_mees.append(test_mee)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "        print(f'Training - Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss.item():.4f}, '\n",
    "            f'MEE: {train_mee:.4f} | Test - Epoch [{epoch+1}/{num_epochs}], '\n",
    "            f'Loss: {test_loss.item():.4f} MEE: {test_mee:.4f} ', end='\\r')\n",
    "\n",
    "        # Check for convergence\n",
    "        if train_mee < min_mee and abs(train_mee-min_mee)>1e-3:\n",
    "            patience_counter = 0\n",
    "            min_mee = train_mee\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter == patience:\n",
    "        #    print(f\"Convergence reached. at epoch {epoch+1} Stopping training.\")\n",
    "            break\n",
    "\n",
    "    print(f'\\n N. Epochs = {epoch+1} - Loss (train | test)= ({train_loss.item():.4} | {test_loss.item():.4}) - MEE (train | test) = ({train_mee} | {test_mee})')\n",
    "\n",
    "    if plot_curves:\n",
    "        hyperparams = [hidden_sizes,learning_rate, momentum, bs, reg_coeff, activation, proportions, num_epochs]\n",
    "        plot_training_curves(epoch, train_losses, test_losses, train_mees, test_mees, hyperparams)\n",
    "\n",
    "    if return_history:\n",
    "        return model, epoch+1, np.array(train_losses), np.array(test_losses), np.array(train_mees), np.array(test_mees)\n",
    "    else:\n",
    "        return model, epoch+1, train_loss.item(), test_loss.item(), train_mee, test_mee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search_kfold(neuron_numbers, learning_rates, momentums, batch_sizes, reg_coeffs, activations,optimiz,proportionss, k_folds, x, y, plot_curves=False, num_epochs=1000):\n",
    "    \"\"\"\n",
    "    Perform grid search with k-fold cross-validation for hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - neuron_numbers (list): List of neuron numbers to search.\n",
    "    - learning_rates (list): List of learning rates to search.\n",
    "    - momentums (list): List of momentum values to search.\n",
    "    - batch_sizes (list): List of batch sizes to search.\n",
    "    - reg_coeffs (list): List of regularization coefficients to search.\n",
    "    - activations (list): List of activation functions to search.\n",
    "    - layerss (list): List of numbers of hidden layers to search.\n",
    "    - k_folds (int): Number of folds for cross-validation.\n",
    "    - x (numpy.ndarray): Input data.\n",
    "    - y (numpy.ndarray): Target data.\n",
    "    - plot_curves (bool, optional): Whether to plot training curves (default: False).\n",
    "    - num_epochs (int, optional): Number of training epochs (default: 1000).\n",
    "\n",
    "    Returns:\n",
    "    - list: List of best hyperparameters.\n",
    "\n",
    "    The function performs grid search with k-fold cross-validation for Monk classifier hyperparameters and returns the best hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    best_mee = float('inf')\n",
    "    best_hyperparams = []\n",
    "\n",
    "    for neuron_number, learning_rate, momentum, bs, reg_coeff, activation, proportions in product(neuron_numbers,learning_rates,momentums,batch_sizes, reg_coeffs, activations, proportionss):\n",
    "        print(f'activation={activation};; neuron_number={neuron_number}; lr={learning_rate}; alpha = {momentum}; batch size = {bs}; lambda = {reg_coeff}; optim = {optimiz}; proportions = {proportions}')\n",
    "\n",
    "        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        # Lists to store training and validation losses and MEEs for each epoch\n",
    "        train_losses, val_losses, train_mees, val_mees = [], [], [], []\n",
    "\n",
    "        # Perform K-fold cross-validation\n",
    "        for fold, (train_indices, val_indices) in enumerate(kf.split(x,y)):\n",
    "            #print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "\n",
    "            # Split the data into training and validation (or test) sets\n",
    "            X_train, X_val = x[train_indices], x[val_indices]\n",
    "            Y_train, Y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "            _, max_epoch, train_loss, val_loss, train_mee, val_mee = training_model(\n",
    "                x_train=X_train, y_train=Y_train, x_test=X_val, y_test=Y_val,\n",
    "                neuron_number=neuron_number, learning_rate=learning_rate, momentum=momentum,\n",
    "                bs=bs, reg_coeff=reg_coeff, activation=activation, proportions=proportions, optimiz=optimiz ,plot_curves=plot_curves, num_epochs=num_epochs)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_mees.append(train_mee)\n",
    "            val_mees.append(val_mee)\n",
    "\n",
    "        print(f'Final Results: activation={activation}; neuron number={neuron_number}; lr={learning_rate}; alpha = {momentum}; batch size = {bs}; lambda = {reg_coeff}; proportions = {proportions} --> '\n",
    "            f'train_loss = {np.mean(train_losses):.4} +- {np.std(train_losses):.4} | '\n",
    "            f'val_loss = {np.mean(val_losses):.4} +- {np.std(val_losses):.4}'\n",
    "            f'train_mee = {np.mean(train_mees):.4} +- {np.std(train_mees):.4} | '\n",
    "            f'val_mee = {np.mean(val_mees):.4} +- {np.std(val_mees):.4}')\n",
    "\n",
    "        if np.mean(val_mees) < best_mee:\n",
    "            best_mee = np.mean(val_mees)\n",
    "            best_hyperparams = [neuron_number, learning_rate, momentum, bs, reg_coeff, activation, proportions]\n",
    "\n",
    "    print(best_hyperparams)\n",
    "    return best_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('ML-CUP23-TR.csv', delimiter=',')\n",
    "X = dataset[:,1:11]\n",
    "y = dataset[:,11:14]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets (80%/20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERFORM GRID SEARCH TO FIND BEST HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation=Tanh();; neuron_number=1000; lr=0.0001; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; proportions = [0.2, 0.3, 0.5]\n",
      "Training - Epoch [4/5000], Loss: 748.7834, MEE: 42.3890 | Test - Epoch [4/5000], Loss: 684.8451 MEE: 40.2560 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch [1699/5000], Loss: 0.1109, MEE: 0.3901 | Test - Epoch [1699/5000], Loss: 0.3063 MEE: 0.6367     \n",
      " N. Epochs = 1699 - Loss (train | test)= (0.1109 | 0.3063) - MEE (train | test) = (0.39006513357162476 | 0.6367249488830566)\n",
      "Training - Epoch [1575/5000], Loss: 0.0754, MEE: 0.3778 | Test - Epoch [1575/5000], Loss: 0.4664 MEE: 0.7821     \n",
      " N. Epochs = 1575 - Loss (train | test)= (0.07538 | 0.4664) - MEE (train | test) = (0.37776419520378113 | 0.78208327293396)\n",
      "Training - Epoch [1456/5000], Loss: 0.1442, MEE: 0.4314 | Test - Epoch [1456/5000], Loss: 0.4440 MEE: 0.8320     \n",
      " N. Epochs = 1456 - Loss (train | test)= (0.1442 | 0.444) - MEE (train | test) = (0.4314453601837158 | 0.8320039510726929)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=0.0001; alpha = 0; batch size = 128; lambda = 0.001; proportions = [0.2, 0.3, 0.5] --> train_loss = 0.1101 +- 0.0281 | val_loss = 0.4056 +- 0.07078train_mee = 0.3998 +- 0.02296 | val_mee = 0.7503 +- 0.08284\n",
      "activation=Tanh();; neuron_number=1000; lr=0.0001; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; proportions = [0.25, 0.25, 0.5]\n",
      "Training - Epoch [1704/5000], Loss: 0.0622, MEE: 0.3580 | Test - Epoch [1704/5000], Loss: 0.2263 MEE: 0.5951     \n",
      " N. Epochs = 1704 - Loss (train | test)= (0.06215 | 0.2263) - MEE (train | test) = (0.3579753041267395 | 0.5950692296028137)\n",
      "Training - Epoch [1566/5000], Loss: 0.0788, MEE: 0.3881 | Test - Epoch [1566/5000], Loss: 0.5581 MEE: 0.8409     \n",
      " N. Epochs = 1566 - Loss (train | test)= (0.0788 | 0.5581) - MEE (train | test) = (0.38805681467056274 | 0.8409231305122375)\n",
      "Training - Epoch [1523/5000], Loss: 0.0806, MEE: 0.3854 | Test - Epoch [1523/5000], Loss: 0.3854 MEE: 0.8068     \n",
      " N. Epochs = 1523 - Loss (train | test)= (0.08062 | 0.3854) - MEE (train | test) = (0.3854028582572937 | 0.8068497180938721)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=0.0001; alpha = 0; batch size = 128; lambda = 0.001; proportions = [0.25, 0.25, 0.5] --> train_loss = 0.07386 +- 0.008309 | val_loss = 0.39 +- 0.1355train_mee = 0.3771 +- 0.0136 | val_mee = 0.7476 +- 0.1088\n",
      "activation=Tanh();; neuron_number=1000; lr=0.0001; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; proportions = [0.15, 0.35, 0.5]\n",
      "Training - Epoch [1660/5000], Loss: 0.0933, MEE: 0.4269 | Test - Epoch [1660/5000], Loss: 0.2978 MEE: 0.6610     \n",
      " N. Epochs = 1660 - Loss (train | test)= (0.0933 | 0.2978) - MEE (train | test) = (0.42692795395851135 | 0.6610029935836792)\n",
      "Training - Epoch [1626/5000], Loss: 0.0810, MEE: 0.3851 | Test - Epoch [1626/5000], Loss: 0.4863 MEE: 0.8000     \n",
      " N. Epochs = 1626 - Loss (train | test)= (0.08102 | 0.4863) - MEE (train | test) = (0.3851464092731476 | 0.8000497817993164)\n",
      "Training - Epoch [1847/5000], Loss: 0.0723, MEE: 0.3279 | Test - Epoch [1847/5000], Loss: 0.3808 MEE: 0.7805     \n",
      " N. Epochs = 1847 - Loss (train | test)= (0.07231 | 0.3808) - MEE (train | test) = (0.32787805795669556 | 0.7804886698722839)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=0.0001; alpha = 0; batch size = 128; lambda = 0.001; proportions = [0.15, 0.35, 0.5] --> train_loss = 0.08221 +- 0.008613 | val_loss = 0.3883 +- 0.07711train_mee = 0.38 +- 0.0406 | val_mee = 0.7472 +- 0.06146\n",
      "activation=Tanh();; neuron_number=1000; lr=0.0001; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; proportions = [0.15, 0.3, 0.55]\n",
      "Training - Epoch [1683/5000], Loss: 0.0808, MEE: 0.4288 | Test - Epoch [1683/5000], Loss: 0.2870 MEE: 0.6557     \n",
      " N. Epochs = 1683 - Loss (train | test)= (0.08083 | 0.287) - MEE (train | test) = (0.4287980794906616 | 0.6556609272956848)\n",
      "Training - Epoch [1751/5000], Loss: 0.0645, MEE: 0.3729 | Test - Epoch [1751/5000], Loss: 0.4353 MEE: 0.7833     \n",
      " N. Epochs = 1751 - Loss (train | test)= (0.0645 | 0.4353) - MEE (train | test) = (0.37290501594543457 | 0.7833142280578613)\n",
      "Training - Epoch [1648/5000], Loss: 0.1040, MEE: 0.4111 | Test - Epoch [1648/5000], Loss: 0.3644 MEE: 0.7870     \n",
      " N. Epochs = 1648 - Loss (train | test)= (0.104 | 0.3644) - MEE (train | test) = (0.4110591411590576 | 0.7870414853096008)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=0.0001; alpha = 0; batch size = 128; lambda = 0.001; proportions = [0.15, 0.3, 0.55] --> train_loss = 0.08312 +- 0.01622 | val_loss = 0.3622 +- 0.06057train_mee = 0.4043 +- 0.02332 | val_mee = 0.742 +- 0.06107\n",
      "activation=Tanh();; neuron_number=1000; lr=0.0001; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; proportions = [0.1, 0.25, 0.55]\n",
      "Training - Epoch [1804/5000], Loss: 0.1099, MEE: 0.4619 | Test - Epoch [1804/5000], Loss: 0.3090 MEE: 0.6684     \n",
      " N. Epochs = 1804 - Loss (train | test)= (0.1099 | 0.309) - MEE (train | test) = (0.461870938539505 | 0.668405294418335)\n",
      "Training - Epoch [2070/5000], Loss: 0.0595, MEE: 0.3709 | Test - Epoch [2070/5000], Loss: 0.4279 MEE: 0.7836     \n",
      " N. Epochs = 2070 - Loss (train | test)= (0.05951 | 0.4279) - MEE (train | test) = (0.3708983361721039 | 0.7836055755615234)\n",
      "Training - Epoch [1799/5000], Loss: 0.1145, MEE: 0.4450 | Test - Epoch [1799/5000], Loss: 0.4094 MEE: 0.8454     \n",
      " N. Epochs = 1799 - Loss (train | test)= (0.1145 | 0.4094) - MEE (train | test) = (0.4450468122959137 | 0.8453834652900696)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=0.0001; alpha = 0; batch size = 128; lambda = 0.001; proportions = [0.1, 0.25, 0.55] --> train_loss = 0.09463 +- 0.02491 | val_loss = 0.3821 +- 0.05225train_mee = 0.4259 +- 0.03952 | val_mee = 0.7658 +- 0.07334\n",
      "activation=Tanh();; neuron_number=1000; lr=0.0001; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; proportions = [0.05, 0.4, 0.55]\n",
      "Training - Epoch [1954/5000], Loss: 0.0760, MEE: 0.3982 | Test - Epoch [1954/5000], Loss: 0.2791 MEE: 0.6180     \n",
      " N. Epochs = 1954 - Loss (train | test)= (0.076 | 0.2791) - MEE (train | test) = (0.39823928475379944 | 0.618025004863739)\n",
      "Training - Epoch [1747/5000], Loss: 0.0844, MEE: 0.4172 | Test - Epoch [1747/5000], Loss: 0.5047 MEE: 0.8509     \n",
      " N. Epochs = 1747 - Loss (train | test)= (0.08436 | 0.5047) - MEE (train | test) = (0.41719377040863037 | 0.8508760929107666)\n",
      "Training - Epoch [1707/5000], Loss: 0.0988, MEE: 0.4404 | Test - Epoch [1707/5000], Loss: 0.3850 MEE: 0.8348     \n",
      " N. Epochs = 1707 - Loss (train | test)= (0.0988 | 0.385) - MEE (train | test) = (0.4403916299343109 | 0.8348165154457092)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=0.0001; alpha = 0; batch size = 128; lambda = 0.001; proportions = [0.05, 0.4, 0.55] --> train_loss = 0.08639 +- 0.009416 | val_loss = 0.3896 +- 0.09214train_mee = 0.4186 +- 0.01724 | val_mee = 0.7679 +- 0.1062\n",
      "activation=Tanh();; neuron_number=1000; lr=0.0001; alpha = 0; batch size = 128; lambda = 0.001; optim = Adam; proportions = [0.25, 0.35, 0.4]\n",
      "Training - Epoch [1882/5000], Loss: 0.0445, MEE: 0.2940 | Test - Epoch [1882/5000], Loss: 0.2546 MEE: 0.5924     \n",
      " N. Epochs = 1882 - Loss (train | test)= (0.04448 | 0.2546) - MEE (train | test) = (0.2940000891685486 | 0.5924374461174011)\n",
      "Training - Epoch [1755/5000], Loss: 0.0516, MEE: 0.3041 | Test - Epoch [1755/5000], Loss: 0.5473 MEE: 0.7887     \n",
      " N. Epochs = 1755 - Loss (train | test)= (0.05158 | 0.5473) - MEE (train | test) = (0.3041076362133026 | 0.7887154221534729)\n",
      "Training - Epoch [1603/5000], Loss: 0.0952, MEE: 0.3340 | Test - Epoch [1603/5000], Loss: 0.3984 MEE: 0.7779     \n",
      " N. Epochs = 1603 - Loss (train | test)= (0.09524 | 0.3984) - MEE (train | test) = (0.33395835757255554 | 0.7779353857040405)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=0.0001; alpha = 0; batch size = 128; lambda = 0.001; proportions = [0.25, 0.35, 0.4] --> train_loss = 0.06377 +- 0.02244 | val_loss = 0.4001 +- 0.1195train_mee = 0.3107 +- 0.01696 | val_mee = 0.7197 +- 0.09009\n",
      "[1000, 0.0001, 0, 128, 0.001, Tanh(), [0.25, 0.35, 0.4]]\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [1e-4]\n",
    "momentums = [0] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "#proportionss = [[0.1,0.8,0.1],[0.2,0.6,0.2],[0.25,0.50,0.25],[0.2,0.7,0.1],[0.1,0.7,0.2],[0.3,0.6,0.1]]\n",
    "#proportionss = [[0.8,0.1,0.1],[0.7,0.2,0.1],[0.6,0.3,0.1],[0.6,0.2,0.2],[0.8,0.15,0.05],[0.5,0.4,0.1],[0.5,0.25,0.25], [0.5,0.3,0.2]]\n",
    "#proportionss = [[0.1,0.1,0.8],[0.1,0.2,0.7],[0.1,0.3,0.6],[0.2,0.2,0.6],[0.05,0.15,0.8],[0.1,0.4,0.5],[0.25,0.25,0.5], [0.2,0.3,0.5]]\n",
    "proportionss = [[0.2,0.3,0.5],[0.25,0.25,0.5],[0.15,0.35,0.5],[0.15,0.3,0.55],[0.1,0.25,0.55],[0.05,0.40,0.55],[0.25,0.35,0.4]]\n",
    "\n",
    "optimiz = 'Adam' #either 'SGD' or 'Adam'\n",
    "best_hp = perform_grid_search_kfold(hidden_neurons,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    activations,\n",
    "                                    optimiz,\n",
    "                                    proportionss=proportionss,\n",
    "                                    k_folds=3,\n",
    "                                    x=X_train,\n",
    "                                    y=y_train,\n",
    "                                    num_epochs=5000,\n",
    "                                    plot_curves=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cmepda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
