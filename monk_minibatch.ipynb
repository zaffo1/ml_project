{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_monk(file_name):\n",
    "    '''\n",
    "    load and preprocess data from the monk dataset\n",
    "    '''\n",
    "\n",
    "    # load the dataset, split into input (X) and output (y) variables\n",
    "    df = pd.read_csv(file_name, delimiter=' ', header=None, names=['remove_this_column','target', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'data_number'], index_col=False)\n",
    "    df.drop(columns='remove_this_column')\n",
    "\n",
    "    x1 = np.array(df['x1'])\n",
    "    x2 = np.array(df['x2'])\n",
    "    x3 = np.array(df['x3'])\n",
    "    x4 = np.array(df['x4'])\n",
    "    x5 = np.array(df['x5'])\n",
    "    x6 = np.array(df['x6'])\n",
    "    target = np.array(df['target'])\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    x1_ =x1.reshape(-1, 1)\n",
    "    # Fit and transform the data to one-hot encoding\n",
    "    input_one_hot = encoder.fit_transform(x1_)\n",
    "    inputs = [x2,x3,x4,x5,x6]\n",
    "\n",
    "    for x in inputs:\n",
    "        data =x.reshape(-1, 1)\n",
    "        # Fit and transform the data to one-hot encoding\n",
    "        one_hot_encoded = encoder.fit_transform(data)\n",
    "\n",
    "        # Display the result\n",
    "        #print(\"Original data:\")\n",
    "        #print(data)\n",
    "\n",
    "        #print(\"\\nOne-hot encoded data:\")\n",
    "        #print(one_hot_encoded)\n",
    "\n",
    "        input_one_hot = np.hstack((input_one_hot, one_hot_encoded))\n",
    "\n",
    "    #print(input_one_hot.shape)\n",
    "    #print(input_one_hot[0])\n",
    "    #print(target)\n",
    "    #target = 2 * target - 1 #to map from {0,1} to {-1,1}\n",
    "\n",
    "    x = torch.tensor(input_one_hot, dtype=torch.float32)\n",
    "    y = torch.tensor(target, dtype=torch.float32).reshape(-1,1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([124, 17])\n",
      "torch.Size([124, 1])\n",
      "torch.Size([432, 17])\n",
      "torch.Size([432, 1])\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = preprocess_monk(file_name='monk_data/monks-1.train')\n",
    "x_test, y_test = preprocess_monk(file_name='monk_data/monks-1.test')\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch [1/300], Loss: 0.2527\n",
      "Training - Epoch [1/300], Accuracy: 0.3548\n",
      "Test - Epoch [1/300], Loss: 0.2505\n",
      "Test - Epoch [1/300], Accuracy: 0.4838\n",
      "Training - Epoch [2/300], Loss: 0.2529\n",
      "Training - Epoch [2/300], Accuracy: 0.3226\n",
      "Test - Epoch [2/300], Loss: 0.2503\n",
      "Test - Epoch [2/300], Accuracy: 0.4606\n",
      "Training - Epoch [3/300], Loss: 0.2520\n",
      "Training - Epoch [3/300], Accuracy: 0.5000\n",
      "Test - Epoch [3/300], Loss: 0.2503\n",
      "Test - Epoch [3/300], Accuracy: 0.4722\n",
      "Training - Epoch [4/300], Loss: 0.2508\n",
      "Training - Epoch [4/300], Accuracy: 0.4839\n",
      "Test - Epoch [4/300], Loss: 0.2504\n",
      "Test - Epoch [4/300], Accuracy: 0.5000\n",
      "Training - Epoch [5/300], Loss: 0.2522\n",
      "Training - Epoch [5/300], Accuracy: 0.4274\n",
      "Test - Epoch [5/300], Loss: 0.2503\n",
      "Test - Epoch [5/300], Accuracy: 0.5000\n",
      "Training - Epoch [6/300], Loss: 0.2514\n",
      "Training - Epoch [6/300], Accuracy: 0.4355\n",
      "Test - Epoch [6/300], Loss: 0.2503\n",
      "Test - Epoch [6/300], Accuracy: 0.4884\n",
      "Training - Epoch [7/300], Loss: 0.2513\n",
      "Training - Epoch [7/300], Accuracy: 0.4839\n",
      "Test - Epoch [7/300], Loss: 0.2503\n",
      "Test - Epoch [7/300], Accuracy: 0.4815\n",
      "Training - Epoch [8/300], Loss: 0.2518\n",
      "Training - Epoch [8/300], Accuracy: 0.4597\n",
      "Test - Epoch [8/300], Loss: 0.2502\n",
      "Test - Epoch [8/300], Accuracy: 0.5046\n",
      "Training - Epoch [9/300], Loss: 0.2513\n",
      "Training - Epoch [9/300], Accuracy: 0.4677\n",
      "Test - Epoch [9/300], Loss: 0.2502\n",
      "Test - Epoch [9/300], Accuracy: 0.5046\n",
      "Training - Epoch [10/300], Loss: 0.2511\n",
      "Training - Epoch [10/300], Accuracy: 0.4113\n",
      "Test - Epoch [10/300], Loss: 0.2502\n",
      "Test - Epoch [10/300], Accuracy: 0.4792\n",
      "Training - Epoch [11/300], Loss: 0.2512\n",
      "Training - Epoch [11/300], Accuracy: 0.4758\n",
      "Test - Epoch [11/300], Loss: 0.2501\n",
      "Test - Epoch [11/300], Accuracy: 0.5023\n",
      "Training - Epoch [12/300], Loss: 0.2523\n",
      "Training - Epoch [12/300], Accuracy: 0.4677\n",
      "Test - Epoch [12/300], Loss: 0.2501\n",
      "Test - Epoch [12/300], Accuracy: 0.5093\n",
      "Training - Epoch [13/300], Loss: 0.2517\n",
      "Training - Epoch [13/300], Accuracy: 0.4919\n",
      "Test - Epoch [13/300], Loss: 0.2501\n",
      "Test - Epoch [13/300], Accuracy: 0.4931\n",
      "Training - Epoch [14/300], Loss: 0.2506\n",
      "Training - Epoch [14/300], Accuracy: 0.4839\n",
      "Test - Epoch [14/300], Loss: 0.2501\n",
      "Test - Epoch [14/300], Accuracy: 0.5093\n",
      "Training - Epoch [15/300], Loss: 0.2502\n",
      "Training - Epoch [15/300], Accuracy: 0.4677\n",
      "Test - Epoch [15/300], Loss: 0.2500\n",
      "Test - Epoch [15/300], Accuracy: 0.4954\n",
      "Training - Epoch [16/300], Loss: 0.2507\n",
      "Training - Epoch [16/300], Accuracy: 0.4839\n",
      "Test - Epoch [16/300], Loss: 0.2499\n",
      "Test - Epoch [16/300], Accuracy: 0.4931\n",
      "Training - Epoch [17/300], Loss: 0.2503\n",
      "Training - Epoch [17/300], Accuracy: 0.4758\n",
      "Test - Epoch [17/300], Loss: 0.2498\n",
      "Test - Epoch [17/300], Accuracy: 0.5046\n",
      "Training - Epoch [18/300], Loss: 0.2494\n",
      "Training - Epoch [18/300], Accuracy: 0.5081\n",
      "Test - Epoch [18/300], Loss: 0.2496\n",
      "Test - Epoch [18/300], Accuracy: 0.5023\n",
      "Training - Epoch [19/300], Loss: 0.2490\n",
      "Training - Epoch [19/300], Accuracy: 0.5242\n",
      "Test - Epoch [19/300], Loss: 0.2493\n",
      "Test - Epoch [19/300], Accuracy: 0.5208\n",
      "Training - Epoch [20/300], Loss: 0.2483\n",
      "Training - Epoch [20/300], Accuracy: 0.5000\n",
      "Test - Epoch [20/300], Loss: 0.2488\n",
      "Test - Epoch [20/300], Accuracy: 0.5301\n",
      "Training - Epoch [21/300], Loss: 0.2470\n",
      "Training - Epoch [21/300], Accuracy: 0.5887\n",
      "Test - Epoch [21/300], Loss: 0.2481\n",
      "Test - Epoch [21/300], Accuracy: 0.5486\n",
      "Training - Epoch [22/300], Loss: 0.2447\n",
      "Training - Epoch [22/300], Accuracy: 0.6210\n",
      "Test - Epoch [22/300], Loss: 0.2467\n",
      "Test - Epoch [22/300], Accuracy: 0.5625\n",
      "Training - Epoch [23/300], Loss: 0.2426\n",
      "Training - Epoch [23/300], Accuracy: 0.6613\n",
      "Test - Epoch [23/300], Loss: 0.2446\n",
      "Test - Epoch [23/300], Accuracy: 0.5903\n",
      "Training - Epoch [24/300], Loss: 0.2389\n",
      "Training - Epoch [24/300], Accuracy: 0.6129\n",
      "Test - Epoch [24/300], Loss: 0.2412\n",
      "Test - Epoch [24/300], Accuracy: 0.6250\n",
      "Training - Epoch [25/300], Loss: 0.2322\n",
      "Training - Epoch [25/300], Accuracy: 0.7016\n",
      "Test - Epoch [25/300], Loss: 0.2341\n",
      "Test - Epoch [25/300], Accuracy: 0.6620\n",
      "Training - Epoch [26/300], Loss: 0.2207\n",
      "Training - Epoch [26/300], Accuracy: 0.6855\n",
      "Test - Epoch [26/300], Loss: 0.2273\n",
      "Test - Epoch [26/300], Accuracy: 0.6713\n",
      "Training - Epoch [27/300], Loss: 0.2059\n",
      "Training - Epoch [27/300], Accuracy: 0.7258\n",
      "Test - Epoch [27/300], Loss: 0.2257\n",
      "Test - Epoch [27/300], Accuracy: 0.6389\n",
      "Training - Epoch [28/300], Loss: 0.1961\n",
      "Training - Epoch [28/300], Accuracy: 0.7742\n",
      "Test - Epoch [28/300], Loss: 0.2096\n",
      "Test - Epoch [28/300], Accuracy: 0.7338\n",
      "Training - Epoch [29/300], Loss: 0.1833\n",
      "Training - Epoch [29/300], Accuracy: 0.7823\n",
      "Test - Epoch [29/300], Loss: 0.2044\n",
      "Test - Epoch [29/300], Accuracy: 0.7384\n",
      "Training - Epoch [30/300], Loss: 0.1725\n",
      "Training - Epoch [30/300], Accuracy: 0.8065\n",
      "Test - Epoch [30/300], Loss: 0.2170\n",
      "Test - Epoch [30/300], Accuracy: 0.6829\n",
      "Training - Epoch [31/300], Loss: 0.1704\n",
      "Training - Epoch [31/300], Accuracy: 0.7903\n",
      "Test - Epoch [31/300], Loss: 0.1968\n",
      "Test - Epoch [31/300], Accuracy: 0.7477\n",
      "Training - Epoch [32/300], Loss: 0.1574\n",
      "Training - Epoch [32/300], Accuracy: 0.8226\n",
      "Test - Epoch [32/300], Loss: 0.1913\n",
      "Test - Epoch [32/300], Accuracy: 0.7639\n",
      "Training - Epoch [33/300], Loss: 0.1536\n",
      "Training - Epoch [33/300], Accuracy: 0.8468\n",
      "Test - Epoch [33/300], Loss: 0.1845\n",
      "Test - Epoch [33/300], Accuracy: 0.7616\n",
      "Training - Epoch [34/300], Loss: 0.1482\n",
      "Training - Epoch [34/300], Accuracy: 0.8387\n",
      "Test - Epoch [34/300], Loss: 0.1858\n",
      "Test - Epoch [34/300], Accuracy: 0.7755\n",
      "Training - Epoch [35/300], Loss: 0.1354\n",
      "Training - Epoch [35/300], Accuracy: 0.8548\n",
      "Test - Epoch [35/300], Loss: 0.1759\n",
      "Test - Epoch [35/300], Accuracy: 0.7917\n",
      "Training - Epoch [36/300], Loss: 0.1262\n",
      "Training - Epoch [36/300], Accuracy: 0.8710\n",
      "Test - Epoch [36/300], Loss: 0.1601\n",
      "Test - Epoch [36/300], Accuracy: 0.8102\n",
      "Training - Epoch [37/300], Loss: 0.1202\n",
      "Training - Epoch [37/300], Accuracy: 0.8871\n",
      "Test - Epoch [37/300], Loss: 0.1510\n",
      "Test - Epoch [37/300], Accuracy: 0.8426\n",
      "Training - Epoch [38/300], Loss: 0.1144\n",
      "Training - Epoch [38/300], Accuracy: 0.8871\n",
      "Test - Epoch [38/300], Loss: 0.1444\n",
      "Test - Epoch [38/300], Accuracy: 0.8519\n",
      "Training - Epoch [39/300], Loss: 0.1079\n",
      "Training - Epoch [39/300], Accuracy: 0.9274\n",
      "Test - Epoch [39/300], Loss: 0.1420\n",
      "Test - Epoch [39/300], Accuracy: 0.8403\n",
      "Training - Epoch [40/300], Loss: 0.1052\n",
      "Training - Epoch [40/300], Accuracy: 0.9032\n",
      "Test - Epoch [40/300], Loss: 0.1312\n",
      "Test - Epoch [40/300], Accuracy: 0.8773\n",
      "Training - Epoch [41/300], Loss: 0.0981\n",
      "Training - Epoch [41/300], Accuracy: 0.9194\n",
      "Test - Epoch [41/300], Loss: 0.1240\n",
      "Test - Epoch [41/300], Accuracy: 0.8981\n",
      "Training - Epoch [42/300], Loss: 0.0908\n",
      "Training - Epoch [42/300], Accuracy: 0.9274\n",
      "Test - Epoch [42/300], Loss: 0.1171\n",
      "Test - Epoch [42/300], Accuracy: 0.9028\n",
      "Training - Epoch [43/300], Loss: 0.0868\n",
      "Training - Epoch [43/300], Accuracy: 0.9274\n",
      "Test - Epoch [43/300], Loss: 0.1128\n",
      "Test - Epoch [43/300], Accuracy: 0.9074\n",
      "Training - Epoch [44/300], Loss: 0.0840\n",
      "Training - Epoch [44/300], Accuracy: 0.9355\n",
      "Test - Epoch [44/300], Loss: 0.1178\n",
      "Test - Epoch [44/300], Accuracy: 0.8843\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the classifier\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        output = self.sigmoid(x)\n",
    "        return output\n",
    "\n",
    "# Set hyperparameters\n",
    "input_size = 17\n",
    "hidden_size = 5\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "momentum = 0.5\n",
    "num_epochs = 300\n",
    "\n",
    "# Create an instance of the model\n",
    "model = SimpleClassifier(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "bs = 4\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Lists to store training and test losses for plotting\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    # Mini-batch training\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "    for inputs, labels in train_dataloader:\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss for this epoch\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        predicted_labels = torch.round(outputs)\n",
    "        correct_predictions = (predicted_labels == labels).sum().item()\n",
    "        total_samples = labels.size(0)\n",
    "        total_accuracy += correct_predictions / total_samples\n",
    "\n",
    "    # Print average training loss for the epoch\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Training - Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "    train_losses.append(average_loss)\n",
    "\n",
    "    # Print average training accuracy for the epoch\n",
    "    average_accuracy = total_accuracy / len(train_dataloader)\n",
    "    print(f'Training - Epoch [{epoch+1}/{num_epochs}], Accuracy: {average_accuracy:.4f}')\n",
    "    train_accuracies.append(average_accuracy)\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        total_test_loss = 0.0\n",
    "        total_test_accuracy = 0.0\n",
    "        for test_inputs, test_labels in test_dataloader:\n",
    "            test_outputs = model(test_inputs)\n",
    "            test_loss = criterion(test_outputs, test_labels)\n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "            # Calculate test accuracy\n",
    "            predicted_labels = torch.round(test_outputs)\n",
    "            correct_predictions = (predicted_labels == test_labels).sum().item()\n",
    "            total_samples = test_labels.size(0)\n",
    "            total_test_accuracy += correct_predictions / total_samples\n",
    "\n",
    "        average_test_loss = total_test_loss / len(test_dataloader)\n",
    "        print(f'Test - Epoch [{epoch+1}/{num_epochs}], Loss: {average_test_loss:.4f}')\n",
    "\n",
    "        # Print average training accuracy for the epoch\n",
    "        average_test_accuracy = total_test_accuracy / len(test_dataloader)\n",
    "        print(f'Test - Epoch [{epoch+1}/{num_epochs}], Accuracy: {average_test_accuracy:.4f}')\n",
    "\n",
    "        test_accuracies.append(average_test_accuracy)\n",
    "        test_losses.append(average_test_loss)\n",
    "\n",
    "# Plot the training and test losses\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss plots\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy plots\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs + 1), test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
