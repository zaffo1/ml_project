{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML CUP - SVM\n",
    "In this notebook we will study the application of neural networks to the ML CUP.\n",
    "Firs we will study the effect of different optimizers (SGD, Adam, RMSProp).\n",
    "Then we will introduce a decaying lr using ReduceLROnpPlateau, and study the effects of this callback.\n",
    "A more detailed explanation of our pipeline is in the presentation slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)   # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)   # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)    # fontsize of the figure suptitle\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_euclidean_error(vectors1, vectors2):\n",
    "    \"\"\"\n",
    "    Compute the mean Euclidean error between two sets of 3D vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - vectors1: NumPy array of shape (N, 3) representing the first set of 3D vectors\n",
    "    - vectors2: NumPy array of shape (N, 3) representing the second set of 3D vectors\n",
    "\n",
    "    Returns:\n",
    "    - mean_error: Mean Euclidean error between the two sets of vectors\n",
    "    \"\"\"\n",
    "    # Check if the input arrays have the correct shape\n",
    "    if vectors1.shape != vectors2.shape or vectors1.shape[1] != 3:\n",
    "        raise ValueError(\"Input arrays must be of shape (N, 3)\")\n",
    "\n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = np.linalg.norm(vectors1 - vectors2, axis=1)\n",
    "\n",
    "    # Calculate the mean Euclidean error\n",
    "    mean_error = np.mean(euclidean_distance)\n",
    "\n",
    "    return mean_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output is 3-dimensional, our model is composed of three SVMs.\n",
    "We create a class to make things easier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSVM:\n",
    "    \"\"\"\n",
    "    A wrapper class for multiple Support Vector Regressors (SVRs) with shared hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - kernel (str, optional): Specifies the kernel type ('linear', 'poly', 'rbf', 'sigmoid', etc.).\n",
    "    - C (float, optional): Regularization parameter. Controls the trade-off between smooth decision function and fitting the training data.\n",
    "    - epsilon (float, optional): Width of the epsilon-insensitive tube. Ignored errors within this range during training.\n",
    "    - degree (int, optional): Degree of the polynomial kernel. Only applicable for 'poly' kernel.\n",
    "\n",
    "    Attributes:\n",
    "    - svr0, svr1, svr2: Support Vector Regressors for each target dimension.\n",
    "\n",
    "    Methods:\n",
    "    - fit(X, y): Fit each SVR on its respective target dimension.\n",
    "    - predict(X): Make predictions using each SVR and return a stacked array of predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel='rbf', C=1.0, epsilon=0.1, degree=3, gamma='scale'):\n",
    "        # Create three support vector regressors with the specified kernel, regularization parameter, and epsilon\n",
    "        self.svr0 = SVR(kernel=kernel, C=C, epsilon=epsilon, degree=degree, gamma=gamma)\n",
    "        self.svr1 = SVR(kernel=kernel, C=C, epsilon=epsilon, degree=degree, gamma=gamma)\n",
    "        self.svr2 = SVR(kernel=kernel, C=C, epsilon=epsilon, degree=degree, gamma=gamma)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Fit each SVR on its respective data\n",
    "        self.svr0.fit(X, y[:,0])\n",
    "        self.svr1.fit(X, y[:,1])\n",
    "        self.svr2.fit(X, y[:,2])\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using each SVR\n",
    "        pred = np.column_stack((self.svr0.predict(X),self.svr1.predict(X),self.svr2.predict(X)))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search_kfold(kernels, Cs, epsilons, degrees,gammas, k_folds, x, y, return_sequence=False):\n",
    "    \"\"\"\n",
    "    Perform grid search with k-fold cross-validation for Support Vector Regression hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - kernels (list): List of kernel types to search.\n",
    "    - Cs (list): List of regularization parameters to search.\n",
    "    - epsilons (list): List of epsilon values to search.\n",
    "    - degrees (list): List of degrees for polynomial kernels.\n",
    "    - k_folds (int): Number of folds for cross-validation.\n",
    "    - x (numpy.ndarray): Input data.\n",
    "    - y (numpy.ndarray): Target data.\n",
    "    - return_sequence (bool, optional): Whether to return the mean Euclidean error sequence (default: False).\n",
    "\n",
    "    Returns:\n",
    "    - list or tuple: Best hyperparameters or tuple with best hyperparameters\n",
    "        and sequences of mean Euclidean errors.\n",
    "\n",
    "    The function performs grid search with k-fold cross-validation for\n",
    "    Support Vector Regression hyperparameters and returns the best hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    mee_sequence = []\n",
    "    mee_sd_sequence = []\n",
    "    best_mee = float('inf')\n",
    "    best_mee_std = 0\n",
    "    best_hyperparams = []\n",
    "    counter = 0\n",
    "    num_combinations = sum(1 for _ in product(kernels, Cs,epsilons, degrees, gammas))\n",
    "    print('total number of grid search combinations explored:',num_combinations)\n",
    "    for kernel, C,epsilon,  degree, gamma in product(kernels, Cs,epsilons, degrees, gammas):\n",
    "        counter += 1\n",
    "        print(f'{counter}/{num_combinations} Hyperparams:',kernel, C, epsilon, degree, gamma)\n",
    "\n",
    "        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "        val_mees = []\n",
    "        train_mees = []\n",
    "\n",
    "        # Perform K-fold cross-validation\n",
    "        for fold, (train_indices, val_indices) in enumerate(kf.split(x,y)):\n",
    "\n",
    "            # Split the data into training and validation (or test) sets\n",
    "            X_train, X_val = x[train_indices], x[val_indices]\n",
    "            y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "            model = MultiSVM(kernel, C, epsilon, degree, gamma)\n",
    "            model.fit(X_train,y_train)\n",
    "            val_mees.append(mean_euclidean_error(model.predict(X_val),y_val))\n",
    "            train_mees.append(mean_euclidean_error(model.predict(X_train),y_train))\n",
    "        print(f'Final Results: kernel={kernel}; C={C}; epsilon={epsilon}; deg = {degree}; gamma = {gamma} --> '\n",
    "            f'train_mee = {np.mean(train_mees):.4} +- {np.std(train_mees):.4}')\n",
    "        print(f'Final Results: kernel={kernel}; C={C}; epsilon={epsilon}; deg = {degree}, gamma = {gamma} --> '\n",
    "            f'val_mee = {np.mean(val_mees):.4} +- {np.std(val_mees):.4}')\n",
    "\n",
    "        mee_sequence.append(np.mean(val_mees))\n",
    "        mee_sd_sequence.append(np.std(val_mees))\n",
    "        if np.mean(val_mees) < best_mee:\n",
    "            best_mee = np.mean(val_mees)\n",
    "            best_mee_std = np.std(val_mees)\n",
    "            best_hyperparams = [kernel, C, epsilon, degree, gamma]\n",
    "\n",
    "    print(f'Best Hp: {best_hyperparams} with MEE = {best_mee} +- {best_mee_std}')\n",
    "    if return_sequence:\n",
    "        return best_hyperparams, np.array(mee_sequence), np.array(mee_sd_sequence)\n",
    "    else:\n",
    "        return best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_std(x,mee,std, label, color):\n",
    "    \"\"\"\n",
    "    Plot mean Euclidean error (MEE) along with standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - x (array-like): Values of the hyperparameter C or epsilon.\n",
    "    - mee (array-like): Mean Euclidean error values.\n",
    "    - std (array-like): Standard deviation of Euclidean error values.\n",
    "    - color (str): Color for the plot.\n",
    "\n",
    "    The function plots the mean Euclidean error (MEE) along with its\n",
    "    standard deviation based on the values of the hyperparameter C or epsilon.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 9))\n",
    "    plt.plot(x,mee, label='MEE $\\pm$ std. (results of k-fold cross validation)', color = color, linewidth=1)\n",
    "    plt.fill_between(x,mee-std, mee+std, color=color, alpha=0.3)\n",
    "\n",
    "    plt.xlabel(f'{label} values')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('ML-CUP23-TR.csv', delimiter=',')\n",
    "X = dataset[:,1:11]\n",
    "y = dataset[:,11:14]\n",
    "\n",
    "# Split the data into training and testing sets (80%/20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coarse Grid Searches (to find best kernel)\n",
    "We perform two grid searches. One with the rbf kernel, and one with polinomial kernel.\n",
    "In the grid search with rbf, we include the hyperparameter 'gamma', related to the std of the gaussian kernel.\n",
    "In the case of the polinomial kernel we include the degree of the polinomial as hyperparameter.\n",
    "We compare the results of k-fold cross validation to choose the kernel most suitable for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#first coarse grid search\n",
    "kernels = ['rbf']\n",
    "Cs = [0.01,0.1,1,10,100,1000]\n",
    "epsilons = [0.01,0.1,1,10]\n",
    "degrees=[0] #only relevant for poly kernel\n",
    "gammas = ['scale',0.01,0.1,1,10] #related to sigma in rbf\n",
    "# Best Hp: ['rbf', 1000, 0.1, 0, 'scale'] with MEE = 0.6396615599226871 +- 0.04980493197143183\n",
    "\n",
    "\n",
    "'''\n",
    "kernels = ['poly']\n",
    "Cs = [0.01,0.1,1,10,100,1000]\n",
    "degrees = np.arange(3,30,1)\n",
    "epsilons = [0.01,0.1,1,10]\n",
    "gammas = ['scale'] # related to sigma in rbf\n",
    "#Best Hp: ['poly', 1000, 0.01, 5] with MEE = 0.8183841594881777 +- 0.12792331527745304\n",
    "'''\n",
    "\n",
    "best_hyperparams = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          epsilons,\n",
    "                          degrees,\n",
    "                          gammas,\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finer Grid Search\n",
    "The best kernel is rbf, now let's study the other hyperparameters more in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#finer grid search\n",
    "'''\n",
    "kernels = ['rbf']\n",
    "Cs = [1000,2000,3000,4000,5000]\n",
    "epsilons = [0.1,0.2,0.3,0.4,0.5]\n",
    "gammas = ['scale',0.1,0.2,0.3,0.4,0.5] #related to sigma in rbf\n",
    "# Best Hp: ['rbf', 3000, 0.1] with MEE = 0.6172759223965806 +- 0.06576969897515686\n",
    "'''\n",
    "\n",
    "\n",
    "#best hp untill now:\n",
    "kernels = ['rbf']\n",
    "Cs = [3000]\n",
    "epsilons = [0.1]\n",
    "gammas = ['scale']\n",
    "\n",
    "best_hyperparams = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          epsilons,\n",
    "                          degrees,\n",
    "                          gammas,\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the effect of the parameter C.\n",
    "Keeping $\\varepsilon$ fixed, we want to study the effect of C on the validation error. \n",
    "- Low C ---> many TR errors are allowed (risk of underfitting)\n",
    "- High C ---> less TR errors allowed (risk of overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['rbf']\n",
    "Cs = np.arange(200, 10000, 200)\n",
    "epsilons = [0.1]\n",
    "#Best Hp: ['rbf', 2800, 0.1] with MEE = 0.6164267250857903 +- 0.0641407687516038\n",
    "\n",
    "best_hyperparams, mees, mees_sd = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          epsilons,\n",
    "                          degrees=[0],\n",
    "                          gammas=['scale'],\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train,\n",
    "                          return_sequence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the values of mees changing the values of C\n",
    "plot_mean_std(Cs,mees,mees_sd, label = 'C', color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the effect of the parameter $\\varepsilon$.\n",
    "Keeping C fixed, we want to study the effect of $\\varepsilon$ on the validation error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['rbf']\n",
    "Cs = [2800]\n",
    "epsilons = np.arange(0.01,0.2,0.01)\n",
    "#Best Hp: ['rbf', 2800, 0.1] with MEE = 0.6164267250857903 +- 0.0641407687516038\n",
    "\n",
    "best_hyperparams, mees, mees_sd = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          epsilons,\n",
    "                          degrees=[0],\n",
    "                          gammas=['scale'],\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train,\n",
    "                          return_sequence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the values of mees changing the values of C\n",
    "plot_mean_std(epsilons,mees,mees_sd, label='$\\epsilon$', color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final model from model selection:\n",
    "kernels = ['rbf']\n",
    "Cs = [2800]\n",
    "epsilons = [0.05]\n",
    "\n",
    "best_hyperparams, mees, mees_sd = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          epsilons,\n",
    "                          degrees=[0],\n",
    "                          gammas=['scale'],\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train,\n",
    "                          return_sequence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the final model on all the training data\n",
    "Finally, we train our best model on all the training data.\n",
    "We test its performances on our internal test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams = ['rbf', 2800, 0.05]\n",
    "model = MultiSVM(*best_hyperparams)\n",
    "model.fit(X_train,y_train)\n",
    "print('Test MEE:',mean_euclidean_error(model.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of support vectors\n",
    "sv0 = model.svr0.support_vectors_.shape[0]\n",
    "sv1 = model.svr1.support_vectors_.shape[0]\n",
    "sv2 = model.svr2.support_vectors_.shape[0]\n",
    "\n",
    "print(\"Number of support vectors:\", sv0,sv1,sv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute R^2\n",
    "and make plots of targets vs predicted outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "o0 = predictions[:,0]\n",
    "o1 = predictions[:,1]\n",
    "o2 = predictions[:,2]\n",
    "y0_test = y_test[:,0]\n",
    "y1_test = y_test[:,1]\n",
    "y2_test = y_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r_squared = r2_score(o0, y0_test)\n",
    "print(r_squared)\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(o0,y0_test, label=f'R$^2$ = {r2_score(o0, y0_test):.5}', marker='.')\n",
    "plt.xlabel('y$_1$ predicted')\n",
    "plt.ylabel('y$_1$ target')\n",
    "plt.legend()\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(o1,y1_test, label=f'R$^2$ = {r2_score(o1, y1_test):.5}', marker='.')\n",
    "plt.xlabel('y$_2$ predicted')\n",
    "plt.ylabel('y$_2$ target')\n",
    "plt.legend()\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(o2,y2_test, label=f'R$^2$ = {r2_score(o2, y2_test):.5}', marker='.')\n",
    "plt.xlabel('y$_3$ predicted')\n",
    "plt.ylabel('y$_3$ target')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Retraining\n",
    "As a final step we retrain our model again wit **all the data**, including the internal test set.\n",
    "We do this **after** model assessment, in this way we do not violate the gold rule!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('ML-CUP23-TR.csv', delimiter=',')\n",
    "X = dataset[:,1:11]\n",
    "y = dataset[:,11:14]\n",
    "best_hyperparams = ['rbf', 2800, 0.05]\n",
    "model = MultiSVM(*best_hyperparams)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load blind Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('ML-CUP23-TS.csv', delimiter=',')\n",
    "X_blind = dataset[:,1:11]\n",
    "y_blind = model.predict(X_blind)\n",
    "id = np.arange(1,len(y_blind)+1, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save outputs in the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "file_path = 'Cantucci_e_Soppressata_ML-CUP-23-TS.csv'\n",
    "\n",
    "df = pd.DataFrame(y_blind, index=id, columns=['Column1', 'Column2', 'Column3'])\n",
    "df.to_csv(file_path, header=False)\n",
    "\n",
    "# Data to be added at the beginning\n",
    "new_lines = [\n",
    "    ['# Noemi Boni',' Lorenzo Zaffina'],\n",
    "    ['# Cantucci_e_Soppressata'],\n",
    "    ['# ML-CUP23'],\n",
    "    ['# 30/01/2024']\n",
    "]\n",
    "\n",
    "# Read the existing content of the CSV file\n",
    "with open(file_path, 'r', newline='') as csvfile:\n",
    "    existing_content = list(csv.reader(csvfile))\n",
    "\n",
    "# Insert the new lines at the beginning\n",
    "existing_content[:0] = new_lines\n",
    "\n",
    "# Write the updated content back to the CSV file\n",
    "with open(file_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerows(existing_content)\n",
    "\n",
    "print(f'Data has been appended to the beginning of {file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
