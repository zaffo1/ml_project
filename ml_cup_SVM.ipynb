{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_euclidean_error(vectors1, vectors2):\n",
    "    \"\"\"\n",
    "    Compute the mean Euclidean error between two sets of 3D vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - vectors1: NumPy array of shape (N, 3) representing the first set of 3D vectors\n",
    "    - vectors2: NumPy array of shape (N, 3) representing the second set of 3D vectors\n",
    "\n",
    "    Returns:\n",
    "    - mean_error: Mean Euclidean error between the two sets of vectors\n",
    "    \"\"\"\n",
    "    # Check if the input arrays have the correct shape\n",
    "    if vectors1.shape != vectors2.shape or vectors1.shape[1] != 3:\n",
    "        raise ValueError(\"Input arrays must be of shape (N, 3)\")\n",
    "\n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = np.linalg.norm(vectors1 - vectors2, axis=1)\n",
    "\n",
    "    # Calculate the mean Euclidean error\n",
    "    mean_error = np.mean(euclidean_distance)\n",
    "\n",
    "    return mean_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output is 3-dimensional, our model is composed of three SVMs.\n",
    "We create a class to make things easier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSVM:\n",
    "    \"\"\"\n",
    "    A wrapper class for multiple Support Vector Regressors (SVRs) with shared hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - kernel (str, optional): Specifies the kernel type ('linear', 'poly', 'rbf', 'sigmoid', etc.).\n",
    "    - C (float, optional): Regularization parameter. Controls the trade-off between smooth decision function and fitting the training data.\n",
    "    - epsilon (float, optional): Width of the epsilon-insensitive tube. Ignored errors within this range during training.\n",
    "    - degree (int, optional): Degree of the polynomial kernel. Only applicable for 'poly' kernel.\n",
    "\n",
    "    Attributes:\n",
    "    - svr0, svr1, svr2: Support Vector Regressors for each target dimension.\n",
    "\n",
    "    Methods:\n",
    "    - fit(X, y): Fit each SVR on its respective target dimension.\n",
    "    - predict(X): Make predictions using each SVR and return a stacked array of predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel='rbf', C=1.0, epsilon=0.1, degree=3):\n",
    "        # Create three support vector regressors with the specified kernel, regularization parameter, and epsilon\n",
    "        self.svr0 = SVR(kernel=kernel, C=C, epsilon=epsilon, degree=degree)\n",
    "        self.svr1 = SVR(kernel=kernel, C=C, epsilon=epsilon, degree=degree)\n",
    "        self.svr2 = SVR(kernel=kernel, C=C, epsilon=epsilon, degree=degree)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Fit each SVR on its respective data\n",
    "        self.svr0.fit(X, y[:,0])\n",
    "        self.svr1.fit(X, y[:,1])\n",
    "        self.svr2.fit(X, y[:,2])\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using each SVR\n",
    "        pred = np.column_stack((self.svr0.predict(X),self.svr1.predict(X),self.svr2.predict(X)))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search_kfold(kernels, Cs, epsilons, degrees, k_folds, x, y, return_sequence=False):\n",
    "    \"\"\"\n",
    "    Perform grid search with k-fold cross-validation for Support Vector Regression hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - kernels (list): List of kernel types to search.\n",
    "    - Cs (list): List of regularization parameters to search.\n",
    "    - epsilons (list): List of epsilon values to search.\n",
    "    - degrees (list): List of degrees for polynomial kernels.\n",
    "    - k_folds (int): Number of folds for cross-validation.\n",
    "    - x (numpy.ndarray): Input data.\n",
    "    - y (numpy.ndarray): Target data.\n",
    "    - return_sequence (bool, optional): Whether to return the mean Euclidean error sequence (default: False).\n",
    "\n",
    "    Returns:\n",
    "    - list or tuple: Best hyperparameters or tuple with best hyperparameters\n",
    "        and sequences of mean Euclidean errors.\n",
    "\n",
    "    The function performs grid search with k-fold cross-validation for\n",
    "    Support Vector Regression hyperparameters and returns the best hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    mee_sequence = []\n",
    "    mee_sd_sequence = []\n",
    "    best_mee = float('inf')\n",
    "    best_mee_std = 0\n",
    "    best_hyperparams = []\n",
    "    counter = 0\n",
    "    num_combinations = sum(1 for _ in product(kernels, Cs,epsilons, degrees))\n",
    "    print('total number of grid search combinations explored:',num_combinations)\n",
    "    for kernel, C,epsilon,  degree in product(kernels, Cs,epsilons, degrees):\n",
    "        counter += 1\n",
    "        print(f'{counter}/{num_combinations} Hyperparams:',kernel, C, epsilon, degree)\n",
    "\n",
    "        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "        val_mees = []\n",
    "        train_mees = []\n",
    "\n",
    "        # Perform K-fold cross-validation\n",
    "        for fold, (train_indices, val_indices) in enumerate(kf.split(x,y)):\n",
    "\n",
    "            # Split the data into training and validation (or test) sets\n",
    "            X_train, X_val = x[train_indices], x[val_indices]\n",
    "            y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "            model = MultiSVM(kernel, C, epsilon, degree)\n",
    "            model.fit(X_train,y_train)\n",
    "            val_mees.append(mean_euclidean_error(model.predict(X_val),y_val))\n",
    "            train_mees.append(mean_euclidean_error(model.predict(X_train),y_train))\n",
    "        print(f'Final Results: kernel={kernel}; C={C}; epsilon={epsilon}; deg = {degree} --> '\n",
    "            f'train_mee = {np.mean(train_mees):.4} +- {np.std(train_mees):.4}')\n",
    "        print(f'Final Results: kernel={kernel}; C={C}; epsilon={epsilon}; deg = {degree} --> '\n",
    "            f'val_mee = {np.mean(val_mees):.4} +- {np.std(val_mees):.4}')\n",
    "\n",
    "        mee_sequence.append(np.mean(val_mees))\n",
    "        mee_sd_sequence.append(np.std(val_mees))\n",
    "        if np.mean(val_mees) < best_mee:\n",
    "            best_mee = np.mean(val_mees)\n",
    "            best_mee_std = np.std(val_mees)\n",
    "            best_hyperparams = [kernel, C, epsilon, degree]\n",
    "\n",
    "    print(f'Best Hp: {best_hyperparams} with MEE = {best_mee} +- {best_mee_std}')\n",
    "    if return_sequence:\n",
    "        return best_hyperparams, np.array(mee_sequence), np.array(mee_sd_sequence)\n",
    "    else:\n",
    "        return best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_std(x,mee,std, label, color):\n",
    "    \"\"\"\n",
    "    Plot mean Euclidean error (MEE) along with standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - x (array-like): Values of the hyperparameter C or epsilon.\n",
    "    - mee (array-like): Mean Euclidean error values.\n",
    "    - std (array-like): Standard deviation of Euclidean error values.\n",
    "    - color (str): Color for the plot.\n",
    "\n",
    "    The function plots the mean Euclidean error (MEE) along with its\n",
    "    standard deviation based on the values of the hyperparameter C or epsilon.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(9, 8))\n",
    "    plt.plot(x,mee, label='MEE $\\pm$ std. (results of k-fold cross validation)', color = color, linewidth=1)\n",
    "    plt.fill_between(x,mee-std, mee+std, color=color, alpha=0.3)\n",
    "\n",
    "    plt.xlabel(f'{label} values')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('ML-CUP23-TR.csv', delimiter=',')\n",
    "X = dataset[:,1:11]\n",
    "y = dataset[:,11:14]\n",
    "\n",
    "# Split the data into training and testing sets (80%/20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coarse Grid Searches (to find best kernel)\n",
    "we perform two grid searches. One with the rbf kernel, and one with polinomial kernel, and in this latter case we include the degree of the polinomial as hyperparameter. We compare the results of k-fold cross validation to choose the kernel most suitable for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of grid search combinations explored: 648\n",
      "1/648 Hyperparams: poly 0.01 0.01 3\n",
      "Final Results: kernel=poly; C=0.01; epsilon=0.01; deg = 3 --> val_mee = 37.66 +- 1.629\n",
      "2/648 Hyperparams: poly 0.01 0.01 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results: kernel=poly; C=0.01; epsilon=0.01; deg = 4 --> val_mee = 38.12 +- 1.643\n",
      "3/648 Hyperparams: poly 0.01 0.01 5\n",
      "Final Results: kernel=poly; C=0.01; epsilon=0.01; deg = 5 --> val_mee = 37.92 +- 1.641\n",
      "4/648 Hyperparams: poly 0.01 0.01 6\n",
      "Final Results: kernel=poly; C=0.01; epsilon=0.01; deg = 6 --> val_mee = 38.15 +- 1.644\n",
      "5/648 Hyperparams: poly 0.01 0.01 7\n",
      "Final Results: kernel=poly; C=0.01; epsilon=0.01; deg = 7 --> val_mee = 38.04 +- 1.641\n",
      "6/648 Hyperparams: poly 0.01 0.01 8\n",
      "Final Results: kernel=poly; C=0.01; epsilon=0.01; deg = 8 --> val_mee = 38.18 +- 1.639\n",
      "7/648 Hyperparams: poly 0.01 0.01 9\n",
      "Final Results: kernel=poly; C=0.01; epsilon=0.01; deg = 9 --> val_mee = 38.1 +- 1.639\n",
      "8/648 Hyperparams: poly 0.01 0.01 10\n",
      "Final Results: kernel=poly; C=0.01; epsilon=0.01; deg = 10 --> val_mee = 38.18 +- 1.635\n",
      "9/648 Hyperparams: poly 0.01 0.01 11\n",
      "Final Results: kernel=poly; C=0.01; epsilon=0.01; deg = 11 --> val_mee = 38.12 +- 1.634\n",
      "10/648 Hyperparams: poly 0.01 0.01 12\n",
      "Final Results: kernel=poly; C=0.01; epsilon=0.01; deg = 12 --> val_mee = 38.17 +- 1.63\n",
      "11/648 Hyperparams: poly 0.01 0.01 13\n",
      "Final Results: kernel=poly; C=0.01; epsilon=0.01; deg = 13 --> val_mee = 38.12 +- 1.631\n",
      "12/648 Hyperparams: poly 0.01 0.01 14\n",
      "Final Results: kernel=poly; C=0.01; epsilon=0.01; deg = 14 --> val_mee = 38.15 +- 1.63\n",
      "13/648 Hyperparams: poly 0.01 0.01 15\n",
      "Final Results: kernel=poly; C=0.01; epsilon=0.01; deg = 15 --> val_mee = 38.09 +- 1.631\n",
      "14/648 Hyperparams: poly 0.01 0.01 16\n",
      "Final Results: kernel=poly; C=0.01; epsilon=0.01; deg = 16 --> val_mee = 38.1 +- 1.629\n",
      "15/648 Hyperparams: poly 0.01 0.01 17\n",
      "Final Results: kernel=poly; C=0.01; epsilon=0.01; deg = 17 --> val_mee = 38.04 +- 1.629\n",
      "16/648 Hyperparams: poly 0.01 0.01 18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m epsilons \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.01\u001b[39m,\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#Best Hp: ['poly', 1000, 0.01, 5] with MEE = 0.8183841594881777 +- 0.12792331527745304\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m best_hyperparams \u001b[38;5;241m=\u001b[39m \u001b[43mperform_grid_search_kfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mepsilons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdegrees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mk_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                          \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 46\u001b[0m, in \u001b[0;36mperform_grid_search_kfold\u001b[0;34m(kernels, Cs, epsilons, degrees, k_folds, x, y, return_sequence)\u001b[0m\n\u001b[1;32m     43\u001b[0m     y_train, y_val \u001b[38;5;241m=\u001b[39m y[train_indices], y[val_indices]\n\u001b[1;32m     45\u001b[0m     model \u001b[38;5;241m=\u001b[39m MultiSVM(kernel, C, epsilon, degree)\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     val_mees\u001b[38;5;241m.\u001b[39mappend(mean_euclidean_error(model\u001b[38;5;241m.\u001b[39mpredict(X_val),y_val))\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal Results: kernel=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; C=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; epsilon=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepsilon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; deg = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdegree\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m --> \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_mee = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(val_mees)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m +- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(val_mees)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 27\u001b[0m, in \u001b[0;36mMultiSVM.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Fit each SVR on its respective data\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvr0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvr1\u001b[38;5;241m.\u001b[39mfit(X, y[:,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvr2\u001b[38;5;241m.\u001b[39mfit(X, y[:,\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/Magistrale/ML/env_ml/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Magistrale/ML/env_ml/lib/python3.10/site-packages/sklearn/svm/_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[0;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[0;32m~/Desktop/Magistrale/ML/env_ml/lib/python3.10/site-packages/sklearn/svm/_base.py:329\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    315\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[1;32m    319\u001b[0m (\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[0;32m--> 329\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_class_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "File \u001b[0;32msklearn/svm/_libsvm.pyx:265\u001b[0m, in \u001b[0;36msklearn.svm._libsvm.fit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "#first coarse grid search\n",
    "kernels = ['rbf']\n",
    "Cs = [0.01,0.1,1,10,100,1000]\n",
    "epsilons = [0.01,0.1,1,10]\n",
    "degrees=[0] #only relevant for poly kernel\n",
    "#Best Hp: ['rbf', 1000, 0.1] with MEE = 0.6396615599226871 +- 0.04980493197143183\n",
    "'''\n",
    "\n",
    "kernels = ['poly']\n",
    "Cs = [0.01,0.1,1,10,100,1000]\n",
    "degrees = np.arange(3,30,1)\n",
    "epsilons = [0.01,0.1,1,10]\n",
    "#Best Hp: ['poly', 1000, 0.01, 5] with MEE = 0.8183841594881777 +- 0.12792331527745304\n",
    "\n",
    "best_hyperparams = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          epsilons,\n",
    "                          degrees,\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finer Grid Search\n",
    "the best kernel is rbf, now let's study the other hyperparameters more in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#finer grid search\n",
    "kernels = ['rbf']\n",
    "Cs = [1000,2000,3000,4000,5000]\n",
    "epsilons = [0.1,0.2,0.3,0.4,0.5]\n",
    "# Best Hp: ['rbf', 3000, 0.1] with MEE = 0.6172759223965806 +- 0.06576969897515686\n",
    "'''\n",
    "\n",
    "'''\n",
    "#best hp untill now:\n",
    "kernels = ['rbf']\n",
    "Cs = [3000]\n",
    "epsilons = [0.1]\n",
    "'''\n",
    "\n",
    "best_hyperparams = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          epsilons,\n",
    "                          degrees,\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the effect of the parameter C.\n",
    "Keeping $\\varepsilon$ fixed, we want to study the effect of C on the validation error. \n",
    "- Low C ---> many TR errors are allowed (risk of underfitting)\n",
    "- High C ---> less TR errors allowed (risk of overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['rbf']\n",
    "Cs = np.arange(200, 10000, 200)\n",
    "epsilons = [0.1]\n",
    "#Best Hp: ['rbf', 2800, 0.1] with MEE = 0.6164267250857903 +- 0.0641407687516038\n",
    "\n",
    "best_hyperparams, mees, mees_sd = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          epsilons,\n",
    "                          degrees=[0],\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train,\n",
    "                          return_sequence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the values of mees changing the values of C\n",
    "plot_mean_std(Cs,mees,mees_sd, label = 'C', color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the effect of the parameter $\\varepsilon$.\n",
    "Keeping C fixed, we want to study the effect of $\\varepsilon$ on the validation error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['rbf']\n",
    "Cs = [2800]\n",
    "epsilons = np.arange(0.01,0.2,0.01)\n",
    "#Best Hp: ['rbf', 2800, 0.1] with MEE = 0.6164267250857903 +- 0.0641407687516038\n",
    "\n",
    "best_hyperparams, mees, mees_sd = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          epsilons,\n",
    "                          degrees=[0],\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train,\n",
    "                          return_sequence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the values of mees changing the values of C\n",
    "plot_mean_std(epsilons,mees,mees_sd, label='$\\epsilon$', color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of grid search combinations explored: 1\n",
      "1/1 Hyperparams: rbf 2800 0.05 0\n",
      "Final Results: kernel=rbf; C=2800; epsilon=0.05; deg = 0 --> train_mee = 0.1696 +- 0.003877\n",
      "Final Results: kernel=rbf; C=2800; epsilon=0.05; deg = 0 --> val_mee = 0.6118 +- 0.06344\n",
      "Best Hp: ['rbf', 2800, 0.05, 0] with MEE = 0.6118105186695858 +- 0.06344420890556922\n"
     ]
    }
   ],
   "source": [
    "#final model from model selection:\n",
    "kernels = ['rbf']\n",
    "Cs = [2800]\n",
    "epsilons = [0.05]\n",
    "\n",
    "best_hyperparams, mees, mees_sd = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          epsilons,\n",
    "                          degrees=[0],\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train,\n",
    "                          return_sequence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the final model on all the training data\n",
    "Finally, we train our best model on all the training data.\n",
    "We test its performances on our internal test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MEE: 0.18916257679159393\n",
      "Test MEE: 0.5096504558449603\n"
     ]
    }
   ],
   "source": [
    "best_hyperparams = ['rbf', 2800, 0.05]\n",
    "model = MultiSVM(*best_hyperparams)\n",
    "model.fit(X_train,y_train)\n",
    "print('Test MEE:',mean_euclidean_error(model.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load blind Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('ML-CUP23-TS.csv', delimiter=',')\n",
    "X_blind = dataset[:,1:11]\n",
    "y_blind = model.predict(X_blind)\n",
    "id = np.arange(1,len(y_blind)+1, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save outputs in the required format\n",
    "! remember to insert the correct date !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been appended to the beginning of output_cup.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "file_path = 'output_cup.csv'\n",
    "\n",
    "df = pd.DataFrame(y_blind, index=id, columns=['Column1', 'Column2', 'Column3'])\n",
    "df.to_csv(file_path, header=False)\n",
    "\n",
    "# Data to be added at the beginning\n",
    "new_lines = [\n",
    "    ['# Noemi Boni',' Lorenzo Zaffina'],\n",
    "    ['# Cantucci_e_Soppressata'],\n",
    "    ['# ML-CUP23'],\n",
    "    ['# 30/01/2024']\n",
    "]\n",
    "\n",
    "# Read the existing content of the CSV file\n",
    "with open(file_path, 'r', newline='') as csvfile:\n",
    "    existing_content = list(csv.reader(csvfile))\n",
    "\n",
    "# Insert the new lines at the beginning\n",
    "existing_content[:0] = new_lines\n",
    "\n",
    "# Write the updated content back to the CSV file\n",
    "with open(file_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerows(existing_content)\n",
    "\n",
    "print(f'Data has been appended to the beginning of {file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
