{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_euclidean_error(vectors1, vectors2):\n",
    "    \"\"\"\n",
    "    Compute the mean Euclidean error between two sets of 3D vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - vectors1: NumPy array of shape (N, 3) representing the first set of 3D vectors\n",
    "    - vectors2: NumPy array of shape (N, 3) representing the second set of 3D vectors\n",
    "\n",
    "    Returns:\n",
    "    - mean_error: Mean Euclidean error between the two sets of vectors\n",
    "    \"\"\"\n",
    "    # Check if the input arrays have the correct shape\n",
    "    if vectors1.shape != vectors2.shape or vectors1.shape[1] != 3:\n",
    "        raise ValueError(\"Input arrays must be of shape (N, 3)\")\n",
    "\n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = np.linalg.norm(vectors1 - vectors2, axis=1)\n",
    "\n",
    "    # Calculate the mean Euclidean error\n",
    "    mean_error = np.mean(euclidean_distance)\n",
    "\n",
    "    return mean_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSVM:\n",
    "    \"\"\"\n",
    "    A wrapper class for multiple Support Vector Regressors (SVRs) with shared hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - kernel (str, optional): Specifies the kernel type ('linear', 'poly', 'rbf', 'sigmoid', etc.).\n",
    "    - C (float, optional): Regularization parameter. Controls the trade-off between smooth decision function and fitting the training data.\n",
    "    - epsilon (float, optional): Width of the epsilon-insensitive tube. Ignored errors within this range during training.\n",
    "    - degree (int, optional): Degree of the polynomial kernel. Only applicable for 'poly' kernel.\n",
    "\n",
    "    Attributes:\n",
    "    - svr0, svr1, svr2: Support Vector Regressors for each target dimension.\n",
    "\n",
    "    Methods:\n",
    "    - fit(X, y): Fit each SVR on its respective target dimension.\n",
    "    - predict(X): Make predictions using each SVR and return a stacked array of predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel='rbf', C=1.0, epsilon=0.1, degree=3):\n",
    "        # Create three support vector regressors with the specified kernel, regularization parameter, and epsilon\n",
    "        self.svr0 = SVR(kernel=kernel, C=C, epsilon=epsilon, degree=degree)\n",
    "        self.svr1 = SVR(kernel=kernel, C=C, epsilon=epsilon, degree=degree)\n",
    "        self.svr2 = SVR(kernel=kernel, C=C, epsilon=epsilon, degree=degree)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Fit each SVR on its respective data\n",
    "        self.svr0.fit(X, y[:,0])\n",
    "        self.svr1.fit(X, y[:,1])\n",
    "        self.svr2.fit(X, y[:,2])\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using each SVR\n",
    "        pred = np.column_stack((self.svr0.predict(X),self.svr1.predict(X),self.svr2.predict(X)))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search_kfold(kernels, Cs, epsilons, degrees, k_folds, x, y, return_sequence=False):\n",
    "    \"\"\"\n",
    "    Perform grid search with k-fold cross-validation for Support Vector Regression hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - kernels (list): List of kernel types to search.\n",
    "    - Cs (list): List of regularization parameters to search.\n",
    "    - epsilons (list): List of epsilon values to search.\n",
    "    - degrees (list): List of degrees for polynomial kernels.\n",
    "    - k_folds (int): Number of folds for cross-validation.\n",
    "    - x (numpy.ndarray): Input data.\n",
    "    - y (numpy.ndarray): Target data.\n",
    "    - return_sequence (bool, optional): Whether to return the mean Euclidean error sequence (default: False).\n",
    "\n",
    "    Returns:\n",
    "    - list or tuple: Best hyperparameters or tuple with best hyperparameters\n",
    "        and sequences of mean Euclidean errors.\n",
    "\n",
    "    The function performs grid search with k-fold cross-validation for\n",
    "    Support Vector Regression hyperparameters and returns the best hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    mee_sequence = []\n",
    "    mee_sd_sequence = []\n",
    "    best_mee = float('inf')\n",
    "    best_mee_std = 0\n",
    "    best_hyperparams = []\n",
    "    counter = 0\n",
    "    num_combinations = sum(1 for _ in product(kernels, Cs,epsilons, degrees))\n",
    "    print('total number of grid search combinations explored:',num_combinations)\n",
    "    for kernel, C,epsilon,  degree in product(kernels, Cs,epsilons, degrees):\n",
    "        counter += 1\n",
    "        print(f'{counter}/{num_combinations} Hyperparams:',kernel, C, epsilon, degree)\n",
    "\n",
    "        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "        val_mees = []\n",
    "\n",
    "        # Perform K-fold cross-validation\n",
    "        for fold, (train_indices, val_indices) in enumerate(kf.split(x,y)):\n",
    "\n",
    "            # Split the data into training and validation (or test) sets\n",
    "            X_train, X_val = x[train_indices], x[val_indices]\n",
    "            y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "            model = MultiSVM(kernel, C, epsilon, degree)\n",
    "            model.fit(X_train,y_train)\n",
    "            val_mees.append(mean_euclidean_error(model.predict(X_val),y_val))\n",
    "\n",
    "        print(f'Final Results: kernel={kernel}; C={C}; epsilon={epsilon}; deg = {degree} --> '\n",
    "            f'val_mee = {np.mean(val_mees):.4} +- {np.std(val_mees):.4}')\n",
    "\n",
    "        mee_sequence.append(np.mean(val_mees))\n",
    "        mee_sd_sequence.append(np.std(val_mees))\n",
    "        if np.mean(val_mees) < best_mee:\n",
    "            best_mee = np.mean(val_mees)\n",
    "            best_mee_std = np.std(val_mees)\n",
    "            best_hyperparams = [kernel, C, epsilon, degree]\n",
    "\n",
    "    print(f'Best Hp: {best_hyperparams} with MEE = {best_mee} +- {best_mee_std}')\n",
    "    if return_sequence:\n",
    "        return best_hyperparams, np.array(mee_sequence), np.array(mee_sd_sequence)\n",
    "    else:\n",
    "        return best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_std(x,mee,std, label, color):\n",
    "    \"\"\"\n",
    "    Plot mean Euclidean error (MEE) along with standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - x (array-like): Values of the hyperparameter C or epsilon.\n",
    "    - mee (array-like): Mean Euclidean error values.\n",
    "    - std (array-like): Standard deviation of Euclidean error values.\n",
    "    - color (str): Color for the plot.\n",
    "\n",
    "    The function plots the mean Euclidean error (MEE) along with its\n",
    "    standard deviation based on the values of the hyperparameter C or epsilon.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(9, 8))\n",
    "    plt.plot(x,mee, label='MEE $\\pm$ std. (results of k-fold cross validation)', color = color, linewidth=1)\n",
    "    plt.fill_between(x,mee-std, mee+std, color=color, alpha=0.3)\n",
    "\n",
    "    plt.xlabel(f'{label} values')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('ML-CUP23-TR.csv', delimiter=',')\n",
    "X = dataset[:,1:11]\n",
    "y = dataset[:,11:14]\n",
    "\n",
    "# Split the data into training and testing sets (80%/20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coarse Grid Searches (to find best kernel)\n",
    "we perform two grid searches. One with the rbf kernel, and one with polinomial kernel, and in this latter case we include the degree of the polinomial as hyperparameter. We compare the results of k-fold cross validation to choose the kernel most suitable for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#first coarse grid search\n",
    "kernels = ['rbf']\n",
    "Cs = [0.01,0.1,1,10,100,1000]\n",
    "epsilons = [0.01,0.1,1,10]\n",
    "degrees=[0] #only relevant for poly kernel\n",
    "#Best Hp: ['rbf', 1000, 0.1] with MEE = 0.6396615599226871 +- 0.04980493197143183\n",
    "'''\n",
    "\n",
    "kernels = ['poly']\n",
    "Cs = [0.01,0.1,1,10,100,1000]\n",
    "degrees = np.arange(3,30,1)\n",
    "epsilons = [0.01,0.1,1,10]\n",
    "#Best Hp: ['poly', 1000, 0.01, 5] with MEE = 0.8183841594881777 +- 0.12792331527745304\n",
    "\n",
    "best_hyperparams = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          epsilons,\n",
    "                          degrees,\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finer Grid Search\n",
    "the best kernel is rbf, now let's study the other hyperparameters more in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#finer grid search\n",
    "kernels = ['rbf']\n",
    "Cs = [1000,2000,3000,4000,5000]\n",
    "epsilons = [0.1,0.2,0.3,0.4,0.5]\n",
    "# Best Hp: ['rbf', 3000, 0.1] with MEE = 0.6172759223965806 +- 0.06576969897515686\n",
    "'''\n",
    "\n",
    "'''\n",
    "#best hp untill now:\n",
    "kernels = ['rbf']\n",
    "Cs = [3000]\n",
    "epsilons = [0.1]\n",
    "'''\n",
    "\n",
    "best_hyperparams = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          epsilons,\n",
    "                          degrees,\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the effect of the parameter C.\n",
    "Keeping $\\varepsilon$ fixed, we want to study the effect of C on the validation error. \n",
    "- Low C ---> many TR errors are allowed (risk of underfitting)\n",
    "- High C ---> less TR errors allowed (risk of overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['rbf']\n",
    "Cs = np.arange(200, 10000, 200)\n",
    "epsilons = [0.1]\n",
    "#Best Hp: ['rbf', 2800, 0.1] with MEE = 0.6164267250857903 +- 0.0641407687516038\n",
    "\n",
    "best_hyperparams, mees, mees_sd = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          epsilons,\n",
    "                          degrees=[0],\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train,\n",
    "                          return_sequence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the values of mees changing the values of C\n",
    "plot_mean_std(Cs,mees,mees_sd, label = 'C', color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the effect of the parameter $\\varepsilon$.\n",
    "Keeping C fixed, we want to study the effect of $\\varepsilon$ on the validation error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['rbf']\n",
    "Cs = [2800]\n",
    "epsilons = np.arange(0.01,0.2,0.01)\n",
    "#Best Hp: ['rbf', 2800, 0.1] with MEE = 0.6164267250857903 +- 0.0641407687516038\n",
    "\n",
    "best_hyperparams, mees, mees_sd = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          epsilons,\n",
    "                          degrees=[0],\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train,\n",
    "                          return_sequence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the values of mees changing the values of C\n",
    "plot_mean_std(epsilons,mees,mees_sd, label='$\\epsilon$', color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the final model on all the training data\n",
    "Finally, we train our best model on all the training data.\n",
    "We test its performances on our internal test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams = ['rbf', 2800, 0.05]\n",
    "model = MultiSVM(*best_hyperparams)\n",
    "model.fit(X_train,y_train)\n",
    "print('Test MEE:',mean_euclidean_error(model.predict(X_test),y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
