{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML CUP - Neural Networks\n",
    "Until now we found the hyperparameters of our best model so far.\n",
    "We now keep these hyperparameter fixed and try to modify the proportions of neurons in each layer of the net,\n",
    "to see if this has some impact to the model performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)   # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)   # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)    # fontsize of the figure suptitle\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(train_losses, test_losses, train_mees, test_mees, hyperparams):\n",
    "    \"\"\"\n",
    "    Plot training and test curves for loss and Mean Euclidean Error (MEE).\n",
    "\n",
    "    Parameters:\n",
    "    - epoch (int): The total number of training epochs.\n",
    "    - train_losses (list): List of training losses for each epoch.\n",
    "    - test_losses (list): List of test losses for each epoch.\n",
    "    - train_mees (list): List of training MEE values for each epoch.\n",
    "    - test_mees (list): List of test MEE values for each epoch.\n",
    "    - hyperparams (list): List of hyperparameters used for the plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    Plots four subplots:\n",
    "    1. Training and test loss curves.\n",
    "    2. Training and test MEE curves.\n",
    "    3. Zoomed-in training and test loss curves with y-axis limit [0, 10].\n",
    "    4. Zoomed-in training and test MEE curves with y-axis limit [0, 10].\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #plt.suptitle(f'Batch Size={hyperparams[3]},Activation Function={hyperparams[5]}, Layers={hyperparams[6]} Hidden Units={hyperparams[0]}, Eta={hyperparams[1]}, Alpha={hyperparams[2]}, Lambda={hyperparams[4]}, dropout = {hyperparams[7]}')\n",
    "    # Loss plots\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss', color = 'red')\n",
    "    plt.plot(test_losses, label='Validation Loss', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "\n",
    "    # MEE plots\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(train_mees, label='Training MEE', color='red')\n",
    "    plt.plot(test_mees, label='Validation MEE', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss plots\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(train_losses, label='Training Loss', color = 'red')\n",
    "    plt.plot(test_losses, label='Validation Loss', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "\n",
    "    # MEE plots\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(train_mees, label='Training MEE', color='red')\n",
    "    plt.plot(test_mees, label='Validation MEE', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_euclidean_error(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Compute the mean Euclidean error between two sets of 3D vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor1: PyTorch tensor of size (N, 3) representing the first set of 3D vectors\n",
    "    - tensor2: PyTorch tensor of size (N, 3) representing the second set of 3D vectors\n",
    "\n",
    "    Returns:\n",
    "    - mean_error: Mean Euclidean error between the two sets of vectors\n",
    "    \"\"\"\n",
    "    # Check if the tensors have the correct shape\n",
    "    if tensor1.shape[1] != 3 or tensor2.shape[1] != 3 or tensor1.shape[0] != tensor2.shape[0]:\n",
    "        raise ValueError(\"Input tensors must be of size (N, 3)\")\n",
    "\n",
    "\n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = torch.norm(tensor1 - tensor2, dim=1)\n",
    "\n",
    "    # Calculate the mean Euclidean error\n",
    "    mean_error = torch.mean(euclidean_distance)\n",
    "\n",
    "    return mean_error.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regression neural network\n",
    "# in this case we keep the number of layers fixed at 3,\n",
    "# and we change hoe the neurons are distributed between layers\n",
    "\n",
    "class RegressorNN(nn.Module):\n",
    "    def __init__(self, hidden_sizes, activation_function, input_size=10, output_size=3):\n",
    "        super(RegressorNN, self).__init__()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers = [nn.Linear(input_size, hidden_sizes[0])]\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_sizes[0], hidden_sizes[1]))\n",
    "        self.layers.append(activation_function)\n",
    "        self.layers.append(nn.Linear(hidden_sizes[1], hidden_sizes[2]))\n",
    "        self.layers.append(activation_function)\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[2], output_size))\n",
    "\n",
    "        # Create a Sequential container for the layers\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def training_model(x_train, y_train, x_test, y_test, neuron_number,learning_rate, momentum, bs, reg_coeff, activation, optimiz, proportions, num_epochs=1000, plot_curves=False, return_history=False):\n",
    "    \"\"\"\n",
    "    Train the regression model and evaluate it on a test (can also be validation, depending on the context).\n",
    "\n",
    "    Parameters:\n",
    "    - x_train (torch.Tensor): Training input data.\n",
    "    - y_train (torch.Tensor): Training target data.\n",
    "    - x_test (torch.Tensor): Test/Validation input data.\n",
    "    - y_test (torch.Tensor): Test/Validation target data.\n",
    "    - neuron_number (int): Total number of neurons across all layers.\n",
    "    - learning_rate (float): Learning rate for the optimizer.\n",
    "    - momentum (float): Momentum for the optimizer, applicable to certain optimizers.\n",
    "    - bs (int): Batch size for training.\n",
    "    - reg_coeff (float): Regularization coefficient for weight decay.\n",
    "    - activation (torch.nn.Module): Activation function for the model.\n",
    "    - optimiz (str): Optimizer type ('SGD', 'Adam', or 'RMS').\n",
    "    - proportions (list): Proportions of neurons in each layer relative to `neuron_number`.\n",
    "    - num_epochs (int, optional): Number of training epochs (default: 1000).\n",
    "    - plot_curves (bool, optional): Whether to plot training and testing curves (default: False).\n",
    "    - return_history (bool, optional): Whether to return training and testing history (default: False).\n",
    "\n",
    "    Returns:\n",
    "    - model (torch.nn.Module): The trained model.\n",
    "    - int: Number of epochs completed.\n",
    "    - float: Final training loss.\n",
    "    - float: Final test loss.\n",
    "    - float: Final training Mean Euclidean Error (MEE).\n",
    "    - float: Final test MEE.\n",
    "    - (optional) numpy.ndarray: Training losses over epochs.\n",
    "    - (optional) numpy.ndarray: Test losses over epochs.\n",
    "    - (optional) numpy.ndarray: Training MEEs over epochs.\n",
    "    - (optional) numpy.ndarray: Test MEEs over epochs.\n",
    "\n",
    "    The function trains a neural network regression model using the specified hyperparameters and evaluates its performance on the test set.\n",
    "    \"\"\"\n",
    "    hidden_sizes = [0,0,0]\n",
    "\n",
    "    for i in range(len(proportions)):\n",
    "        hidden_sizes[i] = int(proportions[i]*neuron_number)\n",
    "        #print(hidden_sizes)\n",
    "    #print(hidden_size)\n",
    "    # Create an instance of the model\n",
    "    model = RegressorNN(hidden_sizes=hidden_sizes, activation_function=activation)\n",
    "    model.to(device)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    if optimiz == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=reg_coeff)\n",
    "    if optimiz == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg_coeff)\n",
    "    if optimiz == 'RMS':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate,momentum=momentum, weight_decay=reg_coeff)\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "    # Lists to store training and test losses for plotting\n",
    "    train_losses, test_losses, train_mees, test_mees = [], [], [], []\n",
    "\n",
    "    # parameters to stop at training convergence\n",
    "    min_mee = float('inf')\n",
    "    patience_counter, patience = 0, 20\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        for inputs, labels in train_dataloader:\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels) #Compute the loss\n",
    "\n",
    "            optimizer.zero_grad()   # Zero the gradients\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        #calculate loss\n",
    "        y_pred = model(x_train)\n",
    "        train_loss = criterion(y_pred, y_train)\n",
    "        # Calculate mee\n",
    "        train_mee = mean_euclidean_error(y_pred,y_train)\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_mees.append(train_mee)\n",
    "\n",
    "        # Evaluation on the test/validation set\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(x_test)\n",
    "            test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "            # Calculate test mee\n",
    "            test_mee = mean_euclidean_error(test_outputs,y_test)\n",
    "            test_mees.append(test_mee)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "        print(f'Training - Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss.item():.4f}, '\n",
    "            f'MEE: {train_mee:.4f} | Test - Epoch [{epoch+1}/{num_epochs}], '\n",
    "            f'Loss: {test_loss.item():.4f} MEE: {test_mee:.4f} ', end='\\r')\n",
    "\n",
    "        # Check for convergence\n",
    "        if train_mee < min_mee and abs(train_mee-min_mee)>1e-3:\n",
    "            patience_counter = 0\n",
    "            min_mee = train_mee\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter == patience:\n",
    "            break\n",
    "\n",
    "        # Update the learning rate using the scheduler\n",
    "        scheduler.step(train_mee)\n",
    "\n",
    "    print(f'\\n N. Epochs = {epoch+1} - Loss (train | test)= ({train_loss.item():.4} | {test_loss.item():.4}) - MEE (train | test) = ({train_mee} | {test_mee})')\n",
    "\n",
    "    if plot_curves:\n",
    "        hyperparams = [hidden_sizes,learning_rate, momentum, bs, reg_coeff, activation, proportions, num_epochs]\n",
    "        plot_training_curves(train_losses, test_losses, train_mees, test_mees, hyperparams)\n",
    "\n",
    "    if return_history:\n",
    "        return model, epoch+1, train_loss.item(), test_loss.item(), train_mee, test_mee, np.array(train_losses), np.array(test_losses), np.array(train_mees), np.array(test_mees)\n",
    "    else:\n",
    "        return model, epoch+1, train_loss.item(), test_loss.item(), train_mee, test_mee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_different_initializations(x_train, y_train, x_test, y_test,\n",
    "                                         neuron_number,learning_rate, momentum,\n",
    "                                         bs, reg_coeff, activation,optimiz,proportions, max_num_epochs=1000,\n",
    "                                         plot_curves=False, N = 5):\n",
    "    \"\"\"\n",
    "    Train the model multiple times with different weight initializations to estimate performance mean and variance.\n",
    "\n",
    "     Parameters:\n",
    "    - x_train (torch.Tensor): Training input data.\n",
    "    - y_train (torch.Tensor): Training target data.\n",
    "    - x_test (torch.Tensor): Test7Validation input data.\n",
    "    - y_test (torch.Tensor): Test/Validation target data.\n",
    "    - neuron_number (int): Total number of neurons across all layers.\n",
    "    - learning_rate (float): Learning rate for the optimizer.\n",
    "    - momentum (float): Momentum for the optimizer, applicable to certain optimizers.\n",
    "    - bs (int): Batch size for training.\n",
    "    - reg_coeff (float): Regularization coefficient for weight decay.\n",
    "    - activation (torch.nn.Module): Activation function for the model.\n",
    "    - optimiz (str): Optimizer type ('SGD', 'Adam', or 'RMS').\n",
    "    - proportions (list): Proportions of neurons in each layer relative to `neuron_number`.\n",
    "    - max_num_epochs (int, optional): Maximum number of training epochs (default: 1000).\n",
    "    - plot_curves (bool, optional): Whether to plot training and testing curves (default: False).\n",
    "    - N (int, optional): Number of times to train the model with different initializations (default: 5).\n",
    "\n",
    "    Returns:\n",
    "    - float: Mean of the final training losses across all initializations.\n",
    "    - float: Mean of the final test losses across all initializations.\n",
    "    - float: Mean of the final training MEEs across all initializations.\n",
    "    - float: Mean of the final test MEEs across all initializations.\n",
    "\n",
    "\n",
    "    Prints the mean and standard deviation of training and test loss, as well as training and test Mean Euclidean Error (MEE).\n",
    "    \"\"\"\n",
    "    final_train_loss = []\n",
    "    final_test_loss = []\n",
    "    final_train_mee = []\n",
    "    final_test_mee = []\n",
    "    train_losses_hist, test_losses_hist, train_mees_hist, test_mees_hist = [[] for _ in range(N)],  [[] for _ in range(N)],  [[] for _ in range(N)],  [[] for _ in range(N)]\n",
    "    models = []\n",
    "    for i in range (0,N):\n",
    "        model , num_epochs,train_loss, test_loss, train_mee, test_mee, train_losses_hist[i], test_losses_hist[i], train_mees_hist[i], test_mees_hist[i] = training_model(x_train,\n",
    "                        y_train,\n",
    "                        x_test,\n",
    "                        y_test,\n",
    "                        neuron_number,\n",
    "                        learning_rate,\n",
    "                        momentum,\n",
    "                        bs,\n",
    "                        reg_coeff,\n",
    "                        activation,\n",
    "                        optimiz,\n",
    "                        proportions,\n",
    "                        plot_curves=plot_curves,\n",
    "                        num_epochs=max_num_epochs,\n",
    "                        return_history=True)\n",
    "\n",
    "        final_train_loss.append(train_loss)\n",
    "        final_test_loss.append(test_loss)\n",
    "        final_train_mee.append(train_mee)\n",
    "        final_test_mee.append(test_mee)\n",
    "        models.append(model)\n",
    "\n",
    "    if plot_curves:\n",
    "        hyperparams = [learning_rate, momentum, bs, reg_coeff, activation, num_epochs]\n",
    "        plot_training_curves(train_losses=train_losses_hist[i],test_losses=test_losses_hist[i],train_mees=train_mees_hist[i],test_mees=test_mees_hist[i],hyperparams=hyperparams)\n",
    "\n",
    "    plt.show()\n",
    "    print(f'Avg of {N} initializations: Loss (train | test/val )= ({np.mean(final_train_loss):.4} +- {np.std(final_train_loss):.4} | {np.mean(final_test_loss):.4} +- {np.std(final_test_loss):.4})'\n",
    "          f'- MEE (train | test/val ) = ( {np.mean(final_train_mee):.4} +-  {np.std(final_train_mee):.4} | {np.mean(final_test_mee):.4} +- {np.std(final_test_mee):.4})')\n",
    "\n",
    "\n",
    "    return np.mean(final_train_loss), np.mean(final_test_loss), np.mean(final_train_mee), np.mean(final_test_mee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search_kfold(neuron_numbers, learning_rates, momentums, batch_sizes, reg_coeffs, activations,optimiz,proportionss, k_folds, x, y,N=3, plot_curves=False, num_epochs=1000):\n",
    "    \"\"\"\n",
    "    Perform grid search with k-fold cross-validation for hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - neuron_numbers (list of int): List of the number of neurons in the hidden layer(s) to be evaluated.\n",
    "    - learning_rates (list of float): List of learning rates to be evaluated.\n",
    "    - momentums (list of float): List of momentum values to be evaluated in optimizer.\n",
    "    - batch_sizes (list of int): List of batch sizes to be used in training.\n",
    "    - reg_coeffs (list of float): List of regularization coefficients to control overfitting.\n",
    "    - activations (list of str): List of activation functions (e.g., 'relu', 'tanh') to be used in the neural network.\n",
    "    - optimiz (str): Type of optimizer to be used (e.g., 'SGD', 'Adam', 'RMSprop').\n",
    "    - proportions (list of float): List of proportions of neurons in each hidden layer relative to the input layer.\n",
    "    - k_folds (int): The number of folds to be used in k-fold cross-validation.\n",
    "    - x (numpy.ndarray): The input features of the dataset.\n",
    "    - y (numpy.ndarray): The target/output values of the dataset.\n",
    "    - N (int, optional): The number of different initializations for each set of hyperparameters (default: 3).\n",
    "    - plot_curves (bool, optional): Flag to plot training curves for loss and MEE (default: False).\n",
    "    - num_epochs (int, optional): The number of epochs for training the model (default: 1000).\n",
    "\n",
    "    Returns:\n",
    "    - list: A list containing the best hyperparameters based on the lowest average MEE across the k-fold validation.\n",
    "\n",
    "\n",
    "    The function performs grid search with k-fold cross-validation for Monk classifier hyperparameters and returns the best hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    best_mee = float('inf')\n",
    "    best_hyperparams = []\n",
    "\n",
    "    for neuron_number, learning_rate, momentum, bs, reg_coeff, activation, proportions in product(neuron_numbers,learning_rates,momentums,batch_sizes, reg_coeffs, activations, proportionss):\n",
    "        print(f'activation={activation};; neuron_number={neuron_number}; lr={learning_rate}; alpha = {momentum}; batch size = {bs}; lambda = {reg_coeff}; optim = {optimiz}; proportions = {proportions}')\n",
    "\n",
    "        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        # Lists to store training and validation losses and MEEs for each epoch\n",
    "        train_losses, val_losses, train_mees, val_mees = [], [], [], []\n",
    "\n",
    "        # Perform K-fold cross-validation\n",
    "        for fold, (train_indices, val_indices) in enumerate(kf.split(x,y)):\n",
    "            #print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "\n",
    "            # Split the data into training and validation (or test) sets\n",
    "            X_train, X_val = x[train_indices], x[val_indices]\n",
    "            Y_train, Y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "            train_loss, val_loss, train_mee, val_mee = train_with_different_initializations(X_train, Y_train, X_val, Y_val,\n",
    "                neuron_number=neuron_number, learning_rate=learning_rate, momentum=momentum,\n",
    "                bs=bs, reg_coeff=reg_coeff, activation=activation, proportions=proportions, optimiz=optimiz ,plot_curves=plot_curves, max_num_epochs=num_epochs, N=N)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_mees.append(train_mee)\n",
    "            val_mees.append(val_mee)\n",
    "\n",
    "        print(f'Final Results: activation={activation}; neuron number={neuron_number}; lr={learning_rate}; alpha = {momentum}; batch size = {bs}; lambda = {reg_coeff}; proportions = {proportions} --> '\n",
    "            f'train_loss = {np.mean(train_losses):.4} +- {np.std(train_losses):.4} | '\n",
    "            f'val_loss = {np.mean(val_losses):.4} +- {np.std(val_losses):.4}'\n",
    "            f'train_mee = {np.mean(train_mees):.4} +- {np.std(train_mees):.4} | '\n",
    "            f'val_mee = {np.mean(val_mees):.4} +- {np.std(val_mees):.4}')\n",
    "\n",
    "        if np.mean(val_mees) < best_mee:\n",
    "            best_mee = np.mean(val_mees)\n",
    "            best_hyperparams = [neuron_number, learning_rate, momentum, bs, reg_coeff, activation, proportions]\n",
    "\n",
    "    print(best_hyperparams)\n",
    "    return best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('ML-CUP23-TR.csv', delimiter=',')\n",
    "X = dataset[:,1:11]\n",
    "y = dataset[:,11:14]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# Split the data into training and testing sets (80%/20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERFORM GRID SEARCH TO FIND BEST CONFIGURATION OF NEURONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [1e-5]\n",
    "momentums = [0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "optimiz = 'RMS'\n",
    "\n",
    "#proportionss = [[0.1,0.8,0.1],[0.2,0.6,0.2],[0.25,0.50,0.25],[0.2,0.7,0.1],[0.1,0.7,0.2],[0.3,0.6,0.1]]\n",
    "#Best = [0.25, 0.5, 0.25] with training_mee = 0.2612 +- 0.0024 | val_mee = 0.75 +- 0.10\n",
    "\n",
    "#proportionss = [[0.8,0.1,0.1],[0.7,0.2,0.1],[0.6,0.3,0.1],[0.6,0.2,0.2],[0.8,0.15,0.05],[0.5,0.4,0.1],[0.5,0.25,0.25], [0.5,0.3,0.2]]\n",
    "#Best = [0.5, 0.25, 0.25] with training_mee = 0.231 +- 0.010 | val_mee = 0.73 +- 0.10\n",
    "\n",
    "#proportionss = [[0.1,0.1,0.8],[0.1,0.2,0.7],[0.1,0.3,0.6],[0.2,0.2,0.6],[0.05,0.15,0.8],[0.1,0.4,0.5],[0.25,0.25,0.5], [0.2,0.3,0.5]]\n",
    "#Best = [0.25, 0.25, 0.5] with training_mee = 0.323 +- 0.01523 | val_mee = 0.6973 +- 0.1045\n",
    "\n",
    "#proportionss = [[0.2,0.3,0.5],[0.25,0.25,0.5],[0.15,0.35,0.5],[0.15,0.3,0.55],[0.1,0.25,0.55],[0.05,0.40,0.55],[0.25,0.35,0.4]]\n",
    "#Best = [0.2, 0.3, 0.5], with training_mee = 0.323 +- 0.010 | val_mee = 0.696 +- 0.091\n",
    "\n",
    "proportionss = [[0.2,0.3,0.5]]\n",
    "best_hp = perform_grid_search_kfold(hidden_neurons,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    activations,\n",
    "                                    optimiz,\n",
    "                                    proportionss,\n",
    "                                    k_folds=3,\n",
    "                                    x=X_train,\n",
    "                                    y=y_train,\n",
    "                                    num_epochs=5000,\n",
    "                                    plot_curves=False,\n",
    "                                    N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final validation results are comparable with the ones obtained using layers of equal size!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cmepda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
