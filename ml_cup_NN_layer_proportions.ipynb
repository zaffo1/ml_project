{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)   # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)   # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)    # fontsize of the figure suptitle\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(train_losses, test_losses, train_mees, test_mees, hyperparams):\n",
    "    \"\"\"\n",
    "    Plot training and test curves for loss and Mean Euclidean Error (MEE).\n",
    "\n",
    "    Parameters:\n",
    "    - epoch (int): The total number of training epochs.\n",
    "    - train_losses (list): List of training losses for each epoch.\n",
    "    - test_losses (list): List of test losses for each epoch.\n",
    "    - train_mees (list): List of training MEE values for each epoch.\n",
    "    - test_mees (list): List of test MEE values for each epoch.\n",
    "    - hyperparams (list): List of hyperparameters used for the plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    Plots four subplots:\n",
    "    1. Training and test loss curves.\n",
    "    2. Training and test MEE curves.\n",
    "    3. Zoomed-in training and test loss curves with y-axis limit [0, 10].\n",
    "    4. Zoomed-in training and test MEE curves with y-axis limit [0, 10].\n",
    "\n",
    "    The hyperparameters are used in the plot title to provide additional context.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #plt.suptitle(f'Batch Size={hyperparams[3]},Activation Function={hyperparams[5]}, Layers={hyperparams[6]} Hidden Units={hyperparams[0]}, Eta={hyperparams[1]}, Alpha={hyperparams[2]}, Lambda={hyperparams[4]}, dropout = {hyperparams[7]}')\n",
    "    # Loss plots\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss', color = 'red')\n",
    "    plt.plot(test_losses, label='Validation Loss', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "\n",
    "    # MEE plots\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(train_mees, label='Training MEE', color='red')\n",
    "    plt.plot(test_mees, label='Validation MEE', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss plots\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(train_losses, label='Training Loss', color = 'red')\n",
    "    plt.plot(test_losses, label='Validation Loss', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "\n",
    "    # MEE plots\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(train_mees, label='Training MEE', color='red')\n",
    "    plt.plot(test_mees, label='Validation MEE', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_euclidean_error(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Compute the mean Euclidean error between two sets of 3D vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor1: PyTorch tensor of size (N, 3) representing the first set of 3D vectors\n",
    "    - tensor2: PyTorch tensor of size (N, 3) representing the second set of 3D vectors\n",
    "\n",
    "    Returns:\n",
    "    - mean_error: Mean Euclidean error between the two sets of vectors\n",
    "    \"\"\"\n",
    "    # Check if the tensors have the correct shape\n",
    "    if tensor1.shape[1] != 3 or tensor2.shape[1] != 3 or tensor1.shape[0] != tensor2.shape[0]:\n",
    "        raise ValueError(\"Input tensors must be of size (N, 3)\")\n",
    "\n",
    "\n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = torch.norm(tensor1 - tensor2, dim=1)\n",
    "\n",
    "    # Calculate the mean Euclidean error\n",
    "    mean_error = torch.mean(euclidean_distance)\n",
    "\n",
    "    return mean_error.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regression eural network\n",
    "\n",
    "class RegressorNN(nn.Module):\n",
    "    def __init__(self, hidden_sizes, activation_function, input_size=10, output_size=3):\n",
    "        super(RegressorNN, self).__init__()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers = [nn.Linear(input_size, hidden_sizes[0])]\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_sizes[0], hidden_sizes[1]))\n",
    "        self.layers.append(activation_function)\n",
    "        self.layers.append(nn.Linear(hidden_sizes[1], hidden_sizes[2]))\n",
    "        self.layers.append(activation_function)\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[2], output_size))\n",
    "\n",
    "        # Create a Sequential container for the layers\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def training_model(x_train, y_train, x_test, y_test, neuron_number,learning_rate, momentum, bs, reg_coeff, activation, optimiz, proportions, num_epochs=1000, plot_curves=False, return_history=False):\n",
    "    \"\"\"\n",
    "    Train the regression model and evaluate it on a test (can also be validation, depending on the context).\n",
    "\n",
    "    Parameters:\n",
    "    - x_train (torch.Tensor): Training input data.\n",
    "    - y_train (torch.Tensor): Training target data.\n",
    "    - x_test (torch.Tensor): Test input data.\n",
    "    - y_test (torch.Tensor): Test target data.\n",
    "    - neuron_number (int): Total number of neurons across all layers.\n",
    "    - learning_rate (float): Learning rate for the optimizer.\n",
    "    - momentum (float): Momentum for the optimizer.\n",
    "    - bs (int): Batch size for training.\n",
    "    - reg_coeff (float): Regularization coefficient for weight decay.\n",
    "    - activation (torch.nn.Module): Activation function for the model.\n",
    "    - layers (int): Number of hidden layers in the model.\n",
    "    - num_epochs (int, optional): Number of training epochs (default: 1000).\n",
    "    - plot_curves (bool, optional): Whether to plot training curves (default: False).\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Tuple containing the number of epochs, final training loss, final test loss, final training MEE, and final test MEE.\n",
    "\n",
    "    The function trains a neural network regression model using the specified hyperparameters and evaluates its performance on the test set.\n",
    "    \"\"\"\n",
    "    hidden_sizes = [0,0,0]\n",
    "\n",
    "    for i in range(len(proportions)):\n",
    "        hidden_sizes[i] = int(proportions[i]*neuron_number)\n",
    "        #print(hidden_sizes)\n",
    "    #print(hidden_size)\n",
    "    # Create an instance of the model\n",
    "    model = RegressorNN(hidden_sizes=hidden_sizes, activation_function=activation)\n",
    "    model.to(device)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    if optimiz == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=reg_coeff)\n",
    "    if optimiz == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg_coeff)\n",
    "    if optimiz == 'RMS':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate,momentum=momentum, weight_decay=reg_coeff)\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "    # Lists to store training and test losses for plotting\n",
    "    train_losses, test_losses, train_mees, test_mees = [], [], [], []\n",
    "\n",
    "    # parameters to stop at training convergence\n",
    "    min_mee = float('inf')\n",
    "    patience_counter, patience = 0, 20\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        for inputs, labels in train_dataloader:\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels) #Compute the loss\n",
    "\n",
    "            optimizer.zero_grad()   # Zero the gradients\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        #calculate loss\n",
    "        y_pred = model(x_train)\n",
    "        train_loss = criterion(y_pred, y_train)\n",
    "        # Calculate mee\n",
    "        train_mee = mean_euclidean_error(y_pred,y_train)\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_mees.append(train_mee)\n",
    "\n",
    "        # Evaluation on the test/validation set\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(x_test)\n",
    "            test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "            # Calculate test mee\n",
    "            test_mee = mean_euclidean_error(test_outputs,y_test)\n",
    "            test_mees.append(test_mee)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "        print(f'Training - Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss.item():.4f}, '\n",
    "            f'MEE: {train_mee:.4f} | Test - Epoch [{epoch+1}/{num_epochs}], '\n",
    "            f'Loss: {test_loss.item():.4f} MEE: {test_mee:.4f} ', end='\\r')\n",
    "\n",
    "        # Check for convergence\n",
    "        if train_mee < min_mee and abs(train_mee-min_mee)>1e-3:\n",
    "            patience_counter = 0\n",
    "            min_mee = train_mee\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter == patience:\n",
    "        #    print(f\"Convergence reached. at epoch {epoch+1} Stopping training.\")\n",
    "            break\n",
    "\n",
    "        # Update the learning rate using the scheduler\n",
    "        scheduler.step(train_mee)\n",
    "\n",
    "    print(f'\\n N. Epochs = {epoch+1} - Loss (train | test)= ({train_loss.item():.4} | {test_loss.item():.4}) - MEE (train | test) = ({train_mee} | {test_mee})')\n",
    "\n",
    "    if plot_curves:\n",
    "        hyperparams = [hidden_sizes,learning_rate, momentum, bs, reg_coeff, activation, proportions, num_epochs]\n",
    "        plot_training_curves(train_losses, test_losses, train_mees, test_mees, hyperparams)\n",
    "\n",
    "    if return_history:\n",
    "        return model, epoch+1, train_loss.item(), test_loss.item(), train_mee, test_mee, np.array(train_losses), np.array(test_losses), np.array(train_mees), np.array(test_mees)\n",
    "    else:\n",
    "        return model, epoch+1, train_loss.item(), test_loss.item(), train_mee, test_mee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_history(N,hist):\n",
    "    min_len = float('inf')\n",
    "    for i in range(0,N):\n",
    "        #print(len(train_losses_hist[i]))\n",
    "        if len(hist[i]) < min_len:\n",
    "            min_len = len(hist[i])\n",
    "\n",
    "    for i in range(0,N):\n",
    "        hist[i]=hist[i][:min_len]\n",
    "\n",
    "    return hist\n",
    "def train_with_different_initializations(x_train, y_train, x_test, y_test,\n",
    "                                         neuron_number,learning_rate, momentum,\n",
    "                                         bs, reg_coeff, activation,optimiz,proportions, max_num_epochs=1000,\n",
    "                                         plot_curves=False, N = 5, return_history=False):\n",
    "    \"\"\"\n",
    "    Train the model multiple times with different weight initializations to estimate performance mean and variance.\n",
    "\n",
    "    Parameters:\n",
    "    - x_train (torch.Tensor): Training input data.\n",
    "    - y_train (torch.Tensor): Training target data.\n",
    "    - x_test (torch.Tensor): Test input data.\n",
    "    - y_test (torch.Tensor): Test target data.\n",
    "    - neuron_number (int): Total number of neurons across all layers.\n",
    "    - learning_rate (float): Learning rate for the optimizer.\n",
    "    - momentum (float): Momentum for the optimizer.\n",
    "    - bs (int): Batch size for training.\n",
    "    - reg_coeff (float): Regularization coefficient for weight decay.\n",
    "    - activation (torch.nn.Module): Activation function for the model.\n",
    "    - layers (int): Number of hidden layers in the model.\n",
    "    - num_epochs (int, optional): Number of training epochs (default: 1000).\n",
    "    - plot_curves (bool, optional): Whether to plot training curves (default: False).\n",
    "    - N (int, optional): Number of times to train the model with different initializations (default: 5).\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    Prints the mean and standard deviation of training and test loss, as well as training and test Mean Euclidean Error (MEE).\n",
    "    \"\"\"\n",
    "    final_train_loss = []\n",
    "    final_test_loss = []\n",
    "    final_train_mee = []\n",
    "    final_test_mee = []\n",
    "    train_losses_hist, test_losses_hist, train_mees_hist, test_mees_hist = [[] for _ in range(N)],  [[] for _ in range(N)],  [[] for _ in range(N)],  [[] for _ in range(N)]\n",
    "    models = []\n",
    "    for i in range (0,N):\n",
    "        model , num_epochs,train_loss, test_loss, train_mee, test_mee, train_losses_hist[i], test_losses_hist[i], train_mees_hist[i], test_mees_hist[i] = training_model(x_train,\n",
    "                        y_train,\n",
    "                        x_test,\n",
    "                        y_test,\n",
    "                        neuron_number,\n",
    "                        learning_rate,\n",
    "                        momentum,\n",
    "                        bs,\n",
    "                        reg_coeff,\n",
    "                        activation,\n",
    "                        optimiz,\n",
    "                        proportions,\n",
    "                        plot_curves=plot_curves,\n",
    "                        num_epochs=max_num_epochs,\n",
    "                        return_history=True)\n",
    "\n",
    "        final_train_loss.append(train_loss)\n",
    "        final_test_loss.append(test_loss)\n",
    "        final_train_mee.append(train_mee)\n",
    "        final_test_mee.append(test_mee)\n",
    "        models.append(model)\n",
    "\n",
    "    if plot_curves:\n",
    "        hyperparams = [learning_rate, momentum, bs, reg_coeff, activation, num_epochs]\n",
    "        plot_training_curves(train_losses=train_losses_hist[i],test_losses=test_losses_hist[i],train_mees=train_mees_hist[i],test_mees=test_mees_hist[i],hyperparams=hyperparams)\n",
    "\n",
    "    plt.show()\n",
    "    print(f'Avg of {N} initializations: Loss (train | test/val )= ({np.mean(final_train_loss):.4} +- {np.std(final_train_loss):.4} | {np.mean(final_test_loss):.4} +- {np.std(final_test_loss):.4})'\n",
    "          f'- MEE (train | test/val ) = ( {np.mean(final_train_mee):.4} +-  {np.std(final_train_mee):.4} | {np.mean(final_test_mee):.4} +- {np.std(final_test_mee):.4})')\n",
    "\n",
    "\n",
    "    if return_history:\n",
    "        train_losses_hist = reshape_history(N,train_losses_hist)\n",
    "        test_losses_hist = reshape_history(N,test_losses_hist)\n",
    "        train_mees_hist = reshape_history(N,train_mees_hist)\n",
    "        test_mees_hist = reshape_history(N,test_mees_hist)\n",
    "        return models, train_losses_hist, test_losses_hist, train_mees_hist, test_mees_hist\n",
    "    else:\n",
    "        return np.mean(final_train_loss), np.mean(final_test_loss), np.mean(final_train_mee), np.mean(final_test_mee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search_kfold(neuron_numbers, learning_rates, momentums, batch_sizes, reg_coeffs, activations,optimiz,proportionss, k_folds, x, y,N=3, plot_curves=False, num_epochs=1000):\n",
    "    \"\"\"\n",
    "    Perform grid search with k-fold cross-validation for hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - neuron_numbers (list): List of neuron numbers to search.\n",
    "    - learning_rates (list): List of learning rates to search.\n",
    "    - momentums (list): List of momentum values to search.\n",
    "    - batch_sizes (list): List of batch sizes to search.\n",
    "    - reg_coeffs (list): List of regularization coefficients to search.\n",
    "    - activations (list): List of activation functions to search.\n",
    "    - layerss (list): List of numbers of hidden layers to search.\n",
    "    - k_folds (int): Number of folds for cross-validation.\n",
    "    - x (numpy.ndarray): Input data.\n",
    "    - y (numpy.ndarray): Target data.\n",
    "    - plot_curves (bool, optional): Whether to plot training curves (default: False).\n",
    "    - num_epochs (int, optional): Number of training epochs (default: 1000).\n",
    "\n",
    "    Returns:\n",
    "    - list: List of best hyperparameters.\n",
    "\n",
    "    The function performs grid search with k-fold cross-validation for Monk classifier hyperparameters and returns the best hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    best_mee = float('inf')\n",
    "    best_hyperparams = []\n",
    "\n",
    "    for neuron_number, learning_rate, momentum, bs, reg_coeff, activation, proportions in product(neuron_numbers,learning_rates,momentums,batch_sizes, reg_coeffs, activations, proportionss):\n",
    "        print(f'activation={activation};; neuron_number={neuron_number}; lr={learning_rate}; alpha = {momentum}; batch size = {bs}; lambda = {reg_coeff}; optim = {optimiz}; proportions = {proportions}')\n",
    "\n",
    "        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        # Lists to store training and validation losses and MEEs for each epoch\n",
    "        train_losses, val_losses, train_mees, val_mees = [], [], [], []\n",
    "\n",
    "        # Perform K-fold cross-validation\n",
    "        for fold, (train_indices, val_indices) in enumerate(kf.split(x,y)):\n",
    "            #print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "\n",
    "            # Split the data into training and validation (or test) sets\n",
    "            X_train, X_val = x[train_indices], x[val_indices]\n",
    "            Y_train, Y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "            train_loss, val_loss, train_mee, val_mee = train_with_different_initializations(X_train, Y_train, X_val, Y_val,\n",
    "                neuron_number=neuron_number, learning_rate=learning_rate, momentum=momentum,\n",
    "                bs=bs, reg_coeff=reg_coeff, activation=activation, proportions=proportions, optimiz=optimiz ,plot_curves=plot_curves, max_num_epochs=num_epochs, N=N)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_mees.append(train_mee)\n",
    "            val_mees.append(val_mee)\n",
    "\n",
    "        print(f'Final Results: activation={activation}; neuron number={neuron_number}; lr={learning_rate}; alpha = {momentum}; batch size = {bs}; lambda = {reg_coeff}; proportions = {proportions} --> '\n",
    "            f'train_loss = {np.mean(train_losses):.4} +- {np.std(train_losses):.4} | '\n",
    "            f'val_loss = {np.mean(val_losses):.4} +- {np.std(val_losses):.4}'\n",
    "            f'train_mee = {np.mean(train_mees):.4} +- {np.std(train_mees):.4} | '\n",
    "            f'val_mee = {np.mean(val_mees):.4} +- {np.std(val_mees):.4}')\n",
    "\n",
    "        if np.mean(val_mees) < best_mee:\n",
    "            best_mee = np.mean(val_mees)\n",
    "            best_hyperparams = [neuron_number, learning_rate, momentum, bs, reg_coeff, activation, proportions]\n",
    "\n",
    "    print(best_hyperparams)\n",
    "    return best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_std(N,train_hist, test_hist,label):\n",
    "    mean_tr = np.array(train_hist).mean(axis=0)\n",
    "    std_tr = np.array(train_hist).std(axis=0)\n",
    "    mean_te = np.array(test_hist).mean(axis=0)\n",
    "    std_te = np.array(test_hist).std(axis=0)\n",
    "\n",
    "    plt.figure(figsize=(9, 8))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(mean_tr, label=f'Training {label} (mean $\\pm$ std)', color = 'red', linewidth=1)\n",
    "    plt.fill_between(range(0,len(train_hist[0])),mean_tr-std_tr, mean_tr+std_tr, color='crimson', alpha=0.3)\n",
    "\n",
    "    plt.plot(mean_te, label=f'Test {label} (mean $\\pm$ std)', color = 'blue', linestyle='--', linewidth=1)\n",
    "    plt.fill_between(range(0,len(test_hist[0])),mean_te-std_te, mean_te+std_te, color='blue', alpha=0.3)\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(label)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "\n",
    "    plt.plot(mean_tr, label=f'Training {label} (mean $\\pm$ std)', color = 'red', linewidth=1)\n",
    "    plt.fill_between(range(0,len(train_hist[0])),mean_tr-std_tr, mean_tr+std_tr, color='crimson', alpha=0.3)\n",
    "\n",
    "    plt.plot(mean_te, label=f'Test {label} (mean $\\pm$ std)', color = 'blue', linestyle='--', linewidth=1)\n",
    "    plt.fill_between(range(0,len(test_hist[0])),mean_te-std_te, mean_te+std_te, color='blue', alpha=0.3)\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(label)\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN THE MODEL USING DIFFERENT INITIALIZATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('ML-CUP23-TR.csv', delimiter=',')\n",
    "X = dataset[:,1:11]\n",
    "y = dataset[:,11:14]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# Split the data into training and testing sets (80%/20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERFORM GRID SEARCH TO FIND BEST HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.1, 0.8, 0.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch [2065/5000], Loss: 1.1543, MEE: 0.8375 | Test - Epoch [2065/5000], Loss: 0.7918 MEE: 1.1040     \n",
      " N. Epochs = 2065 - Loss (train | test)= (1.154 | 0.7918) - MEE (train | test) = (0.8374984264373779 | 1.1039910316467285)\n",
      "Epoch 02325: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2325/5000], Loss: 0.4866 MEE: 0.8448     \n",
      "Epoch 02501: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2501/5000], Loss: 0.4630 MEE: 0.8284 \n",
      "Training - Epoch [2613/5000], Loss: 0.0699, MEE: 0.3328 | Test - Epoch [2613/5000], Loss: 0.4461 MEE: 0.7970 \n",
      " N. Epochs = 2613 - Loss (train | test)= (0.06993 | 0.4461) - MEE (train | test) = (0.33284276723861694 | 0.7970279455184937)\n",
      "Epoch 02391: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2391/5000], Loss: 0.5683 MEE: 0.8543     \n",
      "Epoch 02435: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2435/5000], Loss: 0.5463 MEE: 0.8370 \n",
      "Epoch 02471: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [2471/5000], Loss: 0.5454 MEE: 0.8347 \n",
      "Epoch 02495: reducing learning rate of group 0 to 6.2500e-07.t - Epoch [2495/5000], Loss: 0.5454 MEE: 0.8365 \n",
      "Training - Epoch [2499/5000], Loss: 0.0906, MEE: 0.3356 | Test - Epoch [2499/5000], Loss: 0.5423 MEE: 0.8317 \n",
      " N. Epochs = 2499 - Loss (train | test)= (0.09056 | 0.5423) - MEE (train | test) = (0.33560746908187866 | 0.8317264318466187)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.4383 +- 0.5064 | 0.5934 +- 0.1457)- MEE (train | test/val ) = ( 0.502 +-  0.2372 | 0.9109 +- 0.1373)\n",
      "Epoch 02411: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2411/5000], Loss: 1.0670 MEE: 1.0972     \n",
      "Epoch 02517: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2517/5000], Loss: 1.0077 MEE: 1.0590 \n",
      "Training - Epoch [2549/5000], Loss: 0.1991, MEE: 0.3812 | Test - Epoch [2549/5000], Loss: 0.9949 MEE: 1.0458 \n",
      " N. Epochs = 2549 - Loss (train | test)= (0.1991 | 0.9949) - MEE (train | test) = (0.38117873668670654 | 1.0458065271377563)\n",
      "Epoch 02171: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2171/5000], Loss: 1.2451 MEE: 1.1577   7 \n",
      "Training - Epoch [2245/5000], Loss: 0.2093, MEE: 0.4014 | Test - Epoch [2245/5000], Loss: 1.2059 MEE: 1.1251 \n",
      " N. Epochs = 2245 - Loss (train | test)= (0.2093 | 1.206) - MEE (train | test) = (0.4013994038105011 | 1.1251474618911743)\n",
      "Epoch 02083: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2083/5000], Loss: 1.4816 MEE: 1.1964     \n",
      "Training - Epoch [2217/5000], Loss: 0.1547, MEE: 0.3943 | Test - Epoch [2217/5000], Loss: 1.4045 MEE: 1.1563 \n",
      " N. Epochs = 2217 - Loss (train | test)= (0.1547 | 1.404) - MEE (train | test) = (0.3942592144012451 | 1.156257152557373)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1877 +- 0.02368 | 1.202 +- 0.1672)- MEE (train | test/val ) = ( 0.3923 +-  0.008373 | 1.109 +- 0.0465)\n",
      "Training - Epoch [2148/5000], Loss: 0.2446, MEE: 0.4277 | Test - Epoch [2148/5000], Loss: 0.9694 MEE: 1.1681     \n",
      " N. Epochs = 2148 - Loss (train | test)= (0.2446 | 0.9694) - MEE (train | test) = (0.4276571273803711 | 1.168074131011963)\n",
      "Training - Epoch [2060/5000], Loss: 0.2920, MEE: 0.4472 | Test - Epoch [2060/5000], Loss: 1.0824 MEE: 1.2052     \n",
      " N. Epochs = 2060 - Loss (train | test)= (0.292 | 1.082) - MEE (train | test) = (0.44719114899635315 | 1.2052079439163208)\n",
      "Epoch 02282: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2282/5000], Loss: 0.8509 MEE: 1.1190     \n",
      "Training - Epoch [2312/5000], Loss: 0.1927, MEE: 0.3800 | Test - Epoch [2312/5000], Loss: 0.8406 MEE: 1.1115 \n",
      " N. Epochs = 2312 - Loss (train | test)= (0.1927 | 0.8406) - MEE (train | test) = (0.37996217608451843 | 1.1114519834518433)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.2431 +- 0.04057 | 0.9642 +- 0.0988)- MEE (train | test/val ) = ( 0.4183 +-  0.02824 | 1.162 +- 0.03855)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.1, 0.8, 0.1] --> train_loss = 0.2897 +- 0.1075 | val_loss = 0.9198 +- 0.2503train_mee = 0.4375 +- 0.04681 | val_mee = 1.061 +- 0.1079\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.2, 0.6, 0.2]\n",
      "Epoch 01296: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1296/5000], Loss: 0.3276 MEE: 0.6643     \n",
      "Epoch 01473: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1473/5000], Loss: 0.3116 MEE: 0.6264 \n",
      "Training - Epoch [1518/5000], Loss: 0.0404, MEE: 0.2396 | Test - Epoch [1518/5000], Loss: 0.3119 MEE: 0.6291 \n",
      " N. Epochs = 1518 - Loss (train | test)= (0.04041 | 0.3119) - MEE (train | test) = (0.23962998390197754 | 0.6291258335113525)\n",
      "Epoch 01357: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1357/5000], Loss: 0.3899 MEE: 0.7131     \n",
      "Epoch 01424: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1424/5000], Loss: 0.3738 MEE: 0.6882 \n",
      "Training - Epoch [1560/5000], Loss: 0.1671, MEE: 0.3199 | Test - Epoch [1560/5000], Loss: 0.3620 MEE: 0.6712 \n",
      " N. Epochs = 1560 - Loss (train | test)= (0.1671 | 0.362) - MEE (train | test) = (0.31989413499832153 | 0.6711518168449402)\n",
      "Epoch 01310: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1310/5000], Loss: 0.4353 MEE: 0.7046     \n",
      "Epoch 01339: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1339/5000], Loss: 0.4311 MEE: 0.6902 \n",
      "Training - Epoch [1493/5000], Loss: 0.0360, MEE: 0.2383 | Test - Epoch [1493/5000], Loss: 0.4121 MEE: 0.6750 \n",
      " N. Epochs = 1493 - Loss (train | test)= (0.03599 | 0.4121) - MEE (train | test) = (0.23825259506702423 | 0.6749989986419678)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.08118 +- 0.06081 | 0.362 +- 0.04093)- MEE (train | test/val ) = ( 0.2659 +-  0.03817 | 0.6584 +- 0.02078)\n",
      "Training - Epoch [1253/5000], Loss: 0.1001, MEE: 0.3174 | Test - Epoch [1253/5000], Loss: 0.6960 MEE: 0.9036   5 \n",
      " N. Epochs = 1253 - Loss (train | test)= (0.1001 | 0.696) - MEE (train | test) = (0.3174000382423401 | 0.9035542607307434)\n",
      "Epoch 01267: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1267/5000], Loss: 0.6530 MEE: 0.8709     \n",
      "Epoch 01300: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1300/5000], Loss: 0.6381 MEE: 0.8557 \n",
      "Training - Epoch [1354/5000], Loss: 0.0490, MEE: 0.2537 | Test - Epoch [1354/5000], Loss: 0.6250 MEE: 0.8456 \n",
      " N. Epochs = 1354 - Loss (train | test)= (0.049 | 0.625) - MEE (train | test) = (0.25374317169189453 | 0.845553457736969)\n",
      "Epoch 01328: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1328/5000], Loss: 0.5792 MEE: 0.8202   0 \n",
      "Training - Epoch [1356/5000], Loss: 0.0569, MEE: 0.2491 | Test - Epoch [1356/5000], Loss: 0.5757 MEE: 0.8102 \n",
      " N. Epochs = 1356 - Loss (train | test)= (0.05688 | 0.5757) - MEE (train | test) = (0.24907012283802032 | 0.8102381825447083)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.06864 +- 0.02244 | 0.6323 +- 0.04938)- MEE (train | test/val ) = ( 0.2734 +-  0.03117 | 0.8531 +- 0.03847)\n",
      "Epoch 01263: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1263/5000], Loss: 0.6419 MEE: 0.9356     \n",
      "Epoch 01387: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1387/5000], Loss: 0.5916 MEE: 0.8885 \n",
      "Training - Epoch [1428/5000], Loss: 0.0607, MEE: 0.2531 | Test - Epoch [1428/5000], Loss: 0.5812 MEE: 0.8795 \n",
      " N. Epochs = 1428 - Loss (train | test)= (0.06067 | 0.5812) - MEE (train | test) = (0.2531284689903259 | 0.8795467615127563)\n",
      "Training - Epoch [1318/5000], Loss: 0.0755, MEE: 0.2650 | Test - Epoch [1318/5000], Loss: 0.5994 MEE: 0.9084     \n",
      " N. Epochs = 1318 - Loss (train | test)= (0.07554 | 0.5994) - MEE (train | test) = (0.26496532559394836 | 0.9083538055419922)\n",
      "Epoch 01175: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1175/5000], Loss: 0.6850 MEE: 0.9460     \n",
      "Epoch 01285: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1285/5000], Loss: 0.5994 MEE: 0.9124 \n",
      "Training - Epoch [1286/5000], Loss: 0.1073, MEE: 0.2854 | Test - Epoch [1286/5000], Loss: 0.6009 MEE: 0.9139 \n",
      " N. Epochs = 1286 - Loss (train | test)= (0.1073 | 0.6009) - MEE (train | test) = (0.2853540778160095 | 0.913905680179596)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.08117 +- 0.01945 | 0.5938 +- 0.008955)- MEE (train | test/val ) = ( 0.2678 +-  0.01331 | 0.9006 +- 0.01506)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.2, 0.6, 0.2] --> train_loss = 0.077 +- 0.005908 | val_loss = 0.5294 +- 0.1194train_mee = 0.269 +- 0.003175 | val_mee = 0.804 +- 0.1048\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.25, 0.5, 0.25]\n",
      "Epoch 01149: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1149/5000], Loss: 0.2935 MEE: 0.6378     \n",
      "Epoch 01202: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1202/5000], Loss: 0.2882 MEE: 0.6220 \n",
      "Epoch 01264: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1264/5000], Loss: 0.2848 MEE: 0.6170 \n",
      "Training - Epoch [1293/5000], Loss: 0.0449, MEE: 0.2450 | Test - Epoch [1293/5000], Loss: 0.2839 MEE: 0.6121 \n",
      " N. Epochs = 1293 - Loss (train | test)= (0.04487 | 0.2839) - MEE (train | test) = (0.24495546519756317 | 0.6121137142181396)\n",
      "Epoch 01209: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1209/5000], Loss: 0.3025 MEE: 0.6259     \n",
      "Epoch 01347: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1347/5000], Loss: 0.2889 MEE: 0.5959 \n",
      "Epoch 01403: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1403/5000], Loss: 0.2850 MEE: 0.5818 \n",
      "Training - Epoch [1407/5000], Loss: 0.0224, MEE: 0.2063 | Test - Epoch [1407/5000], Loss: 0.2863 MEE: 0.5872 \n",
      " N. Epochs = 1407 - Loss (train | test)= (0.02237 | 0.2863) - MEE (train | test) = (0.20626333355903625 | 0.587186336517334)\n",
      "Epoch 01124: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1124/5000], Loss: 0.3315 MEE: 0.6711     \n",
      "Epoch 01255: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1255/5000], Loss: 0.3011 MEE: 0.6382 \n",
      "Training - Epoch [1347/5000], Loss: 0.0329, MEE: 0.2354 | Test - Epoch [1347/5000], Loss: 0.2934 MEE: 0.6154 \n",
      " N. Epochs = 1347 - Loss (train | test)= (0.03285 | 0.2934) - MEE (train | test) = (0.2353736162185669 | 0.6153566241264343)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.03337 +- 0.009191 | 0.2879 +- 0.004017)- MEE (train | test/val ) = ( 0.2289 +-  0.01645 | 0.6049 +- 0.01259)\n",
      "Training - Epoch [1072/5000], Loss: 0.0581, MEE: 0.3110 | Test - Epoch [1072/5000], Loss: 0.7010 MEE: 0.8958   0 \n",
      " N. Epochs = 1072 - Loss (train | test)= (0.05814 | 0.701) - MEE (train | test) = (0.31101348996162415 | 0.8958422541618347)\n",
      "Epoch 01074: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1074/5000], Loss: 0.5573 MEE: 0.8135     \n",
      "Epoch 01238: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1238/5000], Loss: 0.4823 MEE: 0.7513 \n",
      "Training - Epoch [1302/5000], Loss: 0.0422, MEE: 0.2320 | Test - Epoch [1302/5000], Loss: 0.4759 MEE: 0.7352 \n",
      " N. Epochs = 1302 - Loss (train | test)= (0.04215 | 0.4759) - MEE (train | test) = (0.23200736939907074 | 0.7351853847503662)\n",
      "Epoch 01189: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1189/5000], Loss: 0.6028 MEE: 0.8259     \n",
      "Training - Epoch [1225/5000], Loss: 0.0474, MEE: 0.2539 | Test - Epoch [1225/5000], Loss: 0.5933 MEE: 0.8160 \n",
      " N. Epochs = 1225 - Loss (train | test)= (0.04739 | 0.5933) - MEE (train | test) = (0.25385236740112305 | 0.8160011768341064)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.04923 +- 0.006653 | 0.5901 +- 0.09193)- MEE (train | test/val ) = ( 0.2656 +-  0.03331 | 0.8157 +- 0.06559)\n",
      "Epoch 01035: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1035/5000], Loss: 0.5416 MEE: 0.8873     \n",
      "Epoch 01196: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1196/5000], Loss: 0.4825 MEE: 0.8293 \n",
      "Epoch 01251: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1251/5000], Loss: 0.4707 MEE: 0.8183 \n",
      "Training - Epoch [1255/5000], Loss: 0.0536, MEE: 0.2375 | Test - Epoch [1255/5000], Loss: 0.4697 MEE: 0.8199 \n",
      " N. Epochs = 1255 - Loss (train | test)= (0.05358 | 0.4697) - MEE (train | test) = (0.2374541312456131 | 0.819889485836029)\n",
      "Epoch 01105: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1105/5000], Loss: 0.5532 MEE: 0.8707     \n",
      "Epoch 01232: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1232/5000], Loss: 0.5052 MEE: 0.8380 \n",
      "Training - Epoch [1236/5000], Loss: 0.0797, MEE: 0.2514 | Test - Epoch [1236/5000], Loss: 0.4990 MEE: 0.8343 \n",
      " N. Epochs = 1236 - Loss (train | test)= (0.07973 | 0.499) - MEE (train | test) = (0.2514229118824005 | 0.8342558741569519)\n",
      "Epoch 01191: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1191/5000], Loss: 0.5536 MEE: 0.8605     \n",
      "Training - Epoch [1241/5000], Loss: 0.0763, MEE: 0.2493 | Test - Epoch [1241/5000], Loss: 0.5400 MEE: 0.8482 \n",
      " N. Epochs = 1241 - Loss (train | test)= (0.07625 | 0.54) - MEE (train | test) = (0.24931325018405914 | 0.8482374548912048)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.06986 +- 0.01159 | 0.5029 +- 0.02883)- MEE (train | test/val ) = ( 0.2461 +-  0.006148 | 0.8341 +- 0.01157)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.25, 0.5, 0.25] --> train_loss = 0.05082 +- 0.01494 | val_loss = 0.4603 +- 0.127train_mee = 0.2469 +- 0.01502 | val_mee = 0.7516 +- 0.104\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.2, 0.7, 0.1]\n",
      "Epoch 02163: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2163/5000], Loss: 0.4421 MEE: 0.7817     \n",
      "Epoch 02311: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2311/5000], Loss: 0.3735 MEE: 0.7183 \n",
      "Epoch 02343: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [2343/5000], Loss: 0.3662 MEE: 0.7145 \n",
      "Training - Epoch [2413/5000], Loss: 0.0793, MEE: 0.3196 | Test - Epoch [2413/5000], Loss: 0.3587 MEE: 0.7041 \n",
      " N. Epochs = 2413 - Loss (train | test)= (0.07931 | 0.3587) - MEE (train | test) = (0.31955501437187195 | 0.7040798664093018)\n",
      "Epoch 02028: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2028/5000], Loss: 0.4162 MEE: 0.7443     \n",
      "Training - Epoch [2067/5000], Loss: 0.0926, MEE: 0.3456 | Test - Epoch [2067/5000], Loss: 0.4051 MEE: 0.7277 \n",
      " N. Epochs = 2067 - Loss (train | test)= (0.09262 | 0.4051) - MEE (train | test) = (0.3456338047981262 | 0.7277489304542542)\n",
      "Epoch 02006: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2006/5000], Loss: 0.4542 MEE: 0.7832     \n",
      "Epoch 02231: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2231/5000], Loss: 0.3767 MEE: 0.7162 \n",
      "Training - Epoch [2265/5000], Loss: 0.0612, MEE: 0.2972 | Test - Epoch [2265/5000], Loss: 0.3830 MEE: 0.7132 \n",
      " N. Epochs = 2265 - Loss (train | test)= (0.06125 | 0.383) - MEE (train | test) = (0.29723262786865234 | 0.713158905506134)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.07773 +- 0.01286 | 0.3823 +- 0.01896)- MEE (train | test/val ) = ( 0.3208 +-  0.01978 | 0.715 +- 0.00975)\n",
      "Epoch 02223: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2223/5000], Loss: 0.9004 MEE: 1.0167     \n",
      "Training - Epoch [2258/5000], Loss: 0.0964, MEE: 0.3040 | Test - Epoch [2258/5000], Loss: 0.9056 MEE: 1.0130 \n",
      " N. Epochs = 2258 - Loss (train | test)= (0.09644 | 0.9056) - MEE (train | test) = (0.3040255308151245 | 1.012980580329895)\n",
      "Epoch 01945: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1945/5000], Loss: 1.9719 MEE: 1.3029     \n",
      "Epoch 02385: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2385/5000], Loss: 0.9880 MEE: 1.0210 \n",
      "Training - Epoch [2445/5000], Loss: 0.1141, MEE: 0.3078 | Test - Epoch [2445/5000], Loss: 0.9653 MEE: 1.0092 \n",
      " N. Epochs = 2445 - Loss (train | test)= (0.1141 | 0.9653) - MEE (train | test) = (0.3078252375125885 | 1.0091602802276611)\n",
      "Epoch 01910: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1910/5000], Loss: 0.9509 MEE: 1.0663     \n",
      "Epoch 02123: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2123/5000], Loss: 0.7959 MEE: 0.9763 \n",
      "Epoch 02225: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [2225/5000], Loss: 0.7780 MEE: 0.9547 \n",
      "Epoch 02250: reducing learning rate of group 0 to 6.2500e-07.t - Epoch [2250/5000], Loss: 0.7804 MEE: 0.9555 \n",
      "Training - Epoch [2254/5000], Loss: 0.0824, MEE: 0.3053 | Test - Epoch [2254/5000], Loss: 0.7815 MEE: 0.9538 \n",
      " N. Epochs = 2254 - Loss (train | test)= (0.0824 | 0.7815) - MEE (train | test) = (0.305346816778183 | 0.9538429975509644)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.09765 +- 0.01298 | 0.8841 +- 0.07654)- MEE (train | test/val ) = ( 0.3057 +-  0.001575 | 0.992 +- 0.02702)\n",
      "Epoch 02101: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2101/5000], Loss: 0.7573 MEE: 1.0234     \n",
      "Epoch 02134: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2134/5000], Loss: 0.7435 MEE: 1.0122 \n",
      "Training - Epoch [2208/5000], Loss: 0.1721, MEE: 0.3123 | Test - Epoch [2208/5000], Loss: 0.7299 MEE: 0.9981 \n",
      " N. Epochs = 2208 - Loss (train | test)= (0.1721 | 0.7299) - MEE (train | test) = (0.31230151653289795 | 0.9980803728103638)\n",
      "Epoch 01945: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1945/5000], Loss: 0.9573 MEE: 1.1334     \n",
      "Epoch 02278: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2278/5000], Loss: 0.6745 MEE: 0.9894 \n",
      "Training - Epoch [2314/5000], Loss: 0.0689, MEE: 0.2787 | Test - Epoch [2314/5000], Loss: 0.6654 MEE: 0.9803 \n",
      " N. Epochs = 2314 - Loss (train | test)= (0.06892 | 0.6654) - MEE (train | test) = (0.2786717712879181 | 0.9802842736244202)\n",
      "Epoch 01985: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1985/5000], Loss: 0.7954 MEE: 1.0612     \n",
      "Epoch 02011: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2011/5000], Loss: 0.7904 MEE: 1.0540 \n",
      "Training - Epoch [2078/5000], Loss: 0.1827, MEE: 0.3342 | Test - Epoch [2078/5000], Loss: 0.7716 MEE: 1.0309 \n",
      " N. Epochs = 2078 - Loss (train | test)= (0.1827 | 0.7716) - MEE (train | test) = (0.33422785997390747 | 1.0309025049209595)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1412 +- 0.05131 | 0.7223 +- 0.04369)- MEE (train | test/val ) = ( 0.3084 +-  0.02285 | 1.003 +- 0.02097)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.2, 0.7, 0.1] --> train_loss = 0.1055 +- 0.02652 | val_loss = 0.6629 +- 0.2092train_mee = 0.3116 +- 0.006568 | val_mee = 0.9034 +- 0.1333\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.1, 0.7, 0.2]\n",
      "Epoch 01496: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1496/5000], Loss: 0.3654 MEE: 0.7542     \n",
      "Epoch 01613: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1613/5000], Loss: 0.3624 MEE: 0.7178 \n",
      "Training - Epoch [1705/5000], Loss: 0.0749, MEE: 0.2830 | Test - Epoch [1705/5000], Loss: 0.3585 MEE: 0.7096 \n",
      " N. Epochs = 1705 - Loss (train | test)= (0.07494 | 0.3585) - MEE (train | test) = (0.2829596698284149 | 0.7096053957939148)\n",
      "Epoch 01518: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1518/5000], Loss: 0.3282 MEE: 0.6871     \n",
      "Training - Epoch [1624/5000], Loss: 0.0512, MEE: 0.2680 | Test - Epoch [1624/5000], Loss: 0.3238 MEE: 0.6757 \n",
      " N. Epochs = 1624 - Loss (train | test)= (0.05118 | 0.3238) - MEE (train | test) = (0.2679867744445801 | 0.6757028698921204)\n",
      "Epoch 01403: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1403/5000], Loss: 0.4409 MEE: 0.7743     \n",
      "Training - Epoch [1434/5000], Loss: 0.1338, MEE: 0.3322 | Test - Epoch [1434/5000], Loss: 0.4288 MEE: 0.7593 \n",
      " N. Epochs = 1434 - Loss (train | test)= (0.1338 | 0.4288) - MEE (train | test) = (0.3321985602378845 | 0.7593188285827637)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.08665 +- 0.03474 | 0.3704 +- 0.04367)- MEE (train | test/val ) = ( 0.2944 +-  0.02743 | 0.7149 +- 0.03434)\n",
      "Epoch 01426: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1426/5000], Loss: 0.6289 MEE: 0.8700     \n",
      "Epoch 01493: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1493/5000], Loss: 0.6176 MEE: 0.8662 \n",
      "Training - Epoch [1567/5000], Loss: 0.0539, MEE: 0.2618 | Test - Epoch [1567/5000], Loss: 0.6015 MEE: 0.8532 \n",
      " N. Epochs = 1567 - Loss (train | test)= (0.05394 | 0.6015) - MEE (train | test) = (0.2618422508239746 | 0.8531850576400757)\n",
      "Epoch 01515: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1515/5000], Loss: 0.7159 MEE: 0.8725     \n",
      "Training - Epoch [1545/5000], Loss: 0.0951, MEE: 0.3017 | Test - Epoch [1545/5000], Loss: 0.7146 MEE: 0.8788 \n",
      " N. Epochs = 1545 - Loss (train | test)= (0.09508 | 0.7146) - MEE (train | test) = (0.30173125863075256 | 0.8787547945976257)\n",
      "Epoch 01515: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1515/5000], Loss: 0.7580 MEE: 0.9176     \n",
      "Epoch 01549: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1549/5000], Loss: 0.7499 MEE: 0.9130 \n",
      "Epoch 01579: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1579/5000], Loss: 0.7371 MEE: 0.9013 \n",
      "Epoch 01611: reducing learning rate of group 0 to 6.2500e-07.t - Epoch [1611/5000], Loss: 0.7358 MEE: 0.8985 \n",
      "Training - Epoch [1612/5000], Loss: 0.0754, MEE: 0.2733 | Test - Epoch [1612/5000], Loss: 0.7353 MEE: 0.8979 \n",
      " N. Epochs = 1612 - Loss (train | test)= (0.07539 | 0.7353) - MEE (train | test) = (0.2732776403427124 | 0.8979102373123169)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.0748 +- 0.0168 | 0.6838 +- 0.05883)- MEE (train | test/val ) = ( 0.279 +-  0.01677 | 0.8766 +- 0.01832)\n",
      "Epoch 01461: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1461/5000], Loss: 0.7614 MEE: 1.0303     \n",
      "Training - Epoch [1494/5000], Loss: 0.1330, MEE: 0.2930 | Test - Epoch [1494/5000], Loss: 0.7440 MEE: 1.0163 \n",
      " N. Epochs = 1494 - Loss (train | test)= (0.133 | 0.744) - MEE (train | test) = (0.2929641306400299 | 1.0162732601165771)\n",
      "Training - Epoch [1302/5000], Loss: 0.1039, MEE: 0.3461 | Test - Epoch [1302/5000], Loss: 0.6402 MEE: 0.9910     \n",
      " N. Epochs = 1302 - Loss (train | test)= (0.1039 | 0.6402) - MEE (train | test) = (0.34607094526290894 | 0.9910101890563965)\n",
      "Epoch 01411: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1411/5000], Loss: 0.7010 MEE: 0.9795     \n",
      "Training - Epoch [1491/5000], Loss: 0.1064, MEE: 0.2893 | Test - Epoch [1491/5000], Loss: 0.6653 MEE: 0.9599 \n",
      " N. Epochs = 1491 - Loss (train | test)= (0.1064 | 0.6653) - MEE (train | test) = (0.28927645087242126 | 0.959922730922699)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1144 +- 0.01316 | 0.6832 +- 0.04421)- MEE (train | test/val ) = ( 0.3094 +-  0.02595 | 0.9891 +- 0.02305)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.1, 0.7, 0.2] --> train_loss = 0.09197 +- 0.01662 | val_loss = 0.5791 +- 0.1476train_mee = 0.2943 +- 0.01245 | val_mee = 0.8602 +- 0.1125\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.3, 0.6, 0.1]\n",
      "Epoch 02010: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2010/5000], Loss: 0.5748 MEE: 0.8000     \n",
      "Training - Epoch [2219/5000], Loss: 0.0642, MEE: 0.2908 | Test - Epoch [2219/5000], Loss: 0.5469 MEE: 0.7580 \n",
      " N. Epochs = 2219 - Loss (train | test)= (0.06417 | 0.5469) - MEE (train | test) = (0.2908194661140442 | 0.7579553723335266)\n",
      "Epoch 01976: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1976/5000], Loss: 0.4585 MEE: 0.7889     \n",
      "Epoch 02190: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2190/5000], Loss: 0.4229 MEE: 0.7176 \n",
      "Epoch 02331: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [2331/5000], Loss: 0.4139 MEE: 0.7027 \n",
      "Training - Epoch [2358/5000], Loss: 0.0597, MEE: 0.2680 | Test - Epoch [2358/5000], Loss: 0.4176 MEE: 0.7012 \n",
      " N. Epochs = 2358 - Loss (train | test)= (0.05975 | 0.4176) - MEE (train | test) = (0.2680351436138153 | 0.7011932730674744)\n",
      "Epoch 02010: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2010/5000], Loss: 0.4527 MEE: 0.7965     \n",
      "Epoch 02138: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2138/5000], Loss: 0.4158 MEE: 0.7415 \n",
      "Training - Epoch [2166/5000], Loss: 0.0836, MEE: 0.3009 | Test - Epoch [2166/5000], Loss: 0.4130 MEE: 0.7342 \n",
      " N. Epochs = 2166 - Loss (train | test)= (0.08365 | 0.413) - MEE (train | test) = (0.30092519521713257 | 0.7341791987419128)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.06919 +- 0.01038 | 0.4592 +- 0.06206)- MEE (train | test/val ) = ( 0.2866 +-  0.01376 | 0.7311 +- 0.02327)\n",
      "Epoch 02016: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2016/5000], Loss: 0.9416 MEE: 1.0243   9 \n",
      "Epoch 02063: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2063/5000], Loss: 0.8947 MEE: 0.9943 \n",
      "Training - Epoch [2103/5000], Loss: 0.0709, MEE: 0.2747 | Test - Epoch [2103/5000], Loss: 0.8889 MEE: 0.9881 \n",
      " N. Epochs = 2103 - Loss (train | test)= (0.07092 | 0.8889) - MEE (train | test) = (0.2746993899345398 | 0.9880660176277161)\n",
      "Epoch 02024: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2024/5000], Loss: 0.9245 MEE: 1.0426     \n",
      "Epoch 02058: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2058/5000], Loss: 0.9019 MEE: 1.0164 \n",
      "Epoch 02218: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [2218/5000], Loss: 0.8970 MEE: 1.0069 \n",
      "Training - Epoch [2249/5000], Loss: 0.0630, MEE: 0.2562 | Test - Epoch [2249/5000], Loss: 0.8837 MEE: 0.9962 \n",
      " N. Epochs = 2249 - Loss (train | test)= (0.06295 | 0.8837) - MEE (train | test) = (0.25617319345474243 | 0.9961634874343872)\n",
      "Training - Epoch [2028/5000], Loss: 0.1107, MEE: 0.3532 | Test - Epoch [2028/5000], Loss: 0.9876 MEE: 1.0416     \n",
      " N. Epochs = 2028 - Loss (train | test)= (0.1107 | 0.9876) - MEE (train | test) = (0.35318925976753235 | 1.0416210889816284)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.08153 +- 0.0209 | 0.9201 +- 0.0478)- MEE (train | test/val ) = ( 0.2947 +-  0.04205 | 1.009 +- 0.02357)\n",
      "Training - Epoch [2033/5000], Loss: 0.1257, MEE: 0.3688 | Test - Epoch [2033/5000], Loss: 0.6599 MEE: 0.9955     \n",
      " N. Epochs = 2033 - Loss (train | test)= (0.1257 | 0.6599) - MEE (train | test) = (0.3688008189201355 | 0.995524525642395)\n",
      "Epoch 01865: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1865/5000], Loss: 0.8965 MEE: 1.0647     \n",
      "Epoch 02275: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2275/5000], Loss: 0.5732 MEE: 0.9017 \n",
      "Training - Epoch [2315/5000], Loss: 0.0649, MEE: 0.2626 | Test - Epoch [2315/5000], Loss: 0.5635 MEE: 0.8931 \n",
      " N. Epochs = 2315 - Loss (train | test)= (0.06493 | 0.5635) - MEE (train | test) = (0.2626414895057678 | 0.8930549621582031)\n",
      "Epoch 02002: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2002/5000], Loss: 0.6972 MEE: 1.0244     \n",
      "Training - Epoch [2181/5000], Loss: 0.1122, MEE: 0.2890 | Test - Epoch [2181/5000], Loss: 0.6213 MEE: 0.9575 \n",
      " N. Epochs = 2181 - Loss (train | test)= (0.1122 | 0.6213) - MEE (train | test) = (0.28899282217025757 | 0.9574570059776306)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1009 +- 0.02606 | 0.6149 +- 0.03961)- MEE (train | test/val ) = ( 0.3068 +-  0.04513 | 0.9487 +- 0.04229)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.3, 0.6, 0.1] --> train_loss = 0.08389 +- 0.01307 | val_loss = 0.6647 +- 0.1914train_mee = 0.296 +- 0.008309 | val_mee = 0.8961 +- 0.1192\n",
      "[1000, 1e-05, 0.9, 128, 0.001, Tanh(), [0.25, 0.5, 0.25]]\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [1e-5]\n",
    "momentums = [0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "proportionss = [[0.1,0.8,0.1],[0.2,0.6,0.2],[0.25,0.50,0.25],[0.2,0.7,0.1],[0.1,0.7,0.2],[0.3,0.6,0.1]]\n",
    "#proportionss = [[0.8,0.1,0.1],[0.7,0.2,0.1],[0.6,0.3,0.1],[0.6,0.2,0.2],[0.8,0.15,0.05],[0.5,0.4,0.1],[0.5,0.25,0.25], [0.5,0.3,0.2]]\n",
    "#proportionss = [[0.1,0.1,0.8],[0.1,0.2,0.7],[0.1,0.3,0.6],[0.2,0.2,0.6],[0.05,0.15,0.8],[0.1,0.4,0.5],[0.25,0.25,0.5], [0.2,0.3,0.5]]\n",
    "#proportionss = [[0.2,0.3,0.5],[0.25,0.25,0.5],[0.15,0.35,0.5],[0.15,0.3,0.55],[0.1,0.25,0.55],[0.05,0.40,0.55],[0.25,0.35,0.4]]\n",
    "optimiz = 'RMS'\n",
    "best_hp = perform_grid_search_kfold(hidden_neurons,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    activations,\n",
    "                                    optimiz,\n",
    "                                    proportionss,\n",
    "                                    k_folds=3,\n",
    "                                    x=X_train,\n",
    "                                    y=y_train,\n",
    "                                    num_epochs=5000,\n",
    "                                    plot_curves=False,\n",
    "                                    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.8, 0.1, 0.1]\n",
      "Epoch 02034: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2034/5000], Loss: 0.8040 MEE: 1.0254     \n",
      "Epoch 02216: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2216/5000], Loss: 0.7480 MEE: 0.9625 \n",
      "Epoch 02339: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [2339/5000], Loss: 0.7365 MEE: 0.9429 \n",
      "Training - Epoch [2400/5000], Loss: 0.0785, MEE: 0.3508 | Test - Epoch [2400/5000], Loss: 0.7396 MEE: 0.9433 \n",
      " N. Epochs = 2400 - Loss (train | test)= (0.07854 | 0.7396) - MEE (train | test) = (0.35078898072242737 | 0.9432961940765381)\n",
      "Epoch 02187: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2187/5000], Loss: 1.0519 MEE: 1.1063     \n",
      "Epoch 02304: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2304/5000], Loss: 1.0152 MEE: 1.0729 \n",
      "Training - Epoch [2447/5000], Loss: 0.0517, MEE: 0.2966 | Test - Epoch [2447/5000], Loss: 0.9815 MEE: 1.0506 \n",
      " N. Epochs = 2447 - Loss (train | test)= (0.0517 | 0.9815) - MEE (train | test) = (0.29660564661026 | 1.050609827041626)\n",
      "Epoch 02240: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2240/5000], Loss: 0.6257 MEE: 0.9005     \n",
      "Epoch 02504: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2504/5000], Loss: 0.5826 MEE: 0.8454 \n",
      "Training - Epoch [2546/5000], Loss: 0.0533, MEE: 0.2999 | Test - Epoch [2546/5000], Loss: 0.5860 MEE: 0.8434 \n",
      " N. Epochs = 2546 - Loss (train | test)= (0.05334 | 0.586) - MEE (train | test) = (0.2999461591243744 | 0.8434329628944397)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.0612 +- 0.01228 | 0.769 +- 0.1628)- MEE (train | test/val ) = ( 0.3158 +-  0.02479 | 0.9458 +- 0.0846)\n",
      "Training - Epoch [2277/5000], Loss: 0.1732, MEE: 0.4329 | Test - Epoch [2277/5000], Loss: 1.2305 MEE: 1.2286     \n",
      " N. Epochs = 2277 - Loss (train | test)= (0.1732 | 1.231) - MEE (train | test) = (0.432889848947525 | 1.2286044359207153)\n",
      "Epoch 02181: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2181/5000], Loss: 1.0095 MEE: 1.1423     \n",
      "Epoch 02420: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2420/5000], Loss: 0.8510 MEE: 1.0706 \n",
      "Training - Epoch [2631/5000], Loss: 0.0642, MEE: 0.2946 | Test - Epoch [2631/5000], Loss: 0.8055 MEE: 1.0438 \n",
      " N. Epochs = 2631 - Loss (train | test)= (0.06422 | 0.8055) - MEE (train | test) = (0.2945971190929413 | 1.0438202619552612)\n",
      "Training - Epoch [2293/5000], Loss: 0.0898, MEE: 0.3381 | Test - Epoch [2293/5000], Loss: 1.2919 MEE: 1.1774     \n",
      " N. Epochs = 2293 - Loss (train | test)= (0.08981 | 1.292) - MEE (train | test) = (0.33814024925231934 | 1.1773723363876343)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1091 +- 0.04653 | 1.109 +- 0.2163)- MEE (train | test/val ) = ( 0.3552 +-  0.05773 | 1.15 +- 0.07789)\n",
      "Training - Epoch [2117/5000], Loss: 0.0912, MEE: 0.3646 | Test - Epoch [2117/5000], Loss: 0.9203 MEE: 1.1738     \n",
      " N. Epochs = 2117 - Loss (train | test)= (0.0912 | 0.9203) - MEE (train | test) = (0.36464616656303406 | 1.173785924911499)\n",
      "Epoch 02131: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2131/5000], Loss: 0.9415 MEE: 1.2105     \n",
      "Epoch 02614: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2614/5000], Loss: 0.7122 MEE: 1.0557 \n",
      "Epoch 02661: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [2661/5000], Loss: 0.7084 MEE: 1.0480 \n",
      "Training - Epoch [2690/5000], Loss: 0.0367, MEE: 0.2442 | Test - Epoch [2690/5000], Loss: 0.7066 MEE: 1.0464 \n",
      " N. Epochs = 2690 - Loss (train | test)= (0.03671 | 0.7066) - MEE (train | test) = (0.24415957927703857 | 1.0464032888412476)\n",
      "Epoch 02223: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2223/5000], Loss: 0.9536 MEE: 1.1790     \n",
      "Training - Epoch [2460/5000], Loss: 0.0589, MEE: 0.2818 | Test - Epoch [2460/5000], Loss: 0.8724 MEE: 1.1224 \n",
      " N. Epochs = 2460 - Loss (train | test)= (0.05889 | 0.8724) - MEE (train | test) = (0.28183287382125854 | 1.1223621368408203)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.06227 +- 0.02238 | 0.8331 +- 0.09152)- MEE (train | test/val ) = ( 0.2969 +-  0.05033 | 1.114 +- 0.05232)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.8, 0.1, 0.1] --> train_loss = 0.07751 +- 0.02232 | val_loss = 0.9038 +- 0.1476train_mee = 0.3226 +- 0.0243 | val_mee = 1.07 +- 0.08902\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.7, 0.2, 0.1]\n",
      "Training - Epoch [2078/5000], Loss: 0.0730, MEE: 0.2921 | Test - Epoch [2078/5000], Loss: 0.3851 MEE: 0.7334     \n",
      " N. Epochs = 2078 - Loss (train | test)= (0.07302 | 0.3851) - MEE (train | test) = (0.29211607575416565 | 0.7333946824073792)\n",
      "Epoch 02146: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2146/5000], Loss: 0.4233 MEE: 0.7163     \n",
      "Training - Epoch [2194/5000], Loss: 0.0267, MEE: 0.2018 | Test - Epoch [2194/5000], Loss: 0.4204 MEE: 0.7064 \n",
      " N. Epochs = 2194 - Loss (train | test)= (0.02675 | 0.4204) - MEE (train | test) = (0.2017587274312973 | 0.7063607573509216)\n",
      "Epoch 02131: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2131/5000], Loss: 0.3978 MEE: 0.7092     \n",
      "Training - Epoch [2167/5000], Loss: 0.0545, MEE: 0.2428 | Test - Epoch [2167/5000], Loss: 0.3970 MEE: 0.7000 \n",
      " N. Epochs = 2167 - Loss (train | test)= (0.05451 | 0.397) - MEE (train | test) = (0.24275583028793335 | 0.6999922394752502)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.05143 +- 0.01902 | 0.4008 +- 0.01464)- MEE (train | test/val ) = ( 0.2455 +-  0.03694 | 0.7132 +- 0.01448)\n",
      "Epoch 02025: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2025/5000], Loss: 0.9142 MEE: 1.0316     \n",
      "Training - Epoch [2062/5000], Loss: 0.0970, MEE: 0.2860 | Test - Epoch [2062/5000], Loss: 0.9037 MEE: 1.0162 \n",
      " N. Epochs = 2062 - Loss (train | test)= (0.09701 | 0.9037) - MEE (train | test) = (0.28600141406059265 | 1.016185998916626)\n",
      "Epoch 01995: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1995/5000], Loss: 0.8177 MEE: 1.0062     \n",
      "Epoch 02071: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2071/5000], Loss: 0.7705 MEE: 0.9783 \n",
      "Epoch 02205: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [2205/5000], Loss: 0.7349 MEE: 0.9499 \n",
      "Training - Epoch [2236/5000], Loss: 0.0460, MEE: 0.2184 | Test - Epoch [2236/5000], Loss: 0.7347 MEE: 0.9469 \n",
      " N. Epochs = 2236 - Loss (train | test)= (0.04598 | 0.7347) - MEE (train | test) = (0.21835270524024963 | 0.9468933939933777)\n",
      "Epoch 02052: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2052/5000], Loss: 1.0798 MEE: 1.0897     \n",
      "Epoch 02153: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2153/5000], Loss: 1.0432 MEE: 1.0608 \n",
      "Training - Epoch [2264/5000], Loss: 0.0669, MEE: 0.2431 | Test - Epoch [2264/5000], Loss: 1.0271 MEE: 1.0449 \n",
      " N. Epochs = 2264 - Loss (train | test)= (0.06694 | 1.027) - MEE (train | test) = (0.24311167001724243 | 1.0449193716049194)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.06998 +- 0.02094 | 0.8885 +- 0.1199)- MEE (train | test/val ) = ( 0.2492 +-  0.02795 | 1.003 +- 0.04114)\n",
      "Epoch 02007: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2007/5000], Loss: 0.8805 MEE: 1.0940     \n",
      "Epoch 02198: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2198/5000], Loss: 0.7803 MEE: 1.0209 \n",
      "Training - Epoch [2270/5000], Loss: 0.0758, MEE: 0.2435 | Test - Epoch [2270/5000], Loss: 0.7744 MEE: 1.0135 \n",
      " N. Epochs = 2270 - Loss (train | test)= (0.07582 | 0.7744) - MEE (train | test) = (0.24346044659614563 | 1.0134979486465454)\n",
      "Epoch 01976: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1976/5000], Loss: 0.8531 MEE: 1.0929     \n",
      "Epoch 02054: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2054/5000], Loss: 0.7939 MEE: 1.0507 \n",
      "Training - Epoch [2343/5000], Loss: 0.0811, MEE: 0.2595 | Test - Epoch [2343/5000], Loss: 0.6961 MEE: 0.9890 \n",
      " N. Epochs = 2343 - Loss (train | test)= (0.08109 | 0.6961) - MEE (train | test) = (0.25952669978141785 | 0.9890084266662598)\n",
      "Epoch 02053: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2053/5000], Loss: 0.5925 MEE: 0.9536     \n",
      "Epoch 02107: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2107/5000], Loss: 0.5807 MEE: 0.9351 \n",
      "Epoch 02150: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [2150/5000], Loss: 0.5748 MEE: 0.9279 \n",
      "Training - Epoch [2191/5000], Loss: 0.0629, MEE: 0.2373 | Test - Epoch [2191/5000], Loss: 0.5634 MEE: 0.9173 \n",
      " N. Epochs = 2191 - Loss (train | test)= (0.06291 | 0.5634) - MEE (train | test) = (0.23732267320156097 | 0.9172961711883545)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.07327 +- 0.00764 | 0.678 +- 0.08709)- MEE (train | test/val ) = ( 0.2468 +-  0.009362 | 0.9733 +- 0.04082)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.7, 0.2, 0.1] --> train_loss = 0.06489 +- 0.009616 | val_loss = 0.6558 +- 0.1997train_mee = 0.2472 +- 0.0015 | val_mee = 0.8964 +- 0.1301\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.6, 0.3, 0.1]\n",
      "Epoch 02011: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2011/5000], Loss: 0.3108 MEE: 0.6670     \n",
      "Training - Epoch [2073/5000], Loss: 0.0520, MEE: 0.2523 | Test - Epoch [2073/5000], Loss: 0.3016 MEE: 0.6551 \n",
      " N. Epochs = 2073 - Loss (train | test)= (0.05195 | 0.3016) - MEE (train | test) = (0.2522762715816498 | 0.655058741569519)\n",
      "Epoch 01896: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1896/5000], Loss: 0.4077 MEE: 0.7438     \n",
      "Epoch 02051: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2051/5000], Loss: 0.3515 MEE: 0.6926 \n",
      "Training - Epoch [2136/5000], Loss: 0.0591, MEE: 0.2521 | Test - Epoch [2136/5000], Loss: 0.3452 MEE: 0.6822 \n",
      " N. Epochs = 2136 - Loss (train | test)= (0.05913 | 0.3452) - MEE (train | test) = (0.25208166241645813 | 0.6822216510772705)\n",
      "Epoch 01922: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1922/5000], Loss: 0.3353 MEE: 0.7072     \n",
      "Training - Epoch [2129/5000], Loss: 0.0442, MEE: 0.2365 | Test - Epoch [2129/5000], Loss: 0.3022 MEE: 0.6371 \n",
      " N. Epochs = 2129 - Loss (train | test)= (0.04421 | 0.3022) - MEE (train | test) = (0.23654979467391968 | 0.6371198892593384)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.05177 +- 0.006093 | 0.3164 +- 0.0204)- MEE (train | test/val ) = ( 0.247 +-  0.007368 | 0.6581 +- 0.01854)\n",
      "Epoch 01974: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1974/5000], Loss: 0.9211 MEE: 1.0070   7 \n",
      "Epoch 02185: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2185/5000], Loss: 0.8177 MEE: 0.9323 \n",
      "Training - Epoch [2211/5000], Loss: 0.0343, MEE: 0.1969 | Test - Epoch [2211/5000], Loss: 0.8146 MEE: 0.9263 \n",
      " N. Epochs = 2211 - Loss (train | test)= (0.03425 | 0.8146) - MEE (train | test) = (0.19691547751426697 | 0.9262995719909668)\n",
      "Epoch 01960: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1960/5000], Loss: 1.1109 MEE: 1.0282     \n",
      "Training - Epoch [2115/5000], Loss: 0.0550, MEE: 0.2318 | Test - Epoch [2115/5000], Loss: 0.9883 MEE: 0.9565 \n",
      " N. Epochs = 2115 - Loss (train | test)= (0.05501 | 0.9883) - MEE (train | test) = (0.23175495862960815 | 0.9564753174781799)\n",
      "Epoch 01929: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1929/5000], Loss: 0.9924 MEE: 1.0193     \n",
      "Epoch 02109: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2109/5000], Loss: 0.9454 MEE: 0.9565 \n",
      "Training - Epoch [2137/5000], Loss: 0.0438, MEE: 0.2154 | Test - Epoch [2137/5000], Loss: 0.9326 MEE: 0.9472 \n",
      " N. Epochs = 2137 - Loss (train | test)= (0.04375 | 0.9326) - MEE (train | test) = (0.21535851061344147 | 0.9471719264984131)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.04434 +- 0.008485 | 0.9118 +- 0.0724)- MEE (train | test/val ) = ( 0.2147 +-  0.01423 | 0.9433 +- 0.01262)\n",
      "Epoch 02056: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2056/5000], Loss: 0.6740 MEE: 0.9837     \n",
      "Epoch 02111: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2111/5000], Loss: 0.6288 MEE: 0.9444 \n",
      "Training - Epoch [2157/5000], Loss: 0.0658, MEE: 0.2408 | Test - Epoch [2157/5000], Loss: 0.6103 MEE: 0.9317 \n",
      " N. Epochs = 2157 - Loss (train | test)= (0.06582 | 0.6103) - MEE (train | test) = (0.24079424142837524 | 0.931705892086029)\n",
      "Epoch 02034: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2034/5000], Loss: 0.6885 MEE: 0.9643     \n",
      "Training - Epoch [2128/5000], Loss: 0.1104, MEE: 0.2657 | Test - Epoch [2128/5000], Loss: 0.6552 MEE: 0.9503 \n",
      " N. Epochs = 2128 - Loss (train | test)= (0.1104 | 0.6552) - MEE (train | test) = (0.2657028138637543 | 0.9503371119499207)\n",
      "Epoch 01952: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1952/5000], Loss: 0.6528 MEE: 0.9570     \n",
      "Training - Epoch [2159/5000], Loss: 0.0392, MEE: 0.2205 | Test - Epoch [2159/5000], Loss: 0.5076 MEE: 0.8480 \n",
      " N. Epochs = 2159 - Loss (train | test)= (0.03918 | 0.5076) - MEE (train | test) = (0.2204921394586563 | 0.8480274081230164)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.07179 +- 0.02936 | 0.591 +- 0.0618)- MEE (train | test/val ) = ( 0.2423 +-  0.01849 | 0.91 +- 0.04449)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.6, 0.3, 0.1] --> train_loss = 0.05597 +- 0.01159 | val_loss = 0.6064 +- 0.2433train_mee = 0.2347 +- 0.01426 | val_mee = 0.8372 +- 0.1273\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.6, 0.2, 0.2]\n",
      "Epoch 01261: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1261/5000], Loss: 0.3610 MEE: 0.6792     \n",
      "Training - Epoch [1363/5000], Loss: 0.0430, MEE: 0.2585 | Test - Epoch [1363/5000], Loss: 0.3463 MEE: 0.6611 \n",
      " N. Epochs = 1363 - Loss (train | test)= (0.04303 | 0.3463) - MEE (train | test) = (0.25848066806793213 | 0.6611420512199402)\n",
      "Epoch 01327: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1327/5000], Loss: 0.4459 MEE: 0.6905     \n",
      "Epoch 01429: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1429/5000], Loss: 0.4172 MEE: 0.6560 \n",
      "Training - Epoch [1474/5000], Loss: 0.0228, MEE: 0.2148 | Test - Epoch [1474/5000], Loss: 0.4118 MEE: 0.6494 \n",
      " N. Epochs = 1474 - Loss (train | test)= (0.0228 | 0.4118) - MEE (train | test) = (0.2147674560546875 | 0.6493801474571228)\n",
      "Epoch 01232: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1232/5000], Loss: 0.4792 MEE: 0.6958     \n",
      "Epoch 01301: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1301/5000], Loss: 0.4768 MEE: 0.6802 \n",
      "Training - Epoch [1362/5000], Loss: 0.0292, MEE: 0.2394 | Test - Epoch [1362/5000], Loss: 0.4656 MEE: 0.6742 \n",
      " N. Epochs = 1362 - Loss (train | test)= (0.02923 | 0.4656) - MEE (train | test) = (0.23943166434764862 | 0.6742433309555054)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.03169 +- 0.00844 | 0.4079 +- 0.04878)- MEE (train | test/val ) = ( 0.2376 +-  0.01789 | 0.6616 +- 0.01016)\n",
      "Epoch 01170: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1170/5000], Loss: 1.0022 MEE: 0.9826   6 \n",
      "Epoch 01415: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1415/5000], Loss: 0.9153 MEE: 0.9214 \n",
      "Training - Epoch [1441/5000], Loss: 0.0290, MEE: 0.2097 | Test - Epoch [1441/5000], Loss: 0.9021 MEE: 0.9058 \n",
      " N. Epochs = 1441 - Loss (train | test)= (0.02898 | 0.9021) - MEE (train | test) = (0.20970980823040009 | 0.9057999849319458)\n",
      "Epoch 01150: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1150/5000], Loss: 0.7992 MEE: 0.9493   7 \n",
      "Training - Epoch [1299/5000], Loss: 0.0356, MEE: 0.2358 | Test - Epoch [1299/5000], Loss: 0.7456 MEE: 0.8889 \n",
      " N. Epochs = 1299 - Loss (train | test)= (0.03564 | 0.7456) - MEE (train | test) = (0.2358219474554062 | 0.8888745307922363)\n",
      "Epoch 01233: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1233/5000], Loss: 0.9266 MEE: 0.9852   9 \n",
      "Epoch 01363: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1363/5000], Loss: 0.9110 MEE: 0.9630 \n",
      "Training - Epoch [1401/5000], Loss: 0.0294, MEE: 0.2146 | Test - Epoch [1401/5000], Loss: 0.8978 MEE: 0.9570 \n",
      " N. Epochs = 1401 - Loss (train | test)= (0.0294 | 0.8978) - MEE (train | test) = (0.21456007659435272 | 0.9570304155349731)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.03134 +- 0.003044 | 0.8485 +- 0.07279)- MEE (train | test/val ) = ( 0.22 +-  0.01134 | 0.9172 +- 0.02898)\n",
      "Epoch 01216: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1216/5000], Loss: 0.6254 MEE: 0.9439     \n",
      "Epoch 01392: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1392/5000], Loss: 0.5499 MEE: 0.8783 \n",
      "Training - Epoch [1502/5000], Loss: 0.0357, MEE: 0.2188 | Test - Epoch [1502/5000], Loss: 0.5289 MEE: 0.8621 \n",
      " N. Epochs = 1502 - Loss (train | test)= (0.03573 | 0.5289) - MEE (train | test) = (0.21882620453834534 | 0.8621066808700562)\n",
      "Epoch 01311: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1311/5000], Loss: 0.5146 MEE: 0.8703     \n",
      "Epoch 01333: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1333/5000], Loss: 0.5068 MEE: 0.8546 \n",
      "Epoch 01395: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1395/5000], Loss: 0.4972 MEE: 0.8448 \n",
      "Training - Epoch [1399/5000], Loss: 0.0312, MEE: 0.2224 | Test - Epoch [1399/5000], Loss: 0.4965 MEE: 0.8395 \n",
      " N. Epochs = 1399 - Loss (train | test)= (0.0312 | 0.4965) - MEE (train | test) = (0.22242240607738495 | 0.8395317196846008)\n",
      "Epoch 01252: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1252/5000], Loss: 0.5538 MEE: 0.8937     \n",
      "Epoch 01367: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1367/5000], Loss: 0.4953 MEE: 0.8423 \n",
      "Training - Epoch [1407/5000], Loss: 0.0602, MEE: 0.2425 | Test - Epoch [1407/5000], Loss: 0.4885 MEE: 0.8348 \n",
      " N. Epochs = 1407 - Loss (train | test)= (0.06023 | 0.4885) - MEE (train | test) = (0.24254246056079865 | 0.8347545862197876)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.04239 +- 0.01275 | 0.5046 +- 0.01747)- MEE (train | test/val ) = ( 0.2279 +-  0.01044 | 0.8455 +- 0.01193)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.6, 0.2, 0.2] --> train_loss = 0.03514 +- 0.005127 | val_loss = 0.587 +- 0.1891train_mee = 0.2285 +- 0.007168 | val_mee = 0.8081 +- 0.1077\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.8, 0.15, 0.05]\n",
      "Epoch 03714: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [3714/5000], Loss: 0.7880 MEE: 0.9772       \n",
      "Epoch 03996: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [3996/5000], Loss: 0.6817 MEE: 0.8854 \n",
      "Training - Epoch [4051/5000], Loss: 0.1203, MEE: 0.3429 | Test - Epoch [4051/5000], Loss: 0.6714 MEE: 0.8752 \n",
      " N. Epochs = 4051 - Loss (train | test)= (0.1203 | 0.6714) - MEE (train | test) = (0.34293100237846375 | 0.8752430081367493)\n",
      "Epoch 03797: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [3797/5000], Loss: 0.5368 MEE: 0.8627       \n",
      "Epoch 04063: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [4063/5000], Loss: 0.4996 MEE: 0.8080 \n",
      "Training - Epoch [4225/5000], Loss: 0.0787, MEE: 0.2879 | Test - Epoch [4225/5000], Loss: 0.4819 MEE: 0.7867 \n",
      " N. Epochs = 4225 - Loss (train | test)= (0.07865 | 0.4819) - MEE (train | test) = (0.2878565192222595 | 0.7867174744606018)\n",
      "Epoch 03669: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [3669/5000], Loss: 0.5749 MEE: 0.9224       \n",
      "Epoch 03690: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [3690/5000], Loss: 0.5601 MEE: 0.9083 \n",
      "Training - Epoch [4105/5000], Loss: 0.1031, MEE: 0.3338 | Test - Epoch [4105/5000], Loss: 0.5166 MEE: 0.8564 \n",
      " N. Epochs = 4105 - Loss (train | test)= (0.1031 | 0.5166) - MEE (train | test) = (0.33383598923683167 | 0.8563663363456726)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1007 +- 0.01709 | 0.5566 +- 0.08239)- MEE (train | test/val ) = ( 0.3215 +-  0.02411 | 0.8394 +- 0.03807)\n",
      "Epoch 03544: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [3544/5000], Loss: 1.3104 MEE: 1.2304     9 \n",
      "Training - Epoch [3993/5000], Loss: 0.1506, MEE: 0.3553 | Test - Epoch [3993/5000], Loss: 1.0229 MEE: 1.0824 \n",
      " N. Epochs = 3993 - Loss (train | test)= (0.1506 | 1.023) - MEE (train | test) = (0.3552735447883606 | 1.082396388053894)\n",
      "Epoch 03656: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [3656/5000], Loss: 1.4001 MEE: 1.2532       \n",
      "Training - Epoch [3986/5000], Loss: 0.1469, MEE: 0.3528 | Test - Epoch [3986/5000], Loss: 1.2121 MEE: 1.1309 \n",
      " N. Epochs = 3986 - Loss (train | test)= (0.1469 | 1.212) - MEE (train | test) = (0.35278916358947754 | 1.1308932304382324)\n",
      "Training - Epoch [4025/5000], Loss: 0.1128, MEE: 0.3286 | Test - Epoch [4025/5000], Loss: 1.1846 MEE: 1.1550       \n",
      " N. Epochs = 4025 - Loss (train | test)= (0.1128 | 1.185) - MEE (train | test) = (0.32860878109931946 | 1.1549770832061768)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1368 +- 0.01701 | 1.14 +- 0.08347)- MEE (train | test/val ) = ( 0.3456 +-  0.01203 | 1.123 +- 0.03018)\n",
      "Epoch 03705: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [3705/5000], Loss: 0.9330 MEE: 1.1663       \n",
      "Epoch 03877: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [3877/5000], Loss: 0.8617 MEE: 1.1202 \n",
      "Epoch 03918: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [3918/5000], Loss: 0.8567 MEE: 1.1133 \n",
      "Epoch 03956: reducing learning rate of group 0 to 6.2500e-07.t - Epoch [3956/5000], Loss: 0.8553 MEE: 1.1099 \n",
      "Training - Epoch [3959/5000], Loss: 0.0803, MEE: 0.2808 | Test - Epoch [3959/5000], Loss: 0.8512 MEE: 1.1077 \n",
      " N. Epochs = 3959 - Loss (train | test)= (0.08027 | 0.8512) - MEE (train | test) = (0.28081047534942627 | 1.1076512336730957)\n",
      "Epoch 03740: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [3740/5000], Loss: 0.9156 MEE: 1.1370       \n",
      "Epoch 03831: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [3831/5000], Loss: 0.8590 MEE: 1.0885 \n",
      "Epoch 03870: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [3870/5000], Loss: 0.8425 MEE: 1.0712 \n",
      "Epoch 03905: reducing learning rate of group 0 to 6.2500e-07.t - Epoch [3905/5000], Loss: 0.8379 MEE: 1.0685 \n",
      "Epoch 03933: reducing learning rate of group 0 to 3.1250e-07.t - Epoch [3933/5000], Loss: 0.8359 MEE: 1.0682 \n",
      "Training - Epoch [3936/5000], Loss: 0.0973, MEE: 0.3162 | Test - Epoch [3936/5000], Loss: 0.8358 MEE: 1.0666 \n",
      " N. Epochs = 3936 - Loss (train | test)= (0.09734 | 0.8358) - MEE (train | test) = (0.3161923885345459 | 1.0666176080703735)\n",
      "Epoch 03529: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [3529/5000], Loss: 0.9674 MEE: 1.2096       \n",
      "Training - Epoch [3732/5000], Loss: 0.1192, MEE: 0.3566 | Test - Epoch [3732/5000], Loss: 0.8307 MEE: 1.0979 \n",
      " N. Epochs = 3732 - Loss (train | test)= (0.1192 | 0.8307) - MEE (train | test) = (0.3565768599510193 | 1.097878336906433)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.09895 +- 0.01594 | 0.8392 +- 0.00869)- MEE (train | test/val ) = ( 0.3179 +-  0.03095 | 1.091 +- 0.0175)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.8, 0.15, 0.05] --> train_loss = 0.1121 +- 0.01743 | val_loss = 0.8452 +- 0.2381train_mee = 0.3283 +- 0.01228 | val_mee = 1.018 +- 0.1267\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.5, 0.4, 0.1]\n",
      "Epoch 01981: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1981/5000], Loss: 0.3602 MEE: 0.7031     \n",
      "Training - Epoch [2151/5000], Loss: 0.0712, MEE: 0.2663 | Test - Epoch [2151/5000], Loss: 0.3420 MEE: 0.6714 \n",
      " N. Epochs = 2151 - Loss (train | test)= (0.0712 | 0.342) - MEE (train | test) = (0.26634931564331055 | 0.671368420124054)\n",
      "Epoch 01937: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1937/5000], Loss: 0.3560 MEE: 0.7273     \n",
      "Epoch 02108: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2108/5000], Loss: 0.3422 MEE: 0.6935 \n",
      "Epoch 02333: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [2333/5000], Loss: 0.3304 MEE: 0.6606 \n",
      "Training - Epoch [2359/5000], Loss: 0.0415, MEE: 0.2214 | Test - Epoch [2359/5000], Loss: 0.3297 MEE: 0.6591 \n",
      " N. Epochs = 2359 - Loss (train | test)= (0.04148 | 0.3297) - MEE (train | test) = (0.2213640958070755 | 0.6590672731399536)\n",
      "Epoch 01832: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1832/5000], Loss: 0.5275 MEE: 0.8047     \n",
      "Epoch 01964: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1964/5000], Loss: 0.4232 MEE: 0.7061 \n",
      "Epoch 02181: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [2181/5000], Loss: 0.3932 MEE: 0.6701 \n",
      "Training - Epoch [2207/5000], Loss: 0.0512, MEE: 0.2384 | Test - Epoch [2207/5000], Loss: 0.3905 MEE: 0.6672 \n",
      " N. Epochs = 2207 - Loss (train | test)= (0.05117 | 0.3905) - MEE (train | test) = (0.23838047683238983 | 0.6671507358551025)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.05462 +- 0.01238 | 0.3541 +- 0.02625)- MEE (train | test/val ) = ( 0.242 +-  0.01855 | 0.6659 +- 0.005104)\n",
      "Epoch 01952: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1952/5000], Loss: 0.8700 MEE: 0.9578     \n",
      "Epoch 02055: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2055/5000], Loss: 0.8320 MEE: 0.9290 \n",
      "Epoch 02208: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [2208/5000], Loss: 0.8120 MEE: 0.9148 \n",
      "Training - Epoch [2260/5000], Loss: 0.0463, MEE: 0.2185 | Test - Epoch [2260/5000], Loss: 0.7998 MEE: 0.9089 \n",
      " N. Epochs = 2260 - Loss (train | test)= (0.04627 | 0.7998) - MEE (train | test) = (0.2184867560863495 | 0.9088948965072632)\n",
      "Epoch 02220: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2220/5000], Loss: 0.8991 MEE: 0.9546     \n",
      "Epoch 02242: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2242/5000], Loss: 0.8888 MEE: 0.9442 \n",
      "Training - Epoch [2244/5000], Loss: 0.0398, MEE: 0.2064 | Test - Epoch [2244/5000], Loss: 0.8783 MEE: 0.9344 \n",
      " N. Epochs = 2244 - Loss (train | test)= (0.0398 | 0.8783) - MEE (train | test) = (0.20644228160381317 | 0.9344233870506287)\n",
      "Epoch 01993: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1993/5000], Loss: 0.8543 MEE: 0.9838     \n",
      "Epoch 02042: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2042/5000], Loss: 0.8491 MEE: 0.9551 \n",
      "Epoch 02087: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [2087/5000], Loss: 0.8395 MEE: 0.9441 \n",
      "Training - Epoch [2161/5000], Loss: 0.0621, MEE: 0.2320 | Test - Epoch [2161/5000], Loss: 0.8320 MEE: 0.9367 \n",
      " N. Epochs = 2161 - Loss (train | test)= (0.06209 | 0.832) - MEE (train | test) = (0.23204569518566132 | 0.9366644620895386)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.04939 +- 0.009362 | 0.8367 +- 0.03222)- MEE (train | test/val ) = ( 0.219 +-  0.01046 | 0.9267 +- 0.0126)\n",
      "Epoch 01824: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1824/5000], Loss: 0.8523 MEE: 1.0649     \n",
      "Epoch 02206: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2206/5000], Loss: 0.5659 MEE: 0.8709 \n",
      "Training - Epoch [2231/5000], Loss: 0.0383, MEE: 0.2115 | Test - Epoch [2231/5000], Loss: 0.5584 MEE: 0.8631 \n",
      " N. Epochs = 2231 - Loss (train | test)= (0.03833 | 0.5584) - MEE (train | test) = (0.2114613652229309 | 0.8631040453910828)\n",
      "Epoch 02033: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [2033/5000], Loss: 0.6002 MEE: 0.9360     \n",
      "Epoch 02137: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2137/5000], Loss: 0.5374 MEE: 0.8719 \n",
      "Epoch 02200: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [2200/5000], Loss: 0.5252 MEE: 0.8611 \n",
      "Training - Epoch [2240/5000], Loss: 0.0393, MEE: 0.2057 | Test - Epoch [2240/5000], Loss: 0.5224 MEE: 0.8577 \n",
      " N. Epochs = 2240 - Loss (train | test)= (0.03935 | 0.5224) - MEE (train | test) = (0.2057470828294754 | 0.8576639890670776)\n",
      "Epoch 01907: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1907/5000], Loss: 0.6706 MEE: 0.9631     \n",
      "Epoch 02059: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [2059/5000], Loss: 0.5651 MEE: 0.8846 \n",
      "Training - Epoch [2095/5000], Loss: 0.0410, MEE: 0.2176 | Test - Epoch [2095/5000], Loss: 0.5570 MEE: 0.8798 \n",
      " N. Epochs = 2095 - Loss (train | test)= (0.04102 | 0.557) - MEE (train | test) = (0.21762172877788544 | 0.8798207640647888)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.03956 +- 0.001108 | 0.5459 +- 0.01663)- MEE (train | test/val ) = ( 0.2116 +-  0.004849 | 0.8669 +- 0.009428)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.5, 0.4, 0.1] --> train_loss = 0.04786 +- 0.00624 | val_loss = 0.5789 +- 0.1984train_mee = 0.2242 +- 0.01296 | val_mee = 0.8198 +- 0.1116\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.5, 0.25, 0.25]\n",
      "Epoch 01101: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1101/5000], Loss: 0.3423 MEE: 0.6100     \n",
      "Training - Epoch [1129/5000], Loss: 0.0268, MEE: 0.2384 | Test - Epoch [1129/5000], Loss: 0.3286 MEE: 0.5971 \n",
      " N. Epochs = 1129 - Loss (train | test)= (0.02684 | 0.3286) - MEE (train | test) = (0.23844414949417114 | 0.5970894694328308)\n",
      "Epoch 01090: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1090/5000], Loss: 0.3020 MEE: 0.6002     \n",
      "Epoch 01243: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1243/5000], Loss: 0.2809 MEE: 0.5755 \n",
      "Training - Epoch [1245/5000], Loss: 0.0222, MEE: 0.2225 | Test - Epoch [1245/5000], Loss: 0.2783 MEE: 0.5704 \n",
      " N. Epochs = 1245 - Loss (train | test)= (0.0222 | 0.2783) - MEE (train | test) = (0.22247211635112762 | 0.5703703165054321)\n",
      "Epoch 01085: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1085/5000], Loss: 0.3182 MEE: 0.5861     \n",
      "Epoch 01154: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1154/5000], Loss: 0.3109 MEE: 0.5768 \n",
      "Training - Epoch [1203/5000], Loss: 0.0239, MEE: 0.2253 | Test - Epoch [1203/5000], Loss: 0.3062 MEE: 0.5743 \n",
      " N. Epochs = 1203 - Loss (train | test)= (0.02395 | 0.3062) - MEE (train | test) = (0.2253023236989975 | 0.5743235945701599)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.02433 +- 0.001915 | 0.3043 +- 0.02058)- MEE (train | test/val ) = ( 0.2287 +-  0.006959 | 0.5806 +- 0.01177)\n",
      "Epoch 01017: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1017/5000], Loss: 0.5280 MEE: 0.8023   1 \n",
      "Epoch 01190: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1190/5000], Loss: 0.4741 MEE: 0.7610 \n",
      "Training - Epoch [1263/5000], Loss: 0.0317, MEE: 0.2298 | Test - Epoch [1263/5000], Loss: 0.4688 MEE: 0.7561 \n",
      " N. Epochs = 1263 - Loss (train | test)= (0.03169 | 0.4688) - MEE (train | test) = (0.22984817624092102 | 0.756051778793335)\n",
      "Epoch 01095: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1095/5000], Loss: 0.6554 MEE: 0.8370     \n",
      "Epoch 01133: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1133/5000], Loss: 0.6519 MEE: 0.8340 \n",
      "Epoch 01171: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1171/5000], Loss: 0.6561 MEE: 0.8284 \n",
      "Training - Epoch [1216/5000], Loss: 0.0253, MEE: 0.2131 | Test - Epoch [1216/5000], Loss: 0.6517 MEE: 0.8247 \n",
      " N. Epochs = 1216 - Loss (train | test)= (0.02532 | 0.6517) - MEE (train | test) = (0.21306832134723663 | 0.8246868848800659)\n",
      "Epoch 01140: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1140/5000], Loss: 0.5139 MEE: 0.8169     \n",
      "Epoch 01184: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1184/5000], Loss: 0.5031 MEE: 0.7952 \n",
      "Training - Epoch [1237/5000], Loss: 0.0239, MEE: 0.2168 | Test - Epoch [1237/5000], Loss: 0.4924 MEE: 0.7842 \n",
      " N. Epochs = 1237 - Loss (train | test)= (0.02395 | 0.4924) - MEE (train | test) = (0.21682289242744446 | 0.784187912940979)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.02698 +- 0.003371 | 0.5376 +- 0.08125)- MEE (train | test/val ) = ( 0.2199 +-  0.00719 | 0.7883 +- 0.02817)\n",
      "Epoch 01025: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1025/5000], Loss: 0.4905 MEE: 0.8574     \n",
      "Training - Epoch [1071/5000], Loss: 0.0590, MEE: 0.2745 | Test - Epoch [1071/5000], Loss: 0.4754 MEE: 0.8383 \n",
      " N. Epochs = 1071 - Loss (train | test)= (0.059 | 0.4754) - MEE (train | test) = (0.27450212836265564 | 0.8383116722106934)\n",
      "Epoch 01079: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1079/5000], Loss: 0.4318 MEE: 0.8051     \n",
      "Epoch 01126: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1126/5000], Loss: 0.4285 MEE: 0.8028 \n",
      "Training - Epoch [1183/5000], Loss: 0.0280, MEE: 0.2295 | Test - Epoch [1183/5000], Loss: 0.4145 MEE: 0.7853 \n",
      " N. Epochs = 1183 - Loss (train | test)= (0.028 | 0.4145) - MEE (train | test) = (0.22949625551700592 | 0.785267174243927)\n",
      "Epoch 01136: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1136/5000], Loss: 0.4599 MEE: 0.8293     \n",
      "Epoch 01160: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1160/5000], Loss: 0.4586 MEE: 0.8134 \n",
      "Epoch 01276: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1276/5000], Loss: 0.4405 MEE: 0.8038 \n",
      "Training - Epoch [1280/5000], Loss: 0.0517, MEE: 0.2315 | Test - Epoch [1280/5000], Loss: 0.4376 MEE: 0.7962 \n",
      " N. Epochs = 1280 - Loss (train | test)= (0.05173 | 0.4376) - MEE (train | test) = (0.23154689371585846 | 0.7961778044700623)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.04624 +- 0.01324 | 0.4425 +- 0.02513)- MEE (train | test/val ) = ( 0.2452 +-  0.02075 | 0.8066 +- 0.02287)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.5, 0.25, 0.25] --> train_loss = 0.03252 +- 0.009765 | val_loss = 0.4282 +- 0.09578train_mee = 0.2313 +- 0.01047 | val_mee = 0.7252 +- 0.1025\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.5, 0.3, 0.2]\n",
      "Epoch 01263: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1263/5000], Loss: 0.3287 MEE: 0.6438     \n",
      "Epoch 01352: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1352/5000], Loss: 0.3207 MEE: 0.6210 \n",
      "Training - Epoch [1354/5000], Loss: 0.0208, MEE: 0.2084 | Test - Epoch [1354/5000], Loss: 0.3204 MEE: 0.6124 \n",
      " N. Epochs = 1354 - Loss (train | test)= (0.02082 | 0.3204) - MEE (train | test) = (0.20837263762950897 | 0.6123605966567993)\n",
      "Epoch 01249: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1249/5000], Loss: 0.3146 MEE: 0.6341     \n",
      "Training - Epoch [1279/5000], Loss: 0.0240, MEE: 0.2177 | Test - Epoch [1279/5000], Loss: 0.3084 MEE: 0.6100 \n",
      " N. Epochs = 1279 - Loss (train | test)= (0.02396 | 0.3084) - MEE (train | test) = (0.21770340204238892 | 0.6099840998649597)\n",
      "Epoch 01138: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1138/5000], Loss: 0.3191 MEE: 0.6679     \n",
      "Epoch 01306: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1306/5000], Loss: 0.2870 MEE: 0.6186 \n",
      "Training - Epoch [1493/5000], Loss: 0.0254, MEE: 0.2051 | Test - Epoch [1493/5000], Loss: 0.2790 MEE: 0.5982 \n",
      " N. Epochs = 1493 - Loss (train | test)= (0.02536 | 0.279) - MEE (train | test) = (0.20514200627803802 | 0.5982031226158142)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.02338 +- 0.001899 | 0.3026 +- 0.01742)- MEE (train | test/val ) = ( 0.2104 +-  0.005326 | 0.6068 +- 0.00619)\n",
      "Training - Epoch [1228/5000], Loss: 0.0375, MEE: 0.2367 | Test - Epoch [1228/5000], Loss: 0.6931 MEE: 0.8654     \n",
      " N. Epochs = 1228 - Loss (train | test)= (0.03754 | 0.6931) - MEE (train | test) = (0.23672746121883392 | 0.865384578704834)\n",
      "Epoch 01107: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1107/5000], Loss: 0.8456 MEE: 0.9198     \n",
      "Epoch 01205: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1205/5000], Loss: 0.8193 MEE: 0.8822 \n",
      "Training - Epoch [1421/5000], Loss: 0.0336, MEE: 0.2195 | Test - Epoch [1421/5000], Loss: 0.7973 MEE: 0.8575 \n",
      " N. Epochs = 1421 - Loss (train | test)= (0.03362 | 0.7973) - MEE (train | test) = (0.21949456632137299 | 0.8575018644332886)\n",
      "Epoch 01139: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1139/5000], Loss: 0.9759 MEE: 0.9714     \n",
      "Epoch 01282: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1282/5000], Loss: 0.9100 MEE: 0.9105 \n",
      "Training - Epoch [1348/5000], Loss: 0.0435, MEE: 0.2307 | Test - Epoch [1348/5000], Loss: 0.8992 MEE: 0.9015 \n",
      " N. Epochs = 1348 - Loss (train | test)= (0.04349 | 0.8992) - MEE (train | test) = (0.23068338632583618 | 0.9015288352966309)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.03822 +- 0.004055 | 0.7966 +- 0.08412)- MEE (train | test/val ) = ( 0.229 +-  0.007139 | 0.8748 +- 0.01917)\n",
      "Epoch 01168: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1168/5000], Loss: 0.5897 MEE: 0.9095     \n",
      "Epoch 01242: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1242/5000], Loss: 0.5397 MEE: 0.8609 \n",
      "Training - Epoch [1296/5000], Loss: 0.0518, MEE: 0.2327 | Test - Epoch [1296/5000], Loss: 0.5351 MEE: 0.8580 \n",
      " N. Epochs = 1296 - Loss (train | test)= (0.05176 | 0.5351) - MEE (train | test) = (0.23265820741653442 | 0.8580161333084106)\n",
      "Epoch 01171: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1171/5000], Loss: 0.5571 MEE: 0.9046     \n",
      "Epoch 01299: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1299/5000], Loss: 0.5164 MEE: 0.8653 \n",
      "Training - Epoch [1345/5000], Loss: 0.0363, MEE: 0.2256 | Test - Epoch [1345/5000], Loss: 0.5092 MEE: 0.8542 \n",
      " N. Epochs = 1345 - Loss (train | test)= (0.0363 | 0.5092) - MEE (train | test) = (0.22564296424388885 | 0.8541948199272156)\n",
      "Epoch 01198: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1198/5000], Loss: 0.5106 MEE: 0.8791     \n",
      "Epoch 01336: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1336/5000], Loss: 0.4768 MEE: 0.8314 \n",
      "Training - Epoch [1394/5000], Loss: 0.0278, MEE: 0.2077 | Test - Epoch [1394/5000], Loss: 0.4517 MEE: 0.8076 \n",
      " N. Epochs = 1394 - Loss (train | test)= (0.02776 | 0.4517) - MEE (train | test) = (0.20766392350196838 | 0.8076270222663879)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.0386 +- 0.009933 | 0.4987 +- 0.03488)- MEE (train | test/val ) = ( 0.222 +-  0.01053 | 0.8399 +- 0.02291)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.5, 0.3, 0.2] --> train_loss = 0.0334 +- 0.007086 | val_loss = 0.5326 +- 0.2031train_mee = 0.2205 +- 0.007655 | val_mee = 0.7739 +- 0.119\n",
      "[1000, 1e-05, 0.9, 128, 0.001, Tanh(), [0.5, 0.25, 0.25]]\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [1e-5]\n",
    "momentums = [0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "#proportionss = [[0.1,0.8,0.1],[0.2,0.6,0.2],[0.25,0.50,0.25],[0.2,0.7,0.1],[0.1,0.7,0.2],[0.3,0.6,0.1]]\n",
    "proportionss = [[0.8,0.1,0.1],[0.7,0.2,0.1],[0.6,0.3,0.1],[0.6,0.2,0.2],[0.8,0.15,0.05],[0.5,0.4,0.1],[0.5,0.25,0.25], [0.5,0.3,0.2]]\n",
    "#proportionss = [[0.1,0.1,0.8],[0.1,0.2,0.7],[0.1,0.3,0.6],[0.2,0.2,0.6],[0.05,0.15,0.8],[0.1,0.4,0.5],[0.25,0.25,0.5], [0.2,0.3,0.5]]\n",
    "#proportionss = [[0.2,0.3,0.5],[0.25,0.25,0.5],[0.15,0.35,0.5],[0.15,0.3,0.55],[0.1,0.25,0.55],[0.05,0.40,0.55],[0.25,0.35,0.4]]\n",
    "optimiz = 'RMS'\n",
    "best_hp = perform_grid_search_kfold(hidden_neurons,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    activations,\n",
    "                                    optimiz,\n",
    "                                    proportionss,\n",
    "                                    k_folds=3,\n",
    "                                    x=X_train,\n",
    "                                    y=y_train,\n",
    "                                    num_epochs=5000,\n",
    "                                    plot_curves=False,\n",
    "                                    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.1, 0.1, 0.8]\n",
      "Epoch 01110: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1110/5000], Loss: 0.4811 MEE: 0.9321   \n",
      "Training - Epoch [1224/5000], Loss: 0.2282, MEE: 0.7180 | Test - Epoch [1224/5000], Loss: 0.4347 MEE: 0.8640 \n",
      " N. Epochs = 1224 - Loss (train | test)= (0.2282 | 0.4347) - MEE (train | test) = (0.7179601788520813 | 0.8640018701553345)\n",
      "Epoch 01341: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1341/5000], Loss: 0.4688 MEE: 0.8862   \n",
      "Training - Epoch [1644/5000], Loss: 0.1611, MEE: 0.6056 | Test - Epoch [1644/5000], Loss: 0.4057 MEE: 0.8139 \n",
      " N. Epochs = 1644 - Loss (train | test)= (0.1611 | 0.4057) - MEE (train | test) = (0.605635941028595 | 0.8138821721076965)\n",
      "Epoch 01191: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1191/5000], Loss: 0.4094 MEE: 0.8389   \n",
      "Training - Epoch [1282/5000], Loss: 0.2024, MEE: 0.6764 | Test - Epoch [1282/5000], Loss: 0.3956 MEE: 0.8208 \n",
      " N. Epochs = 1282 - Loss (train | test)= (0.2024 | 0.3956) - MEE (train | test) = (0.6763911247253418 | 0.8208374977111816)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1972 +- 0.02765 | 0.412 +- 0.01655)- MEE (train | test/val ) = ( 0.6667 +-  0.04637 | 0.8329 +- 0.02217)\n",
      "Epoch 01058: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1058/5000], Loss: 0.5874 MEE: 1.0683   \n",
      "Epoch 01244: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1244/5000], Loss: 0.5458 MEE: 1.0180 \n",
      "Training - Epoch [1330/5000], Loss: 0.1824, MEE: 0.6500 | Test - Epoch [1330/5000], Loss: 0.5344 MEE: 1.0029 \n",
      " N. Epochs = 1330 - Loss (train | test)= (0.1824 | 0.5344) - MEE (train | test) = (0.6499509811401367 | 1.0029399394989014)\n",
      "Epoch 01278: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1278/5000], Loss: 0.5277 MEE: 1.0111   \n",
      "Epoch 01437: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1437/5000], Loss: 0.4888 MEE: 0.9441 \n",
      "Training - Epoch [1441/5000], Loss: 0.1528, MEE: 0.5944 | Test - Epoch [1441/5000], Loss: 0.4958 MEE: 0.9490 \n",
      " N. Epochs = 1441 - Loss (train | test)= (0.1528 | 0.4958) - MEE (train | test) = (0.5943868160247803 | 0.9490052461624146)\n",
      "Epoch 01412: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1412/5000], Loss: 0.4415 MEE: 0.9073   \n",
      "Epoch 01487: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1487/5000], Loss: 0.4270 MEE: 0.8918 \n",
      "Epoch 01531: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1531/5000], Loss: 0.4244 MEE: 0.8821 \n",
      "Training - Epoch [1554/5000], Loss: 0.1372, MEE: 0.5587 | Test - Epoch [1554/5000], Loss: 0.4227 MEE: 0.8815 \n",
      " N. Epochs = 1554 - Loss (train | test)= (0.1372 | 0.4227) - MEE (train | test) = (0.5587456822395325 | 0.8814720511436462)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1574 +- 0.01873 | 0.4843 +- 0.04631)- MEE (train | test/val ) = ( 0.601 +-  0.03753 | 0.9445 +- 0.04969)\n",
      "Epoch 01346: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1346/5000], Loss: 0.4407 MEE: 0.9313   \n",
      "Epoch 01450: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1450/5000], Loss: 0.4221 MEE: 0.9002 \n",
      "Epoch 01476: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1476/5000], Loss: 0.4186 MEE: 0.8941 \n",
      "Training - Epoch [1477/5000], Loss: 0.1432, MEE: 0.5688 | Test - Epoch [1477/5000], Loss: 0.4170 MEE: 0.8926 \n",
      " N. Epochs = 1477 - Loss (train | test)= (0.1432 | 0.417) - MEE (train | test) = (0.568821907043457 | 0.8925999999046326)\n",
      "Epoch 01557: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1557/5000], Loss: 0.3885 MEE: 0.8595   \n",
      "Training - Epoch [1642/5000], Loss: 0.1106, MEE: 0.5094 | Test - Epoch [1642/5000], Loss: 0.3770 MEE: 0.8410 \n",
      " N. Epochs = 1642 - Loss (train | test)= (0.1106 | 0.377) - MEE (train | test) = (0.5094255805015564 | 0.8409518599510193)\n",
      "Epoch 01090: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1090/5000], Loss: 0.5370 MEE: 1.0346   \n",
      "Epoch 01165: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1165/5000], Loss: 0.5314 MEE: 1.0174 \n",
      "Training - Epoch [1169/5000], Loss: 0.2001, MEE: 0.6697 | Test - Epoch [1169/5000], Loss: 0.5192 MEE: 1.0038 \n",
      " N. Epochs = 1169 - Loss (train | test)= (0.2001 | 0.5192) - MEE (train | test) = (0.6696803569793701 | 1.0037850141525269)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1513 +- 0.03697 | 0.4377 +- 0.05987)- MEE (train | test/val ) = ( 0.5826 +-  0.06615 | 0.9124 +- 0.06794)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.1, 0.1, 0.8] --> train_loss = 0.1687 +- 0.02037 | val_loss = 0.4447 +- 0.02992train_mee = 0.6168 +- 0.03606 | val_mee = 0.8966 +- 0.0469\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.1, 0.2, 0.7]\n",
      "Epoch 00919: reducing learning rate of group 0 to 5.0000e-06. - Epoch [919/5000], Loss: 0.3913 MEE: 0.7997     \n",
      "Epoch 01022: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1022/5000], Loss: 0.3572 MEE: 0.7614 \n",
      "Training - Epoch [1058/5000], Loss: 0.1449, MEE: 0.5691 | Test - Epoch [1058/5000], Loss: 0.3479 MEE: 0.7527 \n",
      " N. Epochs = 1058 - Loss (train | test)= (0.1449 | 0.3479) - MEE (train | test) = (0.5691152811050415 | 0.7526698112487793)\n",
      "Epoch 01186: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1186/5000], Loss: 0.3292 MEE: 0.6890   \n",
      "Epoch 01259: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1259/5000], Loss: 0.3144 MEE: 0.6759 \n",
      "Training - Epoch [1263/5000], Loss: 0.0886, MEE: 0.4562 | Test - Epoch [1263/5000], Loss: 0.3173 MEE: 0.6745 \n",
      " N. Epochs = 1263 - Loss (train | test)= (0.0886 | 0.3173) - MEE (train | test) = (0.45622530579566956 | 0.6744872331619263)\n",
      "Epoch 01274: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1274/5000], Loss: 0.3181 MEE: 0.7010   \n",
      "Epoch 01374: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1374/5000], Loss: 0.3008 MEE: 0.6789 \n",
      "Training - Epoch [1416/5000], Loss: 0.0858, MEE: 0.4467 | Test - Epoch [1416/5000], Loss: 0.3039 MEE: 0.6738 \n",
      " N. Epochs = 1416 - Loss (train | test)= (0.08577 | 0.3039) - MEE (train | test) = (0.4467441439628601 | 0.6737825274467468)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1064 +- 0.02723 | 0.3231 +- 0.01841)- MEE (train | test/val ) = ( 0.4907 +-  0.05559 | 0.7003 +- 0.03702)\n",
      "Epoch 01185: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1185/5000], Loss: 0.4396 MEE: 0.8513   \n",
      "Training - Epoch [1238/5000], Loss: 0.0868, MEE: 0.4512 | Test - Epoch [1238/5000], Loss: 0.4228 MEE: 0.8282 \n",
      " N. Epochs = 1238 - Loss (train | test)= (0.08679 | 0.4228) - MEE (train | test) = (0.45117518305778503 | 0.8282046318054199)\n",
      "Epoch 01336: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1336/5000], Loss: 0.4219 MEE: 0.8181   \n",
      "Training - Epoch [1498/5000], Loss: 0.0741, MEE: 0.4168 | Test - Epoch [1498/5000], Loss: 0.4007 MEE: 0.7934 \n",
      " N. Epochs = 1498 - Loss (train | test)= (0.07411 | 0.4007) - MEE (train | test) = (0.41683366894721985 | 0.7934256792068481)\n",
      "Epoch 01142: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1142/5000], Loss: 0.4619 MEE: 0.8830   \n",
      "Training - Epoch [1180/5000], Loss: 0.1023, MEE: 0.4856 | Test - Epoch [1180/5000], Loss: 0.4510 MEE: 0.8645 \n",
      " N. Epochs = 1180 - Loss (train | test)= (0.1023 | 0.451) - MEE (train | test) = (0.48563602566719055 | 0.8644933700561523)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.08773 +- 0.01152 | 0.4248 +- 0.02059)- MEE (train | test/val ) = ( 0.4512 +-  0.02809 | 0.8287 +- 0.02902)\n",
      "Epoch 01011: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1011/5000], Loss: 0.3857 MEE: 0.8673   \n",
      "Epoch 01248: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1248/5000], Loss: 0.3433 MEE: 0.8142 \n",
      "Training - Epoch [1252/5000], Loss: 0.0969, MEE: 0.4726 | Test - Epoch [1252/5000], Loss: 0.3472 MEE: 0.8135 \n",
      " N. Epochs = 1252 - Loss (train | test)= (0.09691 | 0.3472) - MEE (train | test) = (0.47261300683021545 | 0.8135117888450623)\n",
      "Epoch 01252: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1252/5000], Loss: 0.3515 MEE: 0.8265   \n",
      "Training - Epoch [1275/5000], Loss: 0.0919, MEE: 0.4578 | Test - Epoch [1275/5000], Loss: 0.3455 MEE: 0.8133 \n",
      " N. Epochs = 1275 - Loss (train | test)= (0.09193 | 0.3455) - MEE (train | test) = (0.4577713906764984 | 0.813338577747345)\n",
      "Training - Epoch [1105/5000], Loss: 0.1198, MEE: 0.5187 | Test - Epoch [1105/5000], Loss: 0.4259 MEE: 0.8737   \n",
      " N. Epochs = 1105 - Loss (train | test)= (0.1198 | 0.4259) - MEE (train | test) = (0.5186592936515808 | 0.8737270832061768)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1029 +- 0.01212 | 0.3729 +- 0.03752)- MEE (train | test/val ) = ( 0.483 +-  0.02592 | 0.8335 +- 0.02843)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.1, 0.2, 0.7] --> train_loss = 0.09901 +- 0.008107 | val_loss = 0.3736 +- 0.04155train_mee = 0.475 +- 0.01709 | val_mee = 0.7875 +- 0.06169\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.1, 0.3, 0.6]\n",
      "Epoch 01137: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1137/5000], Loss: 0.2874 MEE: 0.6297   \n",
      "Training - Epoch [1190/5000], Loss: 0.0733, MEE: 0.4153 | Test - Epoch [1190/5000], Loss: 0.2727 MEE: 0.6222 \n",
      " N. Epochs = 1190 - Loss (train | test)= (0.07331 | 0.2727) - MEE (train | test) = (0.41530632972717285 | 0.6222448945045471)\n",
      "Epoch 00921: reducing learning rate of group 0 to 5.0000e-06. - Epoch [921/5000], Loss: 0.3769 MEE: 0.7337     \n",
      "Epoch 01193: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1193/5000], Loss: 0.3505 MEE: 0.6772 \n",
      "Training - Epoch [1224/5000], Loss: 0.0779, MEE: 0.4219 | Test - Epoch [1224/5000], Loss: 0.3459 MEE: 0.6669 \n",
      " N. Epochs = 1224 - Loss (train | test)= (0.07787 | 0.3459) - MEE (train | test) = (0.4219200015068054 | 0.666903555393219)\n",
      "Epoch 00916: reducing learning rate of group 0 to 5.0000e-06. - Epoch [916/5000], Loss: 0.3439 MEE: 0.7244     \n",
      "Epoch 01171: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1171/5000], Loss: 0.2963 MEE: 0.6608 \n",
      "Training - Epoch [1202/5000], Loss: 0.0791, MEE: 0.4270 | Test - Epoch [1202/5000], Loss: 0.2954 MEE: 0.6511 \n",
      " N. Epochs = 1202 - Loss (train | test)= (0.07907 | 0.2954) - MEE (train | test) = (0.4269753396511078 | 0.6510735750198364)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.07675 +- 0.002485 | 0.3047 +- 0.03059)- MEE (train | test/val ) = ( 0.4214 +-  0.004778 | 0.6467 +- 0.01849)\n",
      "Training - Epoch [1034/5000], Loss: 0.0769, MEE: 0.4197 | Test - Epoch [1034/5000], Loss: 0.4106 MEE: 0.8006   \n",
      " N. Epochs = 1034 - Loss (train | test)= (0.07686 | 0.4106) - MEE (train | test) = (0.41966885328292847 | 0.8005740642547607)\n",
      "Epoch 00863: reducing learning rate of group 0 to 5.0000e-06. - Epoch [863/5000], Loss: 0.5198 MEE: 0.8628     \n",
      "Epoch 01090: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1090/5000], Loss: 0.4488 MEE: 0.8002 \n",
      "Training - Epoch [1122/5000], Loss: 0.0724, MEE: 0.3941 | Test - Epoch [1122/5000], Loss: 0.4441 MEE: 0.7938 \n",
      " N. Epochs = 1122 - Loss (train | test)= (0.07238 | 0.4441) - MEE (train | test) = (0.39407262206077576 | 0.7937964797019958)\n",
      "Training - Epoch [935/5000], Loss: 0.0827, MEE: 0.4282 | Test - Epoch [935/5000], Loss: 0.4565 MEE: 0.8259     \n",
      " N. Epochs = 935 - Loss (train | test)= (0.08272 | 0.4565) - MEE (train | test) = (0.42822957038879395 | 0.8259277939796448)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.07732 +- 0.004233 | 0.437 +- 0.01939)- MEE (train | test/val ) = ( 0.414 +-  0.01451 | 0.8068 +- 0.01383)\n",
      "Training - Epoch [1013/5000], Loss: 0.0894, MEE: 0.4241 | Test - Epoch [1013/5000], Loss: 0.3923 MEE: 0.8181   \n",
      " N. Epochs = 1013 - Loss (train | test)= (0.08939 | 0.3923) - MEE (train | test) = (0.4241066575050354 | 0.8181240558624268)\n",
      "Epoch 01087: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1087/5000], Loss: 0.3758 MEE: 0.8087    \n",
      "Epoch 01140: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1140/5000], Loss: 0.3697 MEE: 0.7890 \n",
      "Training - Epoch [1189/5000], Loss: 0.0627, MEE: 0.3671 | Test - Epoch [1189/5000], Loss: 0.3684 MEE: 0.7847 \n",
      " N. Epochs = 1189 - Loss (train | test)= (0.06265 | 0.3684) - MEE (train | test) = (0.36712905764579773 | 0.7847450375556946)\n",
      "Epoch 01094: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1094/5000], Loss: 0.3568 MEE: 0.7987   \n",
      "Epoch 01234: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1234/5000], Loss: 0.3465 MEE: 0.7676 \n",
      "Training - Epoch [1256/5000], Loss: 0.0558, MEE: 0.3507 | Test - Epoch [1256/5000], Loss: 0.3450 MEE: 0.7661 \n",
      " N. Epochs = 1256 - Loss (train | test)= (0.0558 | 0.345) - MEE (train | test) = (0.3507313132286072 | 0.7661241292953491)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.06928 +- 0.01449 | 0.3686 +- 0.01929)- MEE (train | test/val ) = ( 0.3807 +-  0.03145 | 0.7897 +- 0.02151)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.1, 0.3, 0.6] --> train_loss = 0.07445 +- 0.003662 | val_loss = 0.3701 +- 0.05405train_mee = 0.4053 +- 0.01772 | val_mee = 0.7477 +- 0.07175\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.2, 0.2, 0.6]\n",
      "Epoch 01118: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1118/5000], Loss: 0.2938 MEE: 0.6228   \n",
      "Epoch 01140: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1140/5000], Loss: 0.2891 MEE: 0.6141 \n",
      "Training - Epoch [1184/5000], Loss: 0.0577, MEE: 0.3690 | Test - Epoch [1184/5000], Loss: 0.2909 MEE: 0.6091 \n",
      " N. Epochs = 1184 - Loss (train | test)= (0.05768 | 0.2909) - MEE (train | test) = (0.3689654469490051 | 0.6091475486755371)\n",
      "Epoch 01065: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1065/5000], Loss: 0.2798 MEE: 0.6336   \n",
      "Training - Epoch [1182/5000], Loss: 0.0633, MEE: 0.3915 | Test - Epoch [1182/5000], Loss: 0.2728 MEE: 0.6209 \n",
      " N. Epochs = 1182 - Loss (train | test)= (0.06332 | 0.2728) - MEE (train | test) = (0.3914700150489807 | 0.6208881139755249)\n",
      "Epoch 01094: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1094/5000], Loss: 0.3040 MEE: 0.6244   \n",
      "Training - Epoch [1158/5000], Loss: 0.0600, MEE: 0.3777 | Test - Epoch [1158/5000], Loss: 0.2985 MEE: 0.6203 \n",
      " N. Epochs = 1158 - Loss (train | test)= (0.06003 | 0.2985) - MEE (train | test) = (0.37766724824905396 | 0.6202840805053711)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.06034 +- 0.002314 | 0.2874 +- 0.01081)- MEE (train | test/val ) = ( 0.3794 +-  0.009266 | 0.6168 +- 0.005398)\n",
      "Training - Epoch [827/5000], Loss: 0.1118, MEE: 0.4904 | Test - Epoch [827/5000], Loss: 0.4934 MEE: 0.8648     \n",
      " N. Epochs = 827 - Loss (train | test)= (0.1118 | 0.4934) - MEE (train | test) = (0.4904343783855438 | 0.8648257255554199)\n",
      "Epoch 00956: reducing learning rate of group 0 to 5.0000e-06. - Epoch [956/5000], Loss: 0.4299 MEE: 0.8259     \n",
      "Epoch 01100: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1100/5000], Loss: 0.4019 MEE: 0.7803 \n",
      "Epoch 01140: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1140/5000], Loss: 0.3978 MEE: 0.7735 \n",
      "Training - Epoch [1161/5000], Loss: 0.0612, MEE: 0.3769 | Test - Epoch [1161/5000], Loss: 0.3966 MEE: 0.7696 \n",
      " N. Epochs = 1161 - Loss (train | test)= (0.06116 | 0.3966) - MEE (train | test) = (0.37686923146247864 | 0.7695947289466858)\n",
      "Epoch 00859: reducing learning rate of group 0 to 5.0000e-06. - Epoch [859/5000], Loss: 0.4861 MEE: 0.8723     \n",
      "Epoch 00955: reducing learning rate of group 0 to 2.5000e-06. - Epoch [955/5000], Loss: 0.4539 MEE: 0.8405 \n",
      "Training - Epoch [996/5000], Loss: 0.0840, MEE: 0.4393 | Test - Epoch [996/5000], Loss: 0.4384 MEE: 0.8177 \n",
      " N. Epochs = 996 - Loss (train | test)= (0.08404 | 0.4384) - MEE (train | test) = (0.43929821252822876 | 0.8177172541618347)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.08567 +- 0.02071 | 0.4428 +- 0.03965)- MEE (train | test/val ) = ( 0.4355 +-  0.04644 | 0.8174 +- 0.03888)\n",
      "Training - Epoch [882/5000], Loss: 0.1120, MEE: 0.4701 | Test - Epoch [882/5000], Loss: 0.4003 MEE: 0.8422      \n",
      " N. Epochs = 882 - Loss (train | test)= (0.112 | 0.4003) - MEE (train | test) = (0.47014498710632324 | 0.8421845436096191)\n",
      "Epoch 00971: reducing learning rate of group 0 to 5.0000e-06. - Epoch [971/5000], Loss: 0.3660 MEE: 0.7966      \n",
      "Epoch 01005: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1005/5000], Loss: 0.3589 MEE: 0.7814 \n",
      "Training - Epoch [1120/5000], Loss: 0.0735, MEE: 0.3964 | Test - Epoch [1120/5000], Loss: 0.3488 MEE: 0.7645 \n",
      " N. Epochs = 1120 - Loss (train | test)= (0.07352 | 0.3488) - MEE (train | test) = (0.3963792324066162 | 0.764514684677124)\n",
      "Epoch 00886: reducing learning rate of group 0 to 5.0000e-06. - Epoch [886/5000], Loss: 0.3857 MEE: 0.8276     \n",
      "Training - Epoch [1216/5000], Loss: 0.0659, MEE: 0.3852 | Test - Epoch [1216/5000], Loss: 0.3284 MEE: 0.7575 \n",
      " N. Epochs = 1216 - Loss (train | test)= (0.06594 | 0.3284) - MEE (train | test) = (0.38524332642555237 | 0.7575458288192749)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.08381 +- 0.02016 | 0.3592 +- 0.03025)- MEE (train | test/val ) = ( 0.4173 +-  0.03767 | 0.7881 +- 0.03836)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.2, 0.2, 0.6] --> train_loss = 0.07661 +- 0.01153 | val_loss = 0.3631 +- 0.06351train_mee = 0.4107 +- 0.02339 | val_mee = 0.7407 +- 0.08847\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.05, 0.15, 0.8]\n",
      "Epoch 01404: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1404/5000], Loss: 0.3840 MEE: 0.8022   \n",
      "Training - Epoch [1539/5000], Loss: 0.1620, MEE: 0.6073 | Test - Epoch [1539/5000], Loss: 0.3574 MEE: 0.7685 \n",
      " N. Epochs = 1539 - Loss (train | test)= (0.162 | 0.3574) - MEE (train | test) = (0.6072893738746643 | 0.7685232758522034)\n",
      "Epoch 01432: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1432/5000], Loss: 0.3390 MEE: 0.7845   \n",
      "Epoch 01556: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1556/5000], Loss: 0.3186 MEE: 0.7673 \n",
      "Training - Epoch [1615/5000], Loss: 0.1448, MEE: 0.5837 | Test - Epoch [1615/5000], Loss: 0.3159 MEE: 0.7568 \n",
      " N. Epochs = 1615 - Loss (train | test)= (0.1448 | 0.3159) - MEE (train | test) = (0.583702802658081 | 0.7568235993385315)\n",
      "Epoch 01357: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1357/5000], Loss: 0.3783 MEE: 0.8267   \n",
      "Epoch 01539: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1539/5000], Loss: 0.3580 MEE: 0.8020 \n",
      "Epoch 01659: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1659/5000], Loss: 0.3514 MEE: 0.7864 \n",
      "Training - Epoch [1663/5000], Loss: 0.1581, MEE: 0.6042 | Test - Epoch [1663/5000], Loss: 0.3520 MEE: 0.7828 \n",
      " N. Epochs = 1663 - Loss (train | test)= (0.1581 | 0.352) - MEE (train | test) = (0.604241132736206 | 0.7828083634376526)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1549 +- 0.007366 | 0.3418 +- 0.01844)- MEE (train | test/val ) = ( 0.5984 +-  0.01047 | 0.7694 +- 0.01063)\n",
      "Epoch 01288: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1288/5000], Loss: 0.5192 MEE: 0.9952   \n",
      "Epoch 01345: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1345/5000], Loss: 0.5074 MEE: 0.9817 \n",
      "Epoch 01370: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1370/5000], Loss: 0.5070 MEE: 0.9779 \n",
      "Training - Epoch [1397/5000], Loss: 0.1779, MEE: 0.6435 | Test - Epoch [1397/5000], Loss: 0.5050 MEE: 0.9772 \n",
      " N. Epochs = 1397 - Loss (train | test)= (0.1779 | 0.505) - MEE (train | test) = (0.6434773802757263 | 0.9771791696548462)\n",
      "Training - Epoch [1419/5000], Loss: 0.1423, MEE: 0.5753 | Test - Epoch [1419/5000], Loss: 0.4827 MEE: 0.9304   \n",
      " N. Epochs = 1419 - Loss (train | test)= (0.1423 | 0.4827) - MEE (train | test) = (0.5752871632575989 | 0.9303653836250305)\n",
      "Epoch 01326: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1326/5000], Loss: 0.5332 MEE: 0.9729   \n",
      "Training - Epoch [1428/5000], Loss: 0.1449, MEE: 0.5783 | Test - Epoch [1428/5000], Loss: 0.5154 MEE: 0.9487 \n",
      " N. Epochs = 1428 - Loss (train | test)= (0.1449 | 0.5154) - MEE (train | test) = (0.5783326029777527 | 0.948724091053009)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1551 +- 0.01621 | 0.501 +- 0.01366)- MEE (train | test/val ) = ( 0.599 +-  0.03145 | 0.9521 +- 0.01926)\n",
      "Epoch 01017: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1017/5000], Loss: 0.5081 MEE: 1.0306   \n",
      "Epoch 01170: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1170/5000], Loss: 0.4730 MEE: 0.9906 \n",
      "Training - Epoch [1221/5000], Loss: 0.2160, MEE: 0.6885 | Test - Epoch [1221/5000], Loss: 0.4703 MEE: 0.9846 \n",
      " N. Epochs = 1221 - Loss (train | test)= (0.216 | 0.4703) - MEE (train | test) = (0.6884600520133972 | 0.9845800399780273)\n",
      "Epoch 01294: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1294/5000], Loss: 0.4550 MEE: 0.9500   \n",
      "Epoch 01474: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1474/5000], Loss: 0.4289 MEE: 0.9150 \n",
      "Epoch 01519: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1519/5000], Loss: 0.4283 MEE: 0.9135 \n",
      "Training - Epoch [1523/5000], Loss: 0.1529, MEE: 0.5849 | Test - Epoch [1523/5000], Loss: 0.4258 MEE: 0.9088 \n",
      " N. Epochs = 1523 - Loss (train | test)= (0.1529 | 0.4258) - MEE (train | test) = (0.5849114060401917 | 0.9087681770324707)\n",
      "Epoch 01348: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1348/5000], Loss: 0.4260 MEE: 0.9284   \n",
      "Epoch 01398: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1398/5000], Loss: 0.4184 MEE: 0.9173 \n",
      "Training - Epoch [1402/5000], Loss: 0.1704, MEE: 0.6155 | Test - Epoch [1402/5000], Loss: 0.4173 MEE: 0.9162 \n",
      " N. Epochs = 1402 - Loss (train | test)= (0.1704 | 0.4173) - MEE (train | test) = (0.6154793500900269 | 0.9161616563796997)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.1798 +- 0.02658 | 0.4378 +- 0.02324)- MEE (train | test/val ) = ( 0.6296 +-  0.04344 | 0.9365 +- 0.03413)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.05, 0.15, 0.8] --> train_loss = 0.1633 +- 0.01168 | val_loss = 0.4269 +- 0.06549train_mee = 0.609 +- 0.01457 | val_mee = 0.886 +- 0.0827\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.1, 0.4, 0.5]\n",
      "Epoch 00885: reducing learning rate of group 0 to 5.0000e-06. - Epoch [885/5000], Loss: 0.3441 MEE: 0.6747       \n",
      "Epoch 01021: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1021/5000], Loss: 0.3075 MEE: 0.6204 \n",
      "Training - Epoch [1046/5000], Loss: 0.0636, MEE: 0.3730 | Test - Epoch [1046/5000], Loss: 0.3047 MEE: 0.6231 \n",
      " N. Epochs = 1046 - Loss (train | test)= (0.0636 | 0.3047) - MEE (train | test) = (0.37297752499580383 | 0.6231434345245361)\n",
      "Training - Epoch [991/5000], Loss: 0.0699, MEE: 0.3891 | Test - Epoch [991/5000], Loss: 0.2866 MEE: 0.6396       \n",
      " N. Epochs = 991 - Loss (train | test)= (0.06994 | 0.2866) - MEE (train | test) = (0.38914719223976135 | 0.6395881175994873)\n",
      "Training - Epoch [1084/5000], Loss: 0.0635, MEE: 0.3806 | Test - Epoch [1084/5000], Loss: 0.2872 MEE: 0.6325     \n",
      " N. Epochs = 1084 - Loss (train | test)= (0.06354 | 0.2872) - MEE (train | test) = (0.3805559575557709 | 0.6325090527534485)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.06569 +- 0.003001 | 0.2928 +- 0.008398)- MEE (train | test/val ) = ( 0.3809 +-  0.006606 | 0.6317 +- 0.006735)\n",
      "Epoch 01042: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1042/5000], Loss: 0.5958 MEE: 0.8449     \n",
      "Epoch 01067: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1067/5000], Loss: 0.5894 MEE: 0.8333 \n",
      "Training - Epoch [1098/5000], Loss: 0.0447, MEE: 0.3097 | Test - Epoch [1098/5000], Loss: 0.5900 MEE: 0.8295 \n",
      " N. Epochs = 1098 - Loss (train | test)= (0.04472 | 0.59) - MEE (train | test) = (0.30972185730934143 | 0.8294672966003418)\n",
      "Epoch 00964: reducing learning rate of group 0 to 5.0000e-06. - Epoch [964/5000], Loss: 0.5780 MEE: 0.8512       \n",
      "Epoch 01021: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1021/5000], Loss: 0.5551 MEE: 0.8248 \n",
      "Epoch 01076: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1076/5000], Loss: 0.5520 MEE: 0.8187 \n",
      "Training - Epoch [1116/5000], Loss: 0.0554, MEE: 0.3338 | Test - Epoch [1116/5000], Loss: 0.5501 MEE: 0.8172 \n",
      " N. Epochs = 1116 - Loss (train | test)= (0.05545 | 0.5501) - MEE (train | test) = (0.333767831325531 | 0.8172088861465454)\n",
      "Epoch 00960: reducing learning rate of group 0 to 5.0000e-06. - Epoch [960/5000], Loss: 0.4066 MEE: 0.7676       \n",
      "Epoch 01013: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1013/5000], Loss: 0.3990 MEE: 0.7619 \n",
      "Epoch 01045: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1045/5000], Loss: 0.3969 MEE: 0.7639 \n",
      "Training - Epoch [1091/5000], Loss: 0.0488, MEE: 0.3290 | Test - Epoch [1091/5000], Loss: 0.3917 MEE: 0.7552 \n",
      " N. Epochs = 1091 - Loss (train | test)= (0.04878 | 0.3917) - MEE (train | test) = (0.3289814293384552 | 0.7551897168159485)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.04965 +- 0.004422 | 0.5106 +- 0.08565)- MEE (train | test/val ) = ( 0.3242 +-  0.01039 | 0.8006 +- 0.03251)\n",
      "Training - Epoch [940/5000], Loss: 0.0868, MEE: 0.3983 | Test - Epoch [940/5000], Loss: 0.3567 MEE: 0.8026       \n",
      " N. Epochs = 940 - Loss (train | test)= (0.08678 | 0.3567) - MEE (train | test) = (0.39832696318626404 | 0.8026337623596191)\n",
      "Epoch 01092: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1092/5000], Loss: 0.3873 MEE: 0.7920     \n",
      "Epoch 01128: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1128/5000], Loss: 0.3833 MEE: 0.7794 \n",
      "Training - Epoch [1132/5000], Loss: 0.0508, MEE: 0.3107 | Test - Epoch [1132/5000], Loss: 0.3787 MEE: 0.7765 \n",
      " N. Epochs = 1132 - Loss (train | test)= (0.0508 | 0.3787) - MEE (train | test) = (0.31071528792381287 | 0.776452898979187)\n",
      "Epoch 00839: reducing learning rate of group 0 to 5.0000e-06. - Epoch [839/5000], Loss: 0.4105 MEE: 0.8560       \n",
      "Epoch 01197: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1197/5000], Loss: 0.3323 MEE: 0.7510 \n",
      "Training - Epoch [1233/5000], Loss: 0.0586, MEE: 0.3409 | Test - Epoch [1233/5000], Loss: 0.3312 MEE: 0.7477 \n",
      " N. Epochs = 1233 - Loss (train | test)= (0.05865 | 0.3312) - MEE (train | test) = (0.34092259407043457 | 0.7477415800094604)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.06541 +- 0.01545 | 0.3555 +- 0.01941)- MEE (train | test/val ) = ( 0.35 +-  0.03634 | 0.7756 +- 0.02242)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.1, 0.4, 0.5] --> train_loss = 0.06025 +- 0.007496 | val_loss = 0.3863 +- 0.09153train_mee = 0.3517 +- 0.02319 | val_mee = 0.736 +- 0.07442\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.25, 0.25, 0.5]\n",
      "Epoch 01018: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1018/5000], Loss: 0.2455 MEE: 0.5845     \n",
      "Training - Epoch [1057/5000], Loss: 0.0406, MEE: 0.3084 | Test - Epoch [1057/5000], Loss: 0.2387 MEE: 0.5598 \n",
      " N. Epochs = 1057 - Loss (train | test)= (0.04062 | 0.2387) - MEE (train | test) = (0.30836597084999084 | 0.5597898364067078)\n",
      "Epoch 01009: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1009/5000], Loss: 0.2444 MEE: 0.5798     \n",
      "Epoch 01058: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1058/5000], Loss: 0.2358 MEE: 0.5596 \n",
      "Training - Epoch [1142/5000], Loss: 0.0388, MEE: 0.3012 | Test - Epoch [1142/5000], Loss: 0.2335 MEE: 0.5477 \n",
      " N. Epochs = 1142 - Loss (train | test)= (0.03877 | 0.2335) - MEE (train | test) = (0.30121031403541565 | 0.5477281212806702)\n",
      "Epoch 01053: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1053/5000], Loss: 0.2466 MEE: 0.5631     \n",
      "Epoch 01082: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1082/5000], Loss: 0.2378 MEE: 0.5431 \n",
      "Training - Epoch [1108/5000], Loss: 0.0399, MEE: 0.3022 | Test - Epoch [1108/5000], Loss: 0.2363 MEE: 0.5413 \n",
      " N. Epochs = 1108 - Loss (train | test)= (0.03989 | 0.2363) - MEE (train | test) = (0.3022107481956482 | 0.5412984490394592)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.03976 +- 0.0007615 | 0.2362 +- 0.00215)- MEE (train | test/val ) = ( 0.3039 +-  0.003164 | 0.5496 +- 0.007665)\n",
      "Epoch 00822: reducing learning rate of group 0 to 5.0000e-06. - Epoch [822/5000], Loss: 0.5104 MEE: 0.8192       \n",
      "Epoch 00846: reducing learning rate of group 0 to 2.5000e-06. - Epoch [846/5000], Loss: 0.5041 MEE: 0.8083 \n",
      "Training - Epoch [1036/5000], Loss: 0.0518, MEE: 0.3401 | Test - Epoch [1036/5000], Loss: 0.4848 MEE: 0.7864 \n",
      " N. Epochs = 1036 - Loss (train | test)= (0.05178 | 0.4848) - MEE (train | test) = (0.3401311933994293 | 0.7863975763320923)\n",
      "Epoch 01070: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1070/5000], Loss: 0.4243 MEE: 0.7458     \n",
      "Epoch 01096: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1096/5000], Loss: 0.4315 MEE: 0.7524 \n",
      "Epoch 01170: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1170/5000], Loss: 0.4234 MEE: 0.7384 \n",
      "Training - Epoch [1174/5000], Loss: 0.0349, MEE: 0.2874 | Test - Epoch [1174/5000], Loss: 0.4210 MEE: 0.7359 \n",
      " N. Epochs = 1174 - Loss (train | test)= (0.0349 | 0.421) - MEE (train | test) = (0.287426233291626 | 0.7358647584915161)\n",
      "Epoch 00872: reducing learning rate of group 0 to 5.0000e-06. - Epoch [872/5000], Loss: 0.4605 MEE: 0.7957       \n",
      "Training - Epoch [998/5000], Loss: 0.0524, MEE: 0.3438 | Test - Epoch [998/5000], Loss: 0.4287 MEE: 0.7779 \n",
      " N. Epochs = 998 - Loss (train | test)= (0.05239 | 0.4287) - MEE (train | test) = (0.34383440017700195 | 0.7778794169425964)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.04636 +- 0.008105 | 0.4448 +- 0.02843)- MEE (train | test/val ) = ( 0.3238 +-  0.02576 | 0.7667 +- 0.02209)\n",
      "Epoch 00987: reducing learning rate of group 0 to 5.0000e-06. - Epoch [987/5000], Loss: 0.3622 MEE: 0.7651       \n",
      "Epoch 01016: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1016/5000], Loss: 0.3601 MEE: 0.7540 \n",
      "Training - Epoch [1064/5000], Loss: 0.0430, MEE: 0.3051 | Test - Epoch [1064/5000], Loss: 0.3525 MEE: 0.7444 \n",
      " N. Epochs = 1064 - Loss (train | test)= (0.043 | 0.3525) - MEE (train | test) = (0.305122435092926 | 0.7444003820419312)\n",
      "Epoch 00970: reducing learning rate of group 0 to 5.0000e-06. - Epoch [970/5000], Loss: 0.3878 MEE: 0.7791       \n",
      "Training - Epoch [1028/5000], Loss: 0.0539, MEE: 0.3345 | Test - Epoch [1028/5000], Loss: 0.3748 MEE: 0.7723 \n",
      " N. Epochs = 1028 - Loss (train | test)= (0.05389 | 0.3748) - MEE (train | test) = (0.33453822135925293 | 0.7722905278205872)\n",
      "Training - Epoch [885/5000], Loss: 0.0777, MEE: 0.3840 | Test - Epoch [885/5000], Loss: 0.4004 MEE: 0.8097       \n",
      " N. Epochs = 885 - Loss (train | test)= (0.07767 | 0.4004) - MEE (train | test) = (0.38395383954048157 | 0.8097056746482849)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.05819 +- 0.01448 | 0.3759 +- 0.01956)- MEE (train | test/val ) = ( 0.3412 +-  0.03253 | 0.7755 +- 0.02676)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.25, 0.25, 0.5] --> train_loss = 0.0481 +- 0.007623 | val_loss = 0.3523 +- 0.08681train_mee = 0.323 +- 0.01523 | val_mee = 0.6973 +- 0.1045\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.2, 0.3, 0.5]\n",
      "Epoch 01094: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1094/5000], Loss: 0.2953 MEE: 0.6147     \n",
      "Training - Epoch [1098/5000], Loss: 0.0503, MEE: 0.3364 | Test - Epoch [1098/5000], Loss: 0.2852 MEE: 0.5948 \n",
      " N. Epochs = 1098 - Loss (train | test)= (0.05031 | 0.2852) - MEE (train | test) = (0.33640968799591064 | 0.5948197245597839)\n",
      "Epoch 01040: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1040/5000], Loss: 0.2448 MEE: 0.6137     \n",
      "Epoch 01137: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1137/5000], Loss: 0.2330 MEE: 0.5888 \n",
      "Training - Epoch [1191/5000], Loss: 0.0431, MEE: 0.3131 | Test - Epoch [1191/5000], Loss: 0.2290 MEE: 0.5755 \n",
      " N. Epochs = 1191 - Loss (train | test)= (0.04315 | 0.229) - MEE (train | test) = (0.31313544511795044 | 0.5754586458206177)\n",
      "Training - Epoch [1009/5000], Loss: 0.0561, MEE: 0.3503 | Test - Epoch [1009/5000], Loss: 0.3068 MEE: 0.6222     \n",
      " N. Epochs = 1009 - Loss (train | test)= (0.05607 | 0.3068) - MEE (train | test) = (0.3502744734287262 | 0.6222019791603088)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.04984 +- 0.005286 | 0.2737 +- 0.03276)- MEE (train | test/val ) = ( 0.3333 +-  0.01532 | 0.5975 +- 0.01918)\n",
      "Training - Epoch [899/5000], Loss: 0.0527, MEE: 0.3514 | Test - Epoch [899/5000], Loss: 0.4123 MEE: 0.7750       \n",
      " N. Epochs = 899 - Loss (train | test)= (0.05272 | 0.4123) - MEE (train | test) = (0.3514446020126343 | 0.774953305721283)\n",
      "Epoch 00877: reducing learning rate of group 0 to 5.0000e-06. - Epoch [877/5000], Loss: 0.5170 MEE: 0.8282       \n",
      "Training - Epoch [1099/5000], Loss: 0.0449, MEE: 0.3189 | Test - Epoch [1099/5000], Loss: 0.4501 MEE: 0.7700 \n",
      " N. Epochs = 1099 - Loss (train | test)= (0.04495 | 0.4501) - MEE (train | test) = (0.3189059793949127 | 0.7699534893035889)\n",
      "Epoch 00925: reducing learning rate of group 0 to 5.0000e-06. - Epoch [925/5000], Loss: 0.4348 MEE: 0.7950       \n",
      "Training - Epoch [1063/5000], Loss: 0.0463, MEE: 0.3267 | Test - Epoch [1063/5000], Loss: 0.4082 MEE: 0.7631 \n",
      " N. Epochs = 1063 - Loss (train | test)= (0.0463 | 0.4082) - MEE (train | test) = (0.32666924595832825 | 0.7630674839019775)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.04799 +- 0.00339 | 0.4236 +- 0.01887)- MEE (train | test/val ) = ( 0.3323 +-  0.01388 | 0.7693 +- 0.004873)\n",
      "Epoch 00895: reducing learning rate of group 0 to 5.0000e-06. - Epoch [895/5000], Loss: 0.3907 MEE: 0.8003       \n",
      "Training - Epoch [1059/5000], Loss: 0.0544, MEE: 0.3253 | Test - Epoch [1059/5000], Loss: 0.3530 MEE: 0.7547 \n",
      " N. Epochs = 1059 - Loss (train | test)= (0.05436 | 0.353) - MEE (train | test) = (0.3253011703491211 | 0.7547079920768738)\n",
      "Training - Epoch [907/5000], Loss: 0.0647, MEE: 0.3582 | Test - Epoch [907/5000], Loss: 0.3254 MEE: 0.7421       \n",
      " N. Epochs = 907 - Loss (train | test)= (0.06468 | 0.3254) - MEE (train | test) = (0.35822629928588867 | 0.7421267032623291)\n",
      "Epoch 00958: reducing learning rate of group 0 to 5.0000e-06. - Epoch [958/5000], Loss: 0.3544 MEE: 0.7744       \n",
      "Epoch 00982: reducing learning rate of group 0 to 2.5000e-06. - Epoch [982/5000], Loss: 0.3387 MEE: 0.7422 \n",
      "Epoch 01031: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1031/5000], Loss: 0.3316 MEE: 0.7344 \n",
      "Epoch 01056: reducing learning rate of group 0 to 6.2500e-07.t - Epoch [1056/5000], Loss: 0.3333 MEE: 0.7337 \n",
      "Training - Epoch [1057/5000], Loss: 0.0470, MEE: 0.3101 | Test - Epoch [1057/5000], Loss: 0.3326 MEE: 0.7327 \n",
      " N. Epochs = 1057 - Loss (train | test)= (0.04699 | 0.3326) - MEE (train | test) = (0.3101467192173004 | 0.732659101486206)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.05534 +- 0.007258 | 0.337 +- 0.01169)- MEE (train | test/val ) = ( 0.3312 +-  0.02007 | 0.7432 +- 0.009031)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.2, 0.3, 0.5] --> train_loss = 0.05106 +- 0.003123 | val_loss = 0.3447 +- 0.06144train_mee = 0.3323 +- 0.0008374 | val_mee = 0.7033 +- 0.07559\n",
      "[1000, 1e-05, 0.9, 128, 0.001, Tanh(), [0.25, 0.25, 0.5]]\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [1e-5]\n",
    "momentums = [0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "#proportionss = [[0.1,0.8,0.1],[0.2,0.6,0.2],[0.25,0.50,0.25],[0.2,0.7,0.1],[0.1,0.7,0.2],[0.3,0.6,0.1]]\n",
    "#proportionss = [[0.8,0.1,0.1],[0.7,0.2,0.1],[0.6,0.3,0.1],[0.6,0.2,0.2],[0.8,0.15,0.05],[0.5,0.4,0.1],[0.5,0.25,0.25], [0.5,0.3,0.2]]\n",
    "proportionss = [[0.1,0.1,0.8],[0.1,0.2,0.7],[0.1,0.3,0.6],[0.2,0.2,0.6],[0.05,0.15,0.8],[0.1,0.4,0.5],[0.25,0.25,0.5], [0.2,0.3,0.5]]\n",
    "#proportionss = [[0.2,0.3,0.5],[0.25,0.25,0.5],[0.15,0.35,0.5],[0.15,0.3,0.55],[0.1,0.25,0.55],[0.05,0.40,0.55],[0.25,0.35,0.4]]\n",
    "optimiz = 'RMS'\n",
    "best_hp = perform_grid_search_kfold(hidden_neurons,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    activations,\n",
    "                                    optimiz,\n",
    "                                    proportionss,\n",
    "                                    k_folds=3,\n",
    "                                    x=X_train,\n",
    "                                    y=y_train,\n",
    "                                    num_epochs=5000,\n",
    "                                    plot_curves=False,\n",
    "                                    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.2, 0.3, 0.5]\n",
      "Epoch 01147: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1147/5000], Loss: 0.2453 MEE: 0.5766     \n",
      "Epoch 01213: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1213/5000], Loss: 0.2356 MEE: 0.5576 \n",
      "Training - Epoch [1253/5000], Loss: 0.0339, MEE: 0.2825 | Test - Epoch [1253/5000], Loss: 0.2341 MEE: 0.5553 \n",
      " N. Epochs = 1253 - Loss (train | test)= (0.03392 | 0.2341) - MEE (train | test) = (0.28251415491104126 | 0.5553089380264282)\n",
      "Epoch 01079: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1079/5000], Loss: 0.2413 MEE: 0.5913     \n",
      "Training - Epoch [1140/5000], Loss: 0.0435, MEE: 0.3185 | Test - Epoch [1140/5000], Loss: 0.2296 MEE: 0.5736 \n",
      " N. Epochs = 1140 - Loss (train | test)= (0.04352 | 0.2296) - MEE (train | test) = (0.318450391292572 | 0.5736222267150879)\n",
      "Epoch 00946: reducing learning rate of group 0 to 5.0000e-06. - Epoch [946/5000], Loss: 0.2510 MEE: 0.6017       \n",
      "Training - Epoch [1121/5000], Loss: 0.0462, MEE: 0.3284 | Test - Epoch [1121/5000], Loss: 0.2322 MEE: 0.5766 \n",
      " N. Epochs = 1121 - Loss (train | test)= (0.04618 | 0.2322) - MEE (train | test) = (0.3283715546131134 | 0.576648473739624)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.04121 +- 0.005265 | 0.232 +- 0.001837)- MEE (train | test/val ) = ( 0.3098 +-  0.0197 | 0.5685 +- 0.009428)\n",
      "Epoch 00812: reducing learning rate of group 0 to 5.0000e-06. - Epoch [812/5000], Loss: 0.5126 MEE: 0.8408       \n",
      "Epoch 01085: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1085/5000], Loss: 0.4474 MEE: 0.7658 \n",
      "Training - Epoch [1190/5000], Loss: 0.0450, MEE: 0.3152 | Test - Epoch [1190/5000], Loss: 0.4449 MEE: 0.7633 \n",
      " N. Epochs = 1190 - Loss (train | test)= (0.04496 | 0.4449) - MEE (train | test) = (0.31517598032951355 | 0.7632999420166016)\n",
      "Epoch 00977: reducing learning rate of group 0 to 5.0000e-06. - Epoch [977/5000], Loss: 0.4565 MEE: 0.7803       \n",
      "Epoch 01012: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1012/5000], Loss: 0.4569 MEE: 0.7760 \n",
      "Epoch 01082: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1082/5000], Loss: 0.4473 MEE: 0.7628 \n",
      "Epoch 01118: reducing learning rate of group 0 to 6.2500e-07.t - Epoch [1118/5000], Loss: 0.4469 MEE: 0.7603 \n",
      "Training - Epoch [1122/5000], Loss: 0.0397, MEE: 0.3012 | Test - Epoch [1122/5000], Loss: 0.4475 MEE: 0.7593 \n",
      " N. Epochs = 1122 - Loss (train | test)= (0.03967 | 0.4475) - MEE (train | test) = (0.3012278974056244 | 0.7593012452125549)\n",
      "Epoch 00821: reducing learning rate of group 0 to 5.0000e-06. - Epoch [821/5000], Loss: 0.4892 MEE: 0.8358       \n",
      "Training - Epoch [866/5000], Loss: 0.0618, MEE: 0.3694 | Test - Epoch [866/5000], Loss: 0.4631 MEE: 0.7954 \n",
      " N. Epochs = 866 - Loss (train | test)= (0.06184 | 0.4631) - MEE (train | test) = (0.3693764805793762 | 0.7954418659210205)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.04882 +- 0.009453 | 0.4518 +- 0.008055)- MEE (train | test/val ) = ( 0.3286 +-  0.02939 | 0.7727 +- 0.01618)\n",
      "Epoch 00707: reducing learning rate of group 0 to 5.0000e-06. - Epoch [707/5000], Loss: 0.5102 MEE: 0.9315       \n",
      "Epoch 00940: reducing learning rate of group 0 to 2.5000e-06. - Epoch [940/5000], Loss: 0.4274 MEE: 0.8403 \n",
      "Epoch 01134: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1134/5000], Loss: 0.4066 MEE: 0.8059 \n",
      "Training - Epoch [1186/5000], Loss: 0.0689, MEE: 0.3637 | Test - Epoch [1186/5000], Loss: 0.4000 MEE: 0.7977 \n",
      " N. Epochs = 1186 - Loss (train | test)= (0.06886 | 0.4) - MEE (train | test) = (0.36368656158447266 | 0.7977185845375061)\n",
      "Epoch 00865: reducing learning rate of group 0 to 5.0000e-06. - Epoch [865/5000], Loss: 0.3635 MEE: 0.7777       \n",
      "Epoch 01128: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1128/5000], Loss: 0.3247 MEE: 0.7339 \n",
      "Training - Epoch [1185/5000], Loss: 0.0503, MEE: 0.3148 | Test - Epoch [1185/5000], Loss: 0.3161 MEE: 0.7231 \n",
      " N. Epochs = 1185 - Loss (train | test)= (0.0503 | 0.3161) - MEE (train | test) = (0.31482967734336853 | 0.7230513691902161)\n",
      "Epoch 00933: reducing learning rate of group 0 to 5.0000e-06. - Epoch [933/5000], Loss: 0.3644 MEE: 0.7794       \n",
      "Epoch 01138: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1138/5000], Loss: 0.3149 MEE: 0.7220 \n",
      "Training - Epoch [1139/5000], Loss: 0.0532, MEE: 0.3204 | Test - Epoch [1139/5000], Loss: 0.3179 MEE: 0.7231 \n",
      " N. Epochs = 1139 - Loss (train | test)= (0.05318 | 0.3179) - MEE (train | test) = (0.3204471468925476 | 0.7231183648109436)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.05744 +- 0.008154 | 0.3447 +- 0.03914)- MEE (train | test/val ) = ( 0.333 +-  0.02183 | 0.748 +- 0.03518)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.2, 0.3, 0.5] --> train_loss = 0.04916 +- 0.006633 | val_loss = 0.3428 +- 0.08976train_mee = 0.3238 +- 0.01007 | val_mee = 0.6964 +- 0.09097\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.25, 0.25, 0.5]\n",
      "Epoch 01039: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1039/5000], Loss: 0.2421 MEE: 0.5740     \n",
      "Epoch 01074: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1074/5000], Loss: 0.2396 MEE: 0.5701 \n",
      "Training - Epoch [1104/5000], Loss: 0.0385, MEE: 0.2991 | Test - Epoch [1104/5000], Loss: 0.2372 MEE: 0.5678 \n",
      " N. Epochs = 1104 - Loss (train | test)= (0.03854 | 0.2372) - MEE (train | test) = (0.2991240918636322 | 0.5677748918533325)\n",
      "Epoch 00937: reducing learning rate of group 0 to 5.0000e-06. - Epoch [937/5000], Loss: 0.3092 MEE: 0.6249       \n",
      "Training - Epoch [1047/5000], Loss: 0.0465, MEE: 0.3283 | Test - Epoch [1047/5000], Loss: 0.2932 MEE: 0.5973 \n",
      " N. Epochs = 1047 - Loss (train | test)= (0.04651 | 0.2932) - MEE (train | test) = (0.3283381760120392 | 0.5972905158996582)\n",
      "Training - Epoch [910/5000], Loss: 0.0666, MEE: 0.3928 | Test - Epoch [910/5000], Loss: 0.2982 MEE: 0.6196       \n",
      " N. Epochs = 910 - Loss (train | test)= (0.06658 | 0.2982) - MEE (train | test) = (0.3928294777870178 | 0.6195551753044128)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.05055 +- 0.0118 | 0.2762 +- 0.02764)- MEE (train | test/val ) = ( 0.3401 +-  0.03915 | 0.5949 +- 0.02121)\n",
      "Epoch 01045: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1045/5000], Loss: 0.4042 MEE: 0.7455     \n",
      "Training - Epoch [1049/5000], Loss: 0.0417, MEE: 0.3148 | Test - Epoch [1049/5000], Loss: 0.4056 MEE: 0.7433 \n",
      " N. Epochs = 1049 - Loss (train | test)= (0.04165 | 0.4056) - MEE (train | test) = (0.3147977590560913 | 0.7433152794837952)\n",
      "Training - Epoch [922/5000], Loss: 0.0630, MEE: 0.3818 | Test - Epoch [922/5000], Loss: 0.4275 MEE: 0.7887       \n",
      " N. Epochs = 922 - Loss (train | test)= (0.06298 | 0.4275) - MEE (train | test) = (0.38183078169822693 | 0.788665235042572)\n",
      "Epoch 00912: reducing learning rate of group 0 to 5.0000e-06. - Epoch [912/5000], Loss: 0.4546 MEE: 0.7873       \n",
      "Epoch 00943: reducing learning rate of group 0 to 2.5000e-06. - Epoch [943/5000], Loss: 0.4425 MEE: 0.7678 \n",
      "Training - Epoch [1033/5000], Loss: 0.0463, MEE: 0.3248 | Test - Epoch [1033/5000], Loss: 0.4397 MEE: 0.7594 \n",
      " N. Epochs = 1033 - Loss (train | test)= (0.04634 | 0.4397) - MEE (train | test) = (0.3248027563095093 | 0.7593966722488403)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.05033 +- 0.009153 | 0.4242 +- 0.0141)- MEE (train | test/val ) = ( 0.3405 +-  0.02953 | 0.7638 +- 0.01877)\n",
      "Epoch 00952: reducing learning rate of group 0 to 5.0000e-06. - Epoch [952/5000], Loss: 0.3659 MEE: 0.7816       \n",
      "Epoch 00984: reducing learning rate of group 0 to 2.5000e-06. - Epoch [984/5000], Loss: 0.3494 MEE: 0.7541 \n",
      "Training - Epoch [987/5000], Loss: 0.0581, MEE: 0.3335 | Test - Epoch [987/5000], Loss: 0.3541 MEE: 0.7560 \n",
      " N. Epochs = 987 - Loss (train | test)= (0.05813 | 0.3541) - MEE (train | test) = (0.3335343897342682 | 0.755980372428894)\n",
      "Epoch 00913: reducing learning rate of group 0 to 5.0000e-06. - Epoch [913/5000], Loss: 0.3335 MEE: 0.7562       \n",
      "Training - Epoch [941/5000], Loss: 0.0579, MEE: 0.3401 | Test - Epoch [941/5000], Loss: 0.3326 MEE: 0.7509 \n",
      " N. Epochs = 941 - Loss (train | test)= (0.05791 | 0.3326) - MEE (train | test) = (0.3401024043560028 | 0.7508642673492432)\n",
      "Training - Epoch [976/5000], Loss: 0.0591, MEE: 0.3663 | Test - Epoch [976/5000], Loss: 0.3359 MEE: 0.7565       \n",
      " N. Epochs = 976 - Loss (train | test)= (0.05913 | 0.3359) - MEE (train | test) = (0.36626148223876953 | 0.7565287351608276)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.05839 +- 0.0005317 | 0.3408 +- 0.009439)- MEE (train | test/val ) = ( 0.3466 +-  0.01414 | 0.7545 +- 0.002551)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.25, 0.25, 0.5] --> train_loss = 0.05309 +- 0.003751 | val_loss = 0.3471 +- 0.0606train_mee = 0.3424 +- 0.002995 | val_mee = 0.7044 +- 0.07752\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.15, 0.35, 0.5]\n",
      "Training - Epoch [919/5000], Loss: 0.0799, MEE: 0.4106 | Test - Epoch [919/5000], Loss: 0.3363 MEE: 0.6617       \n",
      " N. Epochs = 919 - Loss (train | test)= (0.07994 | 0.3363) - MEE (train | test) = (0.41058769822120667 | 0.6617055535316467)\n",
      "Epoch 00949: reducing learning rate of group 0 to 5.0000e-06. - Epoch [949/5000], Loss: 0.2988 MEE: 0.6610       \n",
      "Training - Epoch [1099/5000], Loss: 0.0565, MEE: 0.3576 | Test - Epoch [1099/5000], Loss: 0.2742 MEE: 0.6091 \n",
      " N. Epochs = 1099 - Loss (train | test)= (0.0565 | 0.2742) - MEE (train | test) = (0.35755595564842224 | 0.6090950965881348)\n",
      "Epoch 01012: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1012/5000], Loss: 0.3206 MEE: 0.6374     \n",
      "Epoch 01100: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1100/5000], Loss: 0.3081 MEE: 0.6170 \n",
      "Training - Epoch [1135/5000], Loss: 0.0490, MEE: 0.3287 | Test - Epoch [1135/5000], Loss: 0.3085 MEE: 0.6081 \n",
      " N. Epochs = 1135 - Loss (train | test)= (0.04903 | 0.3085) - MEE (train | test) = (0.3286806643009186 | 0.608112096786499)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.06183 +- 0.01317 | 0.3063 +- 0.02542)- MEE (train | test/val ) = ( 0.3656 +-  0.03392 | 0.6263 +- 0.02504)\n",
      "Epoch 00847: reducing learning rate of group 0 to 5.0000e-06. - Epoch [847/5000], Loss: 0.4475 MEE: 0.7831       \n",
      "Training - Epoch [953/5000], Loss: 0.0555, MEE: 0.3405 | Test - Epoch [953/5000], Loss: 0.4158 MEE: 0.7611 \n",
      " N. Epochs = 953 - Loss (train | test)= (0.05548 | 0.4158) - MEE (train | test) = (0.3404831290245056 | 0.761138916015625)\n",
      "Epoch 00937: reducing learning rate of group 0 to 5.0000e-06. - Epoch [937/5000], Loss: 0.4625 MEE: 0.8074       \n",
      "Training - Epoch [1170/5000], Loss: 0.0424, MEE: 0.3085 | Test - Epoch [1170/5000], Loss: 0.4228 MEE: 0.7614 \n",
      " N. Epochs = 1170 - Loss (train | test)= (0.04239 | 0.4228) - MEE (train | test) = (0.3085474967956543 | 0.76141357421875)\n",
      "Training - Epoch [1044/5000], Loss: 0.0468, MEE: 0.3271 | Test - Epoch [1044/5000], Loss: 0.4460 MEE: 0.7751     \n",
      " N. Epochs = 1044 - Loss (train | test)= (0.0468 | 0.446) - MEE (train | test) = (0.32713887095451355 | 0.7751353979110718)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.04822 +- 0.005437 | 0.4282 +- 0.01289)- MEE (train | test/val ) = ( 0.3254 +-  0.0131 | 0.7659 +- 0.006534)\n",
      "Epoch 00930: reducing learning rate of group 0 to 5.0000e-06. - Epoch [930/5000], Loss: 0.4133 MEE: 0.8158       \n",
      "Epoch 00968: reducing learning rate of group 0 to 2.5000e-06. - Epoch [968/5000], Loss: 0.4119 MEE: 0.7969 \n",
      "Epoch 00992: reducing learning rate of group 0 to 1.2500e-06. - Epoch [992/5000], Loss: 0.4108 MEE: 0.7951 \n",
      "Training - Epoch [1039/5000], Loss: 0.0606, MEE: 0.3385 | Test - Epoch [1039/5000], Loss: 0.4021 MEE: 0.7871 \n",
      " N. Epochs = 1039 - Loss (train | test)= (0.06064 | 0.4021) - MEE (train | test) = (0.33850395679473877 | 0.7870960235595703)\n",
      "Training - Epoch [1057/5000], Loss: 0.0538, MEE: 0.3355 | Test - Epoch [1057/5000], Loss: 0.3407 MEE: 0.7675     \n",
      " N. Epochs = 1057 - Loss (train | test)= (0.05383 | 0.3407) - MEE (train | test) = (0.3355157673358917 | 0.7675370573997498)\n",
      "Epoch 00779: reducing learning rate of group 0 to 5.0000e-06. - Epoch [779/5000], Loss: 0.4464 MEE: 0.8486       \n",
      "Training - Epoch [991/5000], Loss: 0.0744, MEE: 0.3575 | Test - Epoch [991/5000], Loss: 0.4042 MEE: 0.7935 \n",
      " N. Epochs = 991 - Loss (train | test)= (0.07436 | 0.4042) - MEE (train | test) = (0.35749155282974243 | 0.7935054302215576)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.06294 +- 0.008538 | 0.3823 +- 0.02945)- MEE (train | test/val ) = ( 0.3438 +-  0.009732 | 0.7827 +- 0.01105)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.15, 0.35, 0.5] --> train_loss = 0.05766 +- 0.006691 | val_loss = 0.3723 +- 0.05026train_mee = 0.3449 +- 0.01644 | val_mee = 0.725 +- 0.0701\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.15, 0.3, 0.55]\n",
      "Epoch 00975: reducing learning rate of group 0 to 5.0000e-06. - Epoch [975/5000], Loss: 0.3017 MEE: 0.6878      \n",
      "Epoch 01032: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1032/5000], Loss: 0.2779 MEE: 0.6362 \n",
      "Training - Epoch [1292/5000], Loss: 0.0626, MEE: 0.3803 | Test - Epoch [1292/5000], Loss: 0.2625 MEE: 0.6172 \n",
      " N. Epochs = 1292 - Loss (train | test)= (0.06255 | 0.2625) - MEE (train | test) = (0.3803042769432068 | 0.617203414440155)\n",
      "Training - Epoch [995/5000], Loss: 0.0769, MEE: 0.4217 | Test - Epoch [995/5000], Loss: 0.3033 MEE: 0.6524      \n",
      " N. Epochs = 995 - Loss (train | test)= (0.07689 | 0.3033) - MEE (train | test) = (0.4216955602169037 | 0.6523587107658386)\n",
      "Epoch 00900: reducing learning rate of group 0 to 5.0000e-06. - Epoch [900/5000], Loss: 0.3282 MEE: 0.6948      \n",
      "Epoch 01097: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1097/5000], Loss: 0.2977 MEE: 0.6407 \n",
      "Epoch 01125: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1125/5000], Loss: 0.2900 MEE: 0.6299 \n",
      "Training - Epoch [1150/5000], Loss: 0.0655, MEE: 0.3823 | Test - Epoch [1150/5000], Loss: 0.2889 MEE: 0.6293 \n",
      " N. Epochs = 1150 - Loss (train | test)= (0.06545 | 0.2889) - MEE (train | test) = (0.3823474049568176 | 0.6292933225631714)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.0683 +- 0.006187 | 0.2849 +- 0.0169)- MEE (train | test/val ) = ( 0.3948 +-  0.01905 | 0.633 +- 0.01458)\n",
      "Epoch 00950: reducing learning rate of group 0 to 5.0000e-06. - Epoch [950/5000], Loss: 0.4115 MEE: 0.7877       \n",
      "Epoch 01087: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1087/5000], Loss: 0.3950 MEE: 0.7673 \n",
      "Training - Epoch [1160/5000], Loss: 0.0500, MEE: 0.3377 | Test - Epoch [1160/5000], Loss: 0.3850 MEE: 0.7517 \n",
      " N. Epochs = 1160 - Loss (train | test)= (0.05 | 0.385) - MEE (train | test) = (0.33767008781433105 | 0.7516679763793945)\n",
      "Epoch 01011: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1011/5000], Loss: 0.4639 MEE: 0.8012     \n",
      "Training - Epoch [1040/5000], Loss: 0.0602, MEE: 0.3688 | Test - Epoch [1040/5000], Loss: 0.4509 MEE: 0.7916 \n",
      " N. Epochs = 1040 - Loss (train | test)= (0.06017 | 0.4509) - MEE (train | test) = (0.3688178062438965 | 0.7916017770767212)\n",
      "Epoch 00888: reducing learning rate of group 0 to 5.0000e-06. - Epoch [888/5000], Loss: 0.4283 MEE: 0.7955       \n",
      "Epoch 00997: reducing learning rate of group 0 to 2.5000e-06. - Epoch [997/5000], Loss: 0.3954 MEE: 0.7578 \n",
      "Training - Epoch [1040/5000], Loss: 0.0579, MEE: 0.3559 | Test - Epoch [1040/5000], Loss: 0.3915 MEE: 0.7541 \n",
      " N. Epochs = 1040 - Loss (train | test)= (0.05785 | 0.3915) - MEE (train | test) = (0.35594549775123596 | 0.7540661096572876)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.05601 +- 0.004352 | 0.4092 +- 0.02965)- MEE (train | test/val ) = ( 0.3541 +-  0.01278 | 0.7658 +- 0.01829)\n",
      "Epoch 00989: reducing learning rate of group 0 to 5.0000e-06. - Epoch [989/5000], Loss: 0.3862 MEE: 0.8084       \n",
      "Epoch 01013: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1013/5000], Loss: 0.3811 MEE: 0.8069 \n",
      "Training - Epoch [1056/5000], Loss: 0.0850, MEE: 0.3882 | Test - Epoch [1056/5000], Loss: 0.3746 MEE: 0.7920 \n",
      " N. Epochs = 1056 - Loss (train | test)= (0.08497 | 0.3746) - MEE (train | test) = (0.38823017477989197 | 0.7919715642929077)\n",
      "Training - Epoch [861/5000], Loss: 0.0940, MEE: 0.4215 | Test - Epoch [861/5000], Loss: 0.3964 MEE: 0.8211       \n",
      " N. Epochs = 861 - Loss (train | test)= (0.09399 | 0.3964) - MEE (train | test) = (0.42146387696266174 | 0.8210758566856384)\n",
      "Epoch 00934: reducing learning rate of group 0 to 5.0000e-06. - Epoch [934/5000], Loss: 0.3741 MEE: 0.8011      \n",
      "Training - Epoch [970/5000], Loss: 0.0713, MEE: 0.3795 | Test - Epoch [970/5000], Loss: 0.3622 MEE: 0.7738 \n",
      " N. Epochs = 970 - Loss (train | test)= (0.07127 | 0.3622) - MEE (train | test) = (0.3795076310634613 | 0.77383953332901)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.08341 +- 0.00934 | 0.3777 +- 0.01414)- MEE (train | test/val ) = ( 0.3964 +-  0.01808 | 0.7956 +- 0.01946)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.15, 0.3, 0.55] --> train_loss = 0.06924 +- 0.01121 | val_loss = 0.3572 +- 0.05277train_mee = 0.3818 +- 0.01955 | val_mee = 0.7315 +- 0.07071\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.1, 0.25, 0.55]\n",
      "Epoch 01035: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1035/5000], Loss: 0.3336 MEE: 0.7127    \n",
      "Epoch 01128: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1128/5000], Loss: 0.3179 MEE: 0.6841 \n",
      "Epoch 01167: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1167/5000], Loss: 0.3180 MEE: 0.6804 \n",
      "Training - Epoch [1207/5000], Loss: 0.0952, MEE: 0.4630 | Test - Epoch [1207/5000], Loss: 0.3165 MEE: 0.6740 \n",
      " N. Epochs = 1207 - Loss (train | test)= (0.09518 | 0.3165) - MEE (train | test) = (0.4630099833011627 | 0.6739816665649414)\n",
      "Training - Epoch [1142/5000], Loss: 0.1012, MEE: 0.4799 | Test - Epoch [1142/5000], Loss: 0.3241 MEE: 0.7084    \n",
      " N. Epochs = 1142 - Loss (train | test)= (0.1012 | 0.3241) - MEE (train | test) = (0.47986072301864624 | 0.7083888649940491)\n",
      "Epoch 01254: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1254/5000], Loss: 0.2986 MEE: 0.6817    \n",
      "Epoch 01405: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1405/5000], Loss: 0.2761 MEE: 0.6332 \n",
      "Epoch 01500: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1500/5000], Loss: 0.2758 MEE: 0.6343 \n",
      "Training - Epoch [1503/5000], Loss: 0.0679, MEE: 0.3970 | Test - Epoch [1503/5000], Loss: 0.2752 MEE: 0.6296 \n",
      " N. Epochs = 1503 - Loss (train | test)= (0.06793 | 0.2752) - MEE (train | test) = (0.39696893095970154 | 0.6296025514602661)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.08811 +- 0.01448 | 0.3053 +- 0.02149)- MEE (train | test/val ) = ( 0.4466 +-  0.03577 | 0.6707 +- 0.03225)\n",
      "Epoch 01209: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1209/5000], Loss: 0.4959 MEE: 0.8297     \n",
      "Training - Epoch [1250/5000], Loss: 0.0586, MEE: 0.3694 | Test - Epoch [1250/5000], Loss: 0.4813 MEE: 0.8095 \n",
      " N. Epochs = 1250 - Loss (train | test)= (0.05863 | 0.4813) - MEE (train | test) = (0.3694491386413574 | 0.8094507455825806)\n",
      "Training - Epoch [1088/5000], Loss: 0.0706, MEE: 0.4012 | Test - Epoch [1088/5000], Loss: 0.4353 MEE: 0.8138     \n",
      " N. Epochs = 1088 - Loss (train | test)= (0.07063 | 0.4353) - MEE (train | test) = (0.4012107849121094 | 0.8138478398323059)\n",
      "Epoch 01184: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1184/5000], Loss: 0.4239 MEE: 0.7894     \n",
      "Epoch 01213: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1213/5000], Loss: 0.4153 MEE: 0.7740 \n",
      "Training - Epoch [1217/5000], Loss: 0.0614, MEE: 0.3760 | Test - Epoch [1217/5000], Loss: 0.4168 MEE: 0.7753 \n",
      " N. Epochs = 1217 - Loss (train | test)= (0.06142 | 0.4168) - MEE (train | test) = (0.3759715259075165 | 0.775346040725708)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.06356 +- 0.005128 | 0.4445 +- 0.02709)- MEE (train | test/val ) = ( 0.3822 +-  0.0137 | 0.7995 +- 0.01721)\n",
      "Training - Epoch [1214/5000], Loss: 0.0788, MEE: 0.4041 | Test - Epoch [1214/5000], Loss: 0.3281 MEE: 0.7664     \n",
      " N. Epochs = 1214 - Loss (train | test)= (0.07879 | 0.3281) - MEE (train | test) = (0.40412747859954834 | 0.7663625478744507)\n",
      "Epoch 00999: reducing learning rate of group 0 to 5.0000e-06. - Epoch [999/5000], Loss: 0.3984 MEE: 0.8661       \n",
      "Epoch 01077: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1077/5000], Loss: 0.3781 MEE: 0.8324 \n",
      "Training - Epoch [1152/5000], Loss: 0.0902, MEE: 0.4325 | Test - Epoch [1152/5000], Loss: 0.3599 MEE: 0.8069 \n",
      " N. Epochs = 1152 - Loss (train | test)= (0.09016 | 0.3599) - MEE (train | test) = (0.43248140811920166 | 0.8069274425506592)\n",
      "Epoch 01179: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1179/5000], Loss: 0.2977 MEE: 0.7515     \n",
      "Epoch 01200: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1200/5000], Loss: 0.3005 MEE: 0.7477 \n",
      "Training - Epoch [1232/5000], Loss: 0.0640, MEE: 0.3785 | Test - Epoch [1232/5000], Loss: 0.2920 MEE: 0.7414 \n",
      " N. Epochs = 1232 - Loss (train | test)= (0.06402 | 0.292) - MEE (train | test) = (0.3784761428833008 | 0.7413814663887024)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.07766 +- 0.0107 | 0.3267 +- 0.02774)- MEE (train | test/val ) = ( 0.405 +-  0.02206 | 0.7716 +- 0.02701)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.1, 0.25, 0.55] --> train_loss = 0.07644 +- 0.01006 | val_loss = 0.3588 +- 0.06119train_mee = 0.4113 +- 0.02666 | val_mee = 0.7473 +- 0.05535\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.05, 0.4, 0.55]\n",
      "Epoch 01051: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1051/5000], Loss: 0.2912 MEE: 0.6562    \n",
      "Epoch 01234: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1234/5000], Loss: 0.2681 MEE: 0.6120 \n",
      "Training - Epoch [1296/5000], Loss: 0.0643, MEE: 0.3761 | Test - Epoch [1296/5000], Loss: 0.2665 MEE: 0.6009 \n",
      " N. Epochs = 1296 - Loss (train | test)= (0.06434 | 0.2665) - MEE (train | test) = (0.3760848343372345 | 0.6009401082992554)\n",
      "Training - Epoch [956/5000], Loss: 0.1020, MEE: 0.4656 | Test - Epoch [956/5000], Loss: 0.3471 MEE: 0.6944       \n",
      " N. Epochs = 956 - Loss (train | test)= (0.102 | 0.3471) - MEE (train | test) = (0.46557557582855225 | 0.694391131401062)\n",
      "Epoch 00960: reducing learning rate of group 0 to 5.0000e-06. - Epoch [960/5000], Loss: 0.3609 MEE: 0.7120      \n",
      "Training - Epoch [1004/5000], Loss: 0.0977, MEE: 0.4616 | Test - Epoch [1004/5000], Loss: 0.3412 MEE: 0.6854 \n",
      " N. Epochs = 1004 - Loss (train | test)= (0.09769 | 0.3412) - MEE (train | test) = (0.46162769198417664 | 0.6853826642036438)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.08802 +- 0.01684 | 0.3183 +- 0.03666)- MEE (train | test/val ) = ( 0.4344 +-  0.04129 | 0.6602 +- 0.04209)\n",
      "Epoch 01096: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1096/5000], Loss: 0.4682 MEE: 0.8492     \n",
      "Training - Epoch [1218/5000], Loss: 0.0636, MEE: 0.3795 | Test - Epoch [1218/5000], Loss: 0.4463 MEE: 0.8206 \n",
      " N. Epochs = 1218 - Loss (train | test)= (0.06361 | 0.4463) - MEE (train | test) = (0.3794507682323456 | 0.8205595016479492)\n",
      "Epoch 00882: reducing learning rate of group 0 to 5.0000e-06. - Epoch [882/5000], Loss: 0.6538 MEE: 0.9904       \n",
      "Epoch 01227: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1227/5000], Loss: 0.5477 MEE: 0.8836 \n",
      "Training - Epoch [1315/5000], Loss: 0.0710, MEE: 0.3973 | Test - Epoch [1315/5000], Loss: 0.5400 MEE: 0.8718 \n",
      " N. Epochs = 1315 - Loss (train | test)= (0.071 | 0.54) - MEE (train | test) = (0.39730384945869446 | 0.8718390464782715)\n",
      "Epoch 01097: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1097/5000], Loss: 0.4972 MEE: 0.8281     \n",
      "Epoch 01230: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1230/5000], Loss: 0.4744 MEE: 0.8098 \n",
      "Epoch 01269: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1269/5000], Loss: 0.4722 MEE: 0.8071 \n",
      "Training - Epoch [1292/5000], Loss: 0.0490, MEE: 0.3339 | Test - Epoch [1292/5000], Loss: 0.4682 MEE: 0.8014 \n",
      " N. Epochs = 1292 - Loss (train | test)= (0.049 | 0.4682) - MEE (train | test) = (0.3338797688484192 | 0.8014367818832397)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.0612 +- 0.009143 | 0.4848 +- 0.03999)- MEE (train | test/val ) = ( 0.3702 +-  0.0267 | 0.8313 +- 0.02972)\n",
      "Epoch 01121: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1121/5000], Loss: 0.3382 MEE: 0.7804    \n",
      "Epoch 01176: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1176/5000], Loss: 0.3238 MEE: 0.7560 \n",
      "Epoch 01242: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1242/5000], Loss: 0.3145 MEE: 0.7483 \n",
      "Training - Epoch [1266/5000], Loss: 0.0650, MEE: 0.3677 | Test - Epoch [1266/5000], Loss: 0.3153 MEE: 0.7487 \n",
      " N. Epochs = 1266 - Loss (train | test)= (0.06499 | 0.3153) - MEE (train | test) = (0.36773279309272766 | 0.7486640214920044)\n",
      "Epoch 01013: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1013/5000], Loss: 0.3461 MEE: 0.8174     \n",
      "Epoch 01164: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1164/5000], Loss: 0.3241 MEE: 0.7780 \n",
      "Epoch 01203: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1203/5000], Loss: 0.3199 MEE: 0.7713 \n",
      "Training - Epoch [1225/5000], Loss: 0.0632, MEE: 0.3744 | Test - Epoch [1225/5000], Loss: 0.3197 MEE: 0.7703 \n",
      " N. Epochs = 1225 - Loss (train | test)= (0.0632 | 0.3197) - MEE (train | test) = (0.37436777353286743 | 0.7702590823173523)\n",
      "Epoch 01005: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1005/5000], Loss: 0.3620 MEE: 0.7926     \n",
      "Training - Epoch [1031/5000], Loss: 0.0806, MEE: 0.3932 | Test - Epoch [1031/5000], Loss: 0.3590 MEE: 0.7841 \n",
      " N. Epochs = 1031 - Loss (train | test)= (0.08064 | 0.359) - MEE (train | test) = (0.3932410478591919 | 0.7840558886528015)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.06961 +- 0.007837 | 0.3314 +- 0.01965)- MEE (train | test/val ) = ( 0.3784 +-  0.01081 | 0.7677 +- 0.01457)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.05, 0.4, 0.55] --> train_loss = 0.07295 +- 0.0112 | val_loss = 0.3782 +- 0.07562train_mee = 0.3944 +- 0.02853 | val_mee = 0.7531 +- 0.07059\n",
      "activation=Tanh();; neuron_number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; optim = RMS; proportions = [0.25, 0.35, 0.4]\n",
      "Epoch 00962: reducing learning rate of group 0 to 5.0000e-06. - Epoch [962/5000], Loss: 0.2253 MEE: 0.5782       \n",
      "Training - Epoch [1070/5000], Loss: 0.0329, MEE: 0.2715 | Test - Epoch [1070/5000], Loss: 0.2167 MEE: 0.5501 \n",
      " N. Epochs = 1070 - Loss (train | test)= (0.03294 | 0.2167) - MEE (train | test) = (0.2714827060699463 | 0.550058901309967)\n",
      "Epoch 00951: reducing learning rate of group 0 to 5.0000e-06. - Epoch [951/5000], Loss: 0.2693 MEE: 0.5941       \n",
      "Epoch 00992: reducing learning rate of group 0 to 2.5000e-06. - Epoch [992/5000], Loss: 0.2581 MEE: 0.5799 \n",
      "Training - Epoch [1030/5000], Loss: 0.0370, MEE: 0.2889 | Test - Epoch [1030/5000], Loss: 0.2494 MEE: 0.5684 \n",
      " N. Epochs = 1030 - Loss (train | test)= (0.03702 | 0.2494) - MEE (train | test) = (0.2888651490211487 | 0.5683574080467224)\n",
      "Epoch 00912: reducing learning rate of group 0 to 5.0000e-06. - Epoch [912/5000], Loss: 0.3069 MEE: 0.6377       \n",
      "Training - Epoch [1083/5000], Loss: 0.0371, MEE: 0.2859 | Test - Epoch [1083/5000], Loss: 0.2885 MEE: 0.6029 \n",
      " N. Epochs = 1083 - Loss (train | test)= (0.03714 | 0.2885) - MEE (train | test) = (0.2858849763870239 | 0.6028966903686523)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.0357 +- 0.001954 | 0.2515 +- 0.02933)- MEE (train | test/val ) = ( 0.2821 +-  0.00759 | 0.5738 +- 0.02191)\n",
      "Epoch 01000: reducing learning rate of group 0 to 5.0000e-06.t - Epoch [1000/5000], Loss: 0.5014 MEE: 0.7928     \n",
      "Epoch 01053: reducing learning rate of group 0 to 2.5000e-06.t - Epoch [1053/5000], Loss: 0.4837 MEE: 0.7727 \n",
      "Training - Epoch [1090/5000], Loss: 0.0318, MEE: 0.2578 | Test - Epoch [1090/5000], Loss: 0.4774 MEE: 0.7643 \n",
      " N. Epochs = 1090 - Loss (train | test)= (0.03179 | 0.4774) - MEE (train | test) = (0.25784429907798767 | 0.764290452003479)\n",
      "Epoch 00862: reducing learning rate of group 0 to 5.0000e-06. - Epoch [862/5000], Loss: 0.5600 MEE: 0.8631       \n",
      "Epoch 00925: reducing learning rate of group 0 to 2.5000e-06. - Epoch [925/5000], Loss: 0.5286 MEE: 0.8228 \n",
      "Epoch 00946: reducing learning rate of group 0 to 1.2500e-06. - Epoch [946/5000], Loss: 0.5229 MEE: 0.8173 \n",
      "Epoch 00974: reducing learning rate of group 0 to 6.2500e-07. - Epoch [974/5000], Loss: 0.5211 MEE: 0.8155 \n",
      "Training - Epoch [978/5000], Loss: 0.0566, MEE: 0.3254 | Test - Epoch [978/5000], Loss: 0.5189 MEE: 0.8146 \n",
      " N. Epochs = 978 - Loss (train | test)= (0.05656 | 0.5189) - MEE (train | test) = (0.3253759443759918 | 0.8145973086357117)\n",
      "Epoch 00848: reducing learning rate of group 0 to 5.0000e-06. - Epoch [848/5000], Loss: 0.4650 MEE: 0.7864       \n",
      "Epoch 00954: reducing learning rate of group 0 to 2.5000e-06. - Epoch [954/5000], Loss: 0.4302 MEE: 0.7581 \n",
      "Epoch 01062: reducing learning rate of group 0 to 1.2500e-06.t - Epoch [1062/5000], Loss: 0.4143 MEE: 0.7441 \n",
      "Training - Epoch [1092/5000], Loss: 0.0370, MEE: 0.2781 | Test - Epoch [1092/5000], Loss: 0.4142 MEE: 0.7414 \n",
      " N. Epochs = 1092 - Loss (train | test)= (0.03705 | 0.4142) - MEE (train | test) = (0.2781307101249695 | 0.7413827180862427)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.0418 +- 0.01066 | 0.4702 +- 0.04306)- MEE (train | test/val ) = ( 0.2871 +-  0.02829 | 0.7734 +- 0.03058)\n",
      "Epoch 00941: reducing learning rate of group 0 to 5.0000e-06. - Epoch [941/5000], Loss: 0.4060 MEE: 0.7919       \n",
      "Epoch 00967: reducing learning rate of group 0 to 2.5000e-06. - Epoch [967/5000], Loss: 0.3935 MEE: 0.7689 \n",
      "Training - Epoch [1022/5000], Loss: 0.0468, MEE: 0.2768 | Test - Epoch [1022/5000], Loss: 0.3838 MEE: 0.7585 \n",
      " N. Epochs = 1022 - Loss (train | test)= (0.04679 | 0.3838) - MEE (train | test) = (0.2767575681209564 | 0.7585319876670837)\n",
      "Epoch 00959: reducing learning rate of group 0 to 5.0000e-06. - Epoch [959/5000], Loss: 0.3887 MEE: 0.7706       \n",
      "Epoch 00986: reducing learning rate of group 0 to 2.5000e-06. - Epoch [986/5000], Loss: 0.3789 MEE: 0.7541 \n",
      "Training - Epoch [1029/5000], Loss: 0.0447, MEE: 0.2702 | Test - Epoch [1029/5000], Loss: 0.3707 MEE: 0.7475 \n",
      " N. Epochs = 1029 - Loss (train | test)= (0.04469 | 0.3707) - MEE (train | test) = (0.27021172642707825 | 0.7475066184997559)\n",
      "Training - Epoch [972/5000], Loss: 0.0687, MEE: 0.3075 | Test - Epoch [972/5000], Loss: 0.4233 MEE: 0.7920       \n",
      " N. Epochs = 972 - Loss (train | test)= (0.06865 | 0.4233) - MEE (train | test) = (0.3075195252895355 | 0.7920491099357605)\n",
      "Avg of 3 initializations: Loss (train | test/val )= (0.05338 +- 0.01084 | 0.3926 +- 0.02234)- MEE (train | test/val ) = ( 0.2848 +-  0.01627 | 0.766 +- 0.01894)\n",
      "Final Results: activation=Tanh(); neuron number=1000; lr=1e-05; alpha = 0.9; batch size = 128; lambda = 0.001; proportions = [0.25, 0.35, 0.4] --> train_loss = 0.04363 +- 0.007331 | val_loss = 0.3714 +- 0.0905train_mee = 0.2847 +- 0.00206 | val_mee = 0.7044 +- 0.09242\n",
      "[1000, 1e-05, 0.9, 128, 0.001, Tanh(), [0.2, 0.3, 0.5]]\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = [1000] #total number of neurons\n",
    "learning_rates = [1e-5]\n",
    "momentums = [0.9] #if optimiz = 'Adam' it doesn't matter\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "#proportionss = [[0.1,0.8,0.1],[0.2,0.6,0.2],[0.25,0.50,0.25],[0.2,0.7,0.1],[0.1,0.7,0.2],[0.3,0.6,0.1]]\n",
    "#proportionss = [[0.8,0.1,0.1],[0.7,0.2,0.1],[0.6,0.3,0.1],[0.6,0.2,0.2],[0.8,0.15,0.05],[0.5,0.4,0.1],[0.5,0.25,0.25], [0.5,0.3,0.2]]\n",
    "#proportionss = [[0.1,0.1,0.8],[0.1,0.2,0.7],[0.1,0.3,0.6],[0.2,0.2,0.6],[0.05,0.15,0.8],[0.1,0.4,0.5],[0.25,0.25,0.5], [0.2,0.3,0.5]]\n",
    "proportionss = [[0.2,0.3,0.5],[0.25,0.25,0.5],[0.15,0.35,0.5],[0.15,0.3,0.55],[0.1,0.25,0.55],[0.05,0.40,0.55],[0.25,0.35,0.4]]\n",
    "optimiz = 'RMS'\n",
    "best_hp = perform_grid_search_kfold(hidden_neurons,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    activations,\n",
    "                                    optimiz,\n",
    "                                    proportionss,\n",
    "                                    k_folds=3,\n",
    "                                    x=X_train,\n",
    "                                    y=y_train,\n",
    "                                    num_epochs=5000,\n",
    "                                    plot_curves=False,\n",
    "                                    N=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cmepda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
