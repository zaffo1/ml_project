{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monk - SVM\n",
    "In this notebook we apply SVMs to solve the Monk 1, Monk 2 and Monk 3 benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)   # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)   # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)    # fontsize of the figure suptitle\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_monk(file_name):\n",
    "    '''\n",
    "    Load data from the Monk dataset and preprocess using one-hot encoding.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name (str): The file name of the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - x (torch.Tensor): Input data after one-hot encoding.\n",
    "    - y (torch.Tensor): Target data.\n",
    "    '''\n",
    "\n",
    "    # load the dataset, split into input (X) and output (y) variables\n",
    "    df = pd.read_csv(file_name, delimiter=' ', header=None,\n",
    "                     names=['_','target', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'data_number'],\n",
    "                     index_col=False)\n",
    "\n",
    "    # Extract input features and target variable\n",
    "    x1, x2, x3, x4, x5, x6, target = (np.array(df[feature]) for feature in ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'target'])\n",
    "\n",
    "    # Initialize OneHotEncoder\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    # Reshape and transform x1 using one-hot encoding\n",
    "    input_one_hot = encoder.fit_transform(x1.reshape(-1, 1))\n",
    "\n",
    "    # Loop through the remaining input features and concatenate one-hot encoded values\n",
    "    for x in [x2,x3,x4,x5,x6]:\n",
    "        data =x.reshape(-1, 1)\n",
    "        one_hot_encoded = encoder.fit_transform(data)\n",
    "        input_one_hot = np.hstack((input_one_hot, one_hot_encoded))\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    x = torch.tensor(input_one_hot, dtype=torch.float32)#.cuda()\n",
    "    y = torch.tensor(target, dtype=torch.float32).reshape(-1,1)#.cuda()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def perform_grid_search_kfold(kernels, Cs, degrees, gammas, k_folds, x, y):\n",
    "    \"\"\"\n",
    "    Perform grid search with k-fold cross-validation for Support Vector Classification hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - kernels (list): List of kernel types to search.\n",
    "    - Cs (list): List of regularization parameters to search.\n",
    "    - degrees (list): List of degrees for polynomial kernels.\n",
    "    - gammas (list): List of gamma values for RBF/polynomial/sigmoid kernels.\n",
    "    - k_folds (int): Number of folds for cross-validation.\n",
    "    - x (numpy.ndarray): Input data.\n",
    "    - y (numpy.ndarray): Target data.\n",
    "\n",
    "    Returns:\n",
    "    - list: Best hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_hyperparams = []\n",
    "    counter = 0\n",
    "    num_combinations = sum(1 for _ in product(kernels, Cs, degrees, gammas))\n",
    "    print('Total number of grid search combinations explored:', num_combinations)\n",
    "\n",
    "    for kernel, C, degree, gamma in product(kernels, Cs, degrees, gammas):\n",
    "        counter += 1\n",
    "        print(f'{counter}/{num_combinations} Hyperparams:', kernel, C, degree, gamma)\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "        accuracies = []\n",
    "\n",
    "        # Perform K-fold cross-validation\n",
    "        for train_indices, val_indices in kf.split(x,y):\n",
    "            # Split the data into training and validation sets\n",
    "            X_train, X_val = x[train_indices], x[val_indices]\n",
    "            y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "            model = svm.SVC(kernel=kernel, C=C, degree=degree, gamma=gamma)\n",
    "            model.fit(X_train, y_train)\n",
    "            predictions = model.predict(X_val)\n",
    "            accuracies.append(accuracy_score(y_val, predictions))\n",
    "\n",
    "        mean_accuracy = np.mean(accuracies)\n",
    "        std_accuracy = np.std(accuracies)\n",
    "        print(f'Current Results: kernel={kernel}; C={C}; degree={degree}; gamma={gamma} --> '\n",
    "              f'accuracy = {mean_accuracy:.4f}+{std_accuracy:.4}')\n",
    "\n",
    "        if mean_accuracy > best_accuracy:\n",
    "            best_accuracy = mean_accuracy\n",
    "            best_hyperparams = [kernel, C, degree, gamma]\n",
    "\n",
    "    print(f'Best Hyperparameters: {best_hyperparams} with Accuracy = {best_accuracy:.4f}+-{std_accuracy:.4}')\n",
    "    return best_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and preprocess the data\n",
    "X_train, y_train = preprocess_monk(file_name='monk_data/monks-1.train')\n",
    "X_test, y_test = preprocess_monk(file_name='monk_data/monks-1.test')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coarse Grid Searches (to find best kernel)\n",
    "We perform two grid searches. One with the rbf kernel, and one with polinomial kernel.\n",
    "In the grid search with rbf, we include the hyperparameter 'gamma', related to the std of the gaussian kernel.\n",
    "In the case of the polinomial kernel we include the degree of the polinomial as hyperparameter.\n",
    "We compare the results of k-fold cross validation to choose the kernel most suitable for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#first coarse grid search\n",
    "kernels = ['rbf']\n",
    "Cs = [0.01,0.1,1,10,100,1000]\n",
    "degrees=[0] #only relevant for poly kernel\n",
    "gammas = ['scale',0.01,0.1,1,10] #related to sigma in rbf\n",
    "#Best Hyperparameters: ['rbf', 1000, 0, 0.01] with Accuracy = 0.8962+-0.1292\n",
    "'''\n",
    "\n",
    "\n",
    "kernels = ['poly']\n",
    "Cs = [0.01,0.1,1,10,100,1000]\n",
    "degrees = np.arange(3,30,1)\n",
    "gammas = ['scale'] # related to sigma in rbf\n",
    "#Best Hyperparameters: ['poly', 10, 3, 'scale'] with Accuracy = 0.9119+-0.07267\n",
    "\n",
    "\n",
    "best_hyperparams = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          degrees,\n",
    "                          gammas,\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finer Grid Search\n",
    "The best kernel is polinomial, now let's study the other hyperparameters more in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['poly']\n",
    "Cs = np.arange(500,5000,50)\n",
    "degrees = np.arange(3,30,1)\n",
    "gammas = ['scale'] # related to sigma in rbf\n",
    "#Best Hyperparameters: ['poly', 10, 3, 'scale'] with Accuracy = 0.9119+-0.07267\n",
    "\n",
    "\n",
    "best_hyperparams = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          degrees,\n",
    "                          gammas,\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'poly'\n",
    "C = 10\n",
    "degree = 3\n",
    "gamma = 'scale'\n",
    "\n",
    "model = svm.SVC(kernel=kernel, C=C, degree=degree, gamma=gamma)\n",
    "model.fit(X_train, y_train.ravel())\n",
    "print(f'Training Accuracy = {accuracy_score(model.predict(X_train),y_train)}')\n",
    "print(f'Test Accuracy = {accuracy_score(model.predict(X_test),y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = preprocess_monk(file_name='monk_data/monks-2.train')\n",
    "X_test, y_test = preprocess_monk(file_name='monk_data/monks-2.test')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coarse Grid Searches (to find best kernel)\n",
    "We perform two grid searches. One with the rbf kernel, and one with polinomial kernel.\n",
    "In the grid search with rbf, we include the hyperparameter 'gamma', related to the std of the gaussian kernel.\n",
    "In the case of the polinomial kernel we include the degree of the polinomial as hyperparameter.\n",
    "We compare the results of k-fold cross validation to choose the kernel most suitable for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first coarse grid search\n",
    "kernels = ['rbf']\n",
    "Cs = [0.01,0.1,1,10,100,1000]\n",
    "degrees=[0] #only relevant for poly kernel\n",
    "gammas = ['scale',0.01,0.1,1,10] #related to sigma in rbf\n",
    "#Best Hyperparameters: ['rbf', 100, 0, 0.1] with Accuracy = 0.6983+-0.005169\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "kernels = ['poly']\n",
    "Cs = [0.01,0.1,1,10,100,1000]\n",
    "degrees = np.arange(3,30,1)\n",
    "gammas = ['scale'] # related to sigma in rbf\n",
    "#Best Hyperparameters: ['poly', 10, 3, 'scale'] with Accuracy = 0.6982+-0.005169\n",
    "'''\n",
    "\n",
    "best_hyperparams = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          degrees,\n",
    "                          gammas,\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finer Grid Search\n",
    "The best kernel is rbf, now let's study the other hyperparameters more in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['rbf']\n",
    "Cs = np.arange(500,5000,500)\n",
    "degrees = [0]\n",
    "gammas =  ['scale',0.1,0.2,0.3,0.4,0.5,0.6,0.7] # related to sigma in rbf\n",
    "#Best Hyperparameters: ['rbf', 500, 0, 0.1] with Accuracy = 0.6983+-0.03516\n",
    "\n",
    "best_hyperparams = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          degrees,\n",
    "                          gammas,\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'rbf'\n",
    "C = 500\n",
    "gamma = 0.1\n",
    "\n",
    "model = svm.SVC(kernel=kernel, C=C, gamma=gamma)\n",
    "model.fit(X_train, y_train.ravel())\n",
    "print(f'Training Accuracy = {accuracy_score(model.predict(X_train),y_train)}')\n",
    "print(f'Test Accuracy = {accuracy_score(model.predict(X_test),y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = preprocess_monk(file_name='monk_data/monks-3.train')\n",
    "X_test, y_test = preprocess_monk(file_name='monk_data/monks-3.test')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coarse Grid Searches (to find best kernel)\n",
    "We perform two grid searches. One with the rbf kernel, and one with polinomial kernel.\n",
    "In the grid search with rbf, we include the hyperparameter 'gamma', related to the std of the gaussian kernel.\n",
    "In the case of the polinomial kernel we include the degree of the polinomial as hyperparameter.\n",
    "We compare the results of k-fold cross validation to choose the kernel most suitable for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#first coarse grid search\n",
    "kernels = ['rbf']\n",
    "Cs = [0.01,0.1,1,10,100,1000]\n",
    "degrees=[0] #only relevant for poly kernel\n",
    "gammas = ['scale',0.01,0.1,1,10] #related to sigma in rbf\n",
    "#Best Hyperparameters: ['rbf', 10, 0, 0.1] with Accuracy = 0.9348+-0.005749\n",
    "'''\n",
    "\n",
    "\n",
    "kernels = ['poly']\n",
    "Cs = [0.01,0.1,1,10,100,1000]\n",
    "degrees = np.arange(3,30,1)\n",
    "gammas = ['scale'] # related to sigma in rbf\n",
    "#Best Hyperparameters: ['poly', 0.1, 5, 'scale'] with Accuracy = 0.9429+-0.04706\n",
    "\n",
    "best_hyperparams = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          degrees,\n",
    "                          gammas,\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finer Grid Search\n",
    "The best kernel is polinomial, now let's study the other hyperparameters more in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['poly']\n",
    "Cs = np.arange(0.01,0.2,0.01)\n",
    "degrees = np.arange(3,10,1)\n",
    "gammas = ['scale'] # related to sigma in rbf\n",
    "#Best Hyperparameters: ['poly', 0.11, 4, 'scale'] with Accuracy = 0.9510+-0.01211\n",
    "\n",
    "\n",
    "\n",
    "best_hyperparams = perform_grid_search_kfold(kernels,\n",
    "                          Cs,\n",
    "                          degrees,\n",
    "                          gammas,\n",
    "                          k_folds=3,\n",
    "                          x=X_train,\n",
    "                          y=y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'poly'\n",
    "C = 0.11\n",
    "degree = 4\n",
    "gamma = 'scale'\n",
    "\n",
    "model = svm.SVC(kernel=kernel, C=C, degree=degree, gamma=gamma)\n",
    "model.fit(X_train, y_train.ravel())\n",
    "print(f'Training Accuracy = {accuracy_score(model.predict(X_train),y_train)}')\n",
    "print(f'Test Accuracy = {accuracy_score(model.predict(X_test),y_test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
