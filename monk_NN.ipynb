{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for GPU availability\n",
    "torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions\n",
    "\n",
    "## 1) function to preprocess (1-hot encode) MONK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_monk(file_name):\n",
    "    '''\n",
    "    Load data from the Monk dataset and preprocess using one-hot encoding.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name (str): The file name of the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - x (torch.Tensor): Input data after one-hot encoding.\n",
    "    - y (torch.Tensor): Target data.\n",
    "    '''\n",
    "\n",
    "    # load the dataset, split into input (X) and output (y) variables\n",
    "    df = pd.read_csv(file_name, delimiter=' ', header=None,\n",
    "                     names=['_','target', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'data_number'],\n",
    "                     index_col=False)\n",
    "\n",
    "    # Extract input features and target variable\n",
    "    x1, x2, x3, x4, x5, x6, target = (np.array(df[feature]) for feature in ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'target'])\n",
    "\n",
    "    # Initialize OneHotEncoder\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    # Reshape and transform x1 using one-hot encoding\n",
    "    input_one_hot = encoder.fit_transform(x1.reshape(-1, 1))\n",
    "\n",
    "    # Loop through the remaining input features and concatenate one-hot encoded values\n",
    "    for x in [x2,x3,x4,x5,x6]:\n",
    "        data =x.reshape(-1, 1)\n",
    "        one_hot_encoded = encoder.fit_transform(data)\n",
    "        input_one_hot = np.hstack((input_one_hot, one_hot_encoded))\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    x = torch.tensor(input_one_hot, dtype=torch.float32)#.cuda()\n",
    "    y = torch.tensor(target, dtype=torch.float32).reshape(-1,1)#.cuda()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to plot training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(epoch, train_losses, test_losses, train_accuracies, test_accuracies, hyperparams):\n",
    "    '''\n",
    "    Plot training and test curves for loss and accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    - epoch (int): The number of epochs.\n",
    "    - train_losses (list): List of training losses for each epoch.\n",
    "    - test_losses (list): List of test losses for each epoch.\n",
    "    - train_accuracies (list): List of training accuracies for each epoch.\n",
    "    - test_accuracies (list): List of test accuracies for each epoch.\n",
    "    - hyperparams (list): List of hyperparameters [hidden_units, lr, alpha, batch_size, lambda].\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.suptitle(f'Batch Size={hyperparams[3]}, Hidden Units={hyperparams[0]}, Eta={hyperparams[1]}, Alpha={hyperparams[2]}, Lambda={hyperparams[4]}')\n",
    "    # Loss plots\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epoch + 2), train_losses, label='Training Loss', color = 'red')\n",
    "    plt.plot(range(1, epoch + 2), test_losses, label='Test Loss', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy plots\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epoch + 2), train_accuracies, label='Training Accuracy', color='red')\n",
    "    plt.plot(range(1, epoch + 2), test_accuracies, label='Test Accuracy', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "\n",
    "class MonkClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size=17, output_size=1):\n",
    "        super(MonkClassifier, self).__init__()\n",
    "\n",
    "        # Custom weight initialization function\n",
    "        #def init_weights(m):\n",
    "        #    if type(m) == nn.Linear:\n",
    "        #        if hasattr(m, 'weight') and m.weight is not None:\n",
    "        #            init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        #            if m.bias is not None:\n",
    "        #               init.constant_(m.bias, 0)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Apply custom weight initialization to linear layers\n",
    "        #self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = self.sigmoid(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(x_train, y_train, x_test, y_test, hidden_size,learning_rate, momentum, bs, reg_coeff, num_epochs=5000, plot_curves=False):\n",
    "    '''\n",
    "    Train a Monk classifier model.\n",
    "\n",
    "    Parameters:\n",
    "    - x_train (torch.Tensor): Training input data.\n",
    "    - y_train (torch.Tensor): Training target data.\n",
    "    - x_test (torch.Tensor): Test (or Validation) input data.\n",
    "    - y_test (torch.Tensor): Test (or Validation) target data.\n",
    "    - hidden_size (int): Size of the hidden layer in the model.\n",
    "    - learning_rate (float): Learning rate for the optimizer.\n",
    "    - momentum (float): Momentum for the optimizer.\n",
    "    - batch_size (int): Batch size for training and testing.\n",
    "    - reg_coeff (float): Regularization coefficient for weight decay.\n",
    "    - num_epochs (int): Maximum number of training epochs (default: 1000).\n",
    "    - plot_curves (bool): Whether to plot training curves (default: False).\n",
    "\n",
    "    Returns:\n",
    "    - epoch (int): Number of epochs completed.\n",
    "    - train_loss (float): Final training loss.\n",
    "    - test_loss (float): Final test (or validation) loss.\n",
    "    - train_accuracy (float): Final training accuracy.\n",
    "    - test_accuracy (float): Final test (or validation) accuracy.\n",
    "    '''\n",
    "\n",
    "    # Create an instance of the model\n",
    "    model = MonkClassifier(hidden_size)\n",
    "    #model.to(device)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=reg_coeff)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "    # Lists to store training and test losses for plotting\n",
    "    train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n",
    "\n",
    "    # parameters to stop at training convergence\n",
    "    min_loss = float('inf')\n",
    "    #prev_accuracy = 0\n",
    "    patience_counter, patience = 0, 20\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        for inputs, labels in train_dataloader:\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels) #Compute the loss\n",
    "\n",
    "            optimizer.zero_grad()   # Zero the gradients\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        #calculate loss\n",
    "        y_pred = model(x_train)\n",
    "        train_loss = criterion(y_pred, y_train)\n",
    "        # Calculate accuracy\n",
    "        predicted_labels = torch.round(y_pred)\n",
    "        correct_predictions = (predicted_labels == y_train).sum().item()\n",
    "        total_samples = y_train.size(0)\n",
    "        accuracy = correct_predictions / total_samples\n",
    "\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_accuracies.append(accuracy)\n",
    "        # Evaluation on the test/validation set\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(x_test)\n",
    "            test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "            # Calculate test accuracy\n",
    "            predicted_test_labels = torch.round(test_outputs)\n",
    "            correct_test_predictions = (predicted_test_labels == y_test).sum().item()\n",
    "            total_test_samples = y_test.size(0)\n",
    "            test_accuracy = correct_test_predictions / total_test_samples\n",
    "\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "        print(f'Training - Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss.item():.4f}, '\n",
    "            f'Accuracy: {accuracy:.4f} | Test - Epoch [{epoch+1}/{num_epochs}], '\n",
    "            f'Loss: {test_loss.item():.4f} Accuracy: {test_accuracy:.4f} ', end='\\r')\n",
    "\n",
    "        # Check for convergence\n",
    "        if train_loss.item() < min_loss and abs(train_loss.item()-min_loss)>1e-4:\n",
    "            patience_counter = 0\n",
    "            min_loss = train_loss.item()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter == patience:\n",
    "        #    print(f\"Convergence reached. at epoch {epoch+1} Stopping training.\")\n",
    "            break\n",
    "        prev_loss = train_loss.item()\n",
    "\n",
    "    print(f'\\n N. Epochs = {epoch+1} - Loss (train | test)= ({train_loss.item():.4} | {test_loss.item():.4}) - Accuracy (train | test) = ({accuracy} | {test_accuracy})')\n",
    "\n",
    "    if plot_curves:\n",
    "        hyperparams = [hidden_size,learning_rate, momentum, bs, reg_coeff, num_epochs]\n",
    "        plot_training_curves(epoch, train_losses, test_losses, train_accuracies, test_accuracies, hyperparams)\n",
    "\n",
    "    return epoch+1, train_loss.item(), test_loss.item(), accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_different_initializations(x_train, y_train, x_test, y_test,\n",
    "                                         hidden_size,learning_rate, momentum,\n",
    "                                         bs, reg_coeff, num_epochs=5000,\n",
    "                                         plot_curves=False, N = 3):\n",
    "    '''\n",
    "    train the model N times with different weight initializations,\n",
    "    to estimate the model performances with mean and variance\n",
    "    '''\n",
    "    train_losses, test_losses, train_accs, test_accs = [], [], [], []\n",
    "    for i in range (0,N):\n",
    "        _, train_loss, test_loss, train_acc, test_acc = training_model(x_train,\n",
    "                        y_train,\n",
    "                        x_test,\n",
    "                        y_test,\n",
    "                        hidden_size,\n",
    "                        learning_rate,\n",
    "                        momentum,\n",
    "                        bs,\n",
    "                        reg_coeff,\n",
    "                        plot_curves=plot_curves,\n",
    "                        num_epochs=num_epochs)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "    print(f'Train Loss: {np.mean(train_losses):.4} +- {np.std(train_losses):.4}')\n",
    "    print(f'Test Loss: {np.mean(test_losses):.4} +- {np.std(test_losses):.4}')\n",
    "    print(f'Train Acc: {np.mean(train_accs)*100:.4} +- {np.std(train_accs)*100:.4} %')\n",
    "    print(f'Test Acc: {np.mean(test_accs)*100:.4} +- {np.std(test_accs)*100:.4} %')\n",
    "    return _, np.mean(train_losses), np.mean(test_losses), np.mean(train_accs), np.mean(test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search_kfold(hidden_sizes, learning_rates, momentums, batch_sizes, reg_coeffs, k_folds, x, y, plot_curves=False, num_epochs=5000):\n",
    "    '''\n",
    "    Perform grid search with k-fold cross-validation for Monk classifier hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - hidden_sizes (list): List of hidden layer sizes to explore.\n",
    "    - learning_rates (list): List of learning rates to explore.\n",
    "    - momentums (list): List of momentum values to explore.\n",
    "    - batch_sizes (list): List of batch sizes to explore.\n",
    "    - reg_coeffs (list): List of regularization coefficients to explore.\n",
    "    - k_folds (int): Number of folds for cross-validation.\n",
    "    - x (torch.Tensor): Input data.\n",
    "    - y (torch.Tensor): Target data.\n",
    "    - plot_curves (bool): Whether to plot training curves for each hyperparameter combination (default: False).\n",
    "\n",
    "    Returns:\n",
    "    - best_hyperparams (list): List of best hyperparameters based on highest average validation accuracy.\n",
    "    '''\n",
    "\n",
    "    best_acc = 0\n",
    "    best_hyperparams = []\n",
    "\n",
    "    for hidden_size, learning_rate, momentum, bs, reg_coeff in product(hidden_sizes,learning_rates,momentums,batch_sizes, reg_coeffs):\n",
    "        print(f'hidden_size={hidden_size}; lr={learning_rate}; alpha = {momentum}; batch size = {bs}; lambda = {reg_coeff}')\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        # Lists to store training and validation losses and accuracies for each epoch\n",
    "        train_losses, val_losses, train_acc, val_acc = [], [], [], []\n",
    "\n",
    "        # Perform K-fold cross-validation\n",
    "        for fold, (train_indices, val_indices) in enumerate(kf.split(x,y)):\n",
    "            #print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "\n",
    "            # Split the data into training and validation (or test) sets\n",
    "            X_train, X_val = x[train_indices], x[val_indices]\n",
    "            Y_train, Y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "            max_epoch, average_loss, average_val_loss, average_accuracy, average_val_accuracy = train_with_different_initializations(\n",
    "                x_train=X_train, y_train=Y_train, x_test=X_val, y_test=Y_val,\n",
    "                hidden_size=hidden_size, learning_rate=learning_rate, momentum=momentum,\n",
    "                bs=bs, reg_coeff=reg_coeff, plot_curves=plot_curves, num_epochs=num_epochs)\n",
    "\n",
    "            train_losses.append(average_loss)\n",
    "            val_losses.append(average_val_loss)\n",
    "            train_acc.append(average_accuracy)\n",
    "            val_acc.append(average_val_accuracy)\n",
    "\n",
    "        print(f'Final Results: hidden_size={hidden_size}; lr={learning_rate}; alpha = {momentum}; batch size = {bs}; lambda = {reg_coeff} --> '\n",
    "            f'train_loss = {np.mean(train_losses):.4} +- {np.std(train_losses):.4} | '\n",
    "            f'val_loss = {np.mean(val_losses):.4} +- {np.std(val_losses):.4}'\n",
    "            f'train_acc = {np.mean(train_acc):.4} +- {np.std(train_acc):.4} | '\n",
    "            f'val_acc = {np.mean(val_acc):.4} +- {np.std(val_acc):.4}')\n",
    "\n",
    "        if np.mean(val_acc) >= best_acc:\n",
    "            best_acc = np.mean(val_acc)\n",
    "            best_hyperparams = [hidden_size, learning_rate, momentum, bs, reg_coeff]\n",
    "\n",
    "    print(best_hyperparams)\n",
    "    return best_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monk 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = preprocess_monk(file_name='monk_data/monks-1.train')\n",
    "x_test, y_test = preprocess_monk(file_name='monk_data/monks-1.test')\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Model Selection using a grid serch with k-fold cross validation\n",
    "\n",
    "inizialmente modificare gli iperparametri ad occhio per avere un'idea di cosa succede,\n",
    "poi fare una grid search più fine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [5]\n",
    "learning_rates = [0.01]\n",
    "momentums = [0.9]\n",
    "batch_sizes = [4]\n",
    "reg_coeffs = [0]\n",
    "\n",
    "\n",
    "best_hp = perform_grid_search_kfold(hidden_sizes,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    k_folds=5,\n",
    "                                    x=x_train,\n",
    "                                    y=y_train,\n",
    "                                    plot_curves=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model (with best hyperparameters) using different weights initiaizatons\n",
    "in order to estimate a mean and variance for the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_different_initializations(x_train=x_train,\n",
    "               y_train=y_train,\n",
    "               x_test=x_test,\n",
    "               y_test=y_test,\n",
    "               hidden_size=best_hp[0],\n",
    "               learning_rate=best_hp[1],\n",
    "               momentum=best_hp[2],\n",
    "               bs = best_hp[3],\n",
    "               reg_coeff= best_hp[4],\n",
    "               plot_curves=False,\n",
    "               N=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain the model to make final plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on all the test set using the best hyperparameters found\n",
    "\n",
    "training_model(x_train=x_train,\n",
    "               y_train=y_train,\n",
    "               x_test=x_test,\n",
    "               y_test=y_test,\n",
    "               hidden_size=best_hp[0],\n",
    "               learning_rate=best_hp[1],\n",
    "               momentum=best_hp[2],\n",
    "               bs = best_hp[3],\n",
    "               reg_coeff= best_hp[4],\n",
    "               plot_curves=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monk 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = preprocess_monk(file_name='monk_data/monks-2.train')\n",
    "x_test, y_test = preprocess_monk(file_name='monk_data/monks-2.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [4]\n",
    "learning_rates = [0.05]\n",
    "momentums = [0.8]\n",
    "batch_sizes = [4]\n",
    "reg_coeffs = [0]\n",
    "\n",
    "\n",
    "best_hp = perform_grid_search_kfold(hidden_sizes,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    k_folds=5,\n",
    "                                    x=x_train,\n",
    "                                    y=y_train,\n",
    "                                    plot_curves=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_different_initializations(x_train=x_train,\n",
    "               y_train=y_train,\n",
    "               x_test=x_test,\n",
    "               y_test=y_test,\n",
    "               hidden_size=best_hp[0],\n",
    "               learning_rate=best_hp[1],\n",
    "               momentum=best_hp[2],\n",
    "               bs = best_hp[3],\n",
    "               reg_coeff= best_hp[4],\n",
    "               plot_curves=False,\n",
    "               N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on all the test set using the best hyperparameters found\n",
    "\n",
    "training_model(x_train=x_train,\n",
    "               y_train=y_train,\n",
    "               x_test=x_test,\n",
    "               y_test=y_test,\n",
    "               hidden_size=best_hp[0],\n",
    "               learning_rate=best_hp[1],\n",
    "               momentum=best_hp[2],\n",
    "               bs = best_hp[3],\n",
    "               reg_coeff= best_hp[4],\n",
    "               plot_curves=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monk 3 (no regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = preprocess_monk(file_name='monk_data/monks-3.train')\n",
    "x_test, y_test = preprocess_monk(file_name='monk_data/monks-3.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [5]\n",
    "learning_rates = [0.003]\n",
    "momentums = [0.9]\n",
    "batch_sizes = [16]\n",
    "reg_coeffs = [0]\n",
    "\n",
    "\n",
    "best_hp = perform_grid_search_kfold(hidden_sizes,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    k_folds=5,\n",
    "                                    x=x_train,\n",
    "                                    y=y_train,\n",
    "                                    plot_curves=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_different_initializations(x_train=x_train,\n",
    "               y_train=y_train,\n",
    "               x_test=x_test,\n",
    "               y_test=y_test,\n",
    "               hidden_size=best_hp[0],\n",
    "               learning_rate=best_hp[1],\n",
    "               momentum=best_hp[2],\n",
    "               bs = best_hp[3],\n",
    "               reg_coeff= best_hp[4],\n",
    "               plot_curves=False,\n",
    "               N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on all the test set using the best hyperparameters found\n",
    "\n",
    "training_model(x_train=x_train,\n",
    "               y_train=y_train,\n",
    "               x_test=x_test,\n",
    "               y_test=y_test,\n",
    "               hidden_size=best_hp[0],\n",
    "               learning_rate=best_hp[1],\n",
    "               momentum=best_hp[2],\n",
    "               bs = best_hp[3],\n",
    "               reg_coeff= best_hp[4],\n",
    "               plot_curves=True,\n",
    "               num_epochs=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monk 3 (with regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = preprocess_monk(file_name='monk_data/monks-3.train')\n",
    "x_test, y_test = preprocess_monk(file_name='monk_data/monks-3.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [5]\n",
    "learning_rates = [0.003]\n",
    "momentums = [0.9]\n",
    "batch_sizes = [16]\n",
    "reg_coeffs = [0.005]\n",
    "\n",
    "\n",
    "best_hp = perform_grid_search_kfold(hidden_sizes,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    k_folds=5,\n",
    "                                    x=x_train,\n",
    "                                    y=y_train,\n",
    "                                    plot_curves=False,\n",
    "                                    num_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_different_initializations(x_train=x_train,\n",
    "               y_train=y_train,\n",
    "               x_test=x_test,\n",
    "               y_test=y_test,\n",
    "               hidden_size=best_hp[0],\n",
    "               learning_rate=best_hp[1],\n",
    "               momentum=best_hp[2],\n",
    "               bs = best_hp[3],\n",
    "               reg_coeff= best_hp[4],\n",
    "               plot_curves=False,\n",
    "               num_epochs=5000,\n",
    "               N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on all the test set using the best hyperparameters found\n",
    "\n",
    "training_model(x_train=x_train,\n",
    "               y_train=y_train,\n",
    "               x_test=x_test,\n",
    "               y_test=y_test,\n",
    "               hidden_size=best_hp[0],\n",
    "               learning_rate=best_hp[1],\n",
    "               momentum=best_hp[2],\n",
    "               bs = best_hp[3],\n",
    "               reg_coeff= best_hp[4],\n",
    "               plot_curves=True,\n",
    "               num_epochs=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
