{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e97bdd98",
   "metadata": {},
   "source": [
    "# ML CUP - Neural Networks - Manual lr decay (and momentum)\n",
    "In this notebook we implement a manual decay of the learning rate, and also a decay of the momentum (for the optimizers that require it). We study if this implementation improves the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e5998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.optim import RMSprop\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a68be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "#function made to reproduce an allarm, made run at the after a grid search to allert that the previous is finished\n",
    "#based of windows system\n",
    "def play_sound():\n",
    "    sound_file = \"C:\\Windows\\Media\\Alarm01.wav\"  # Sostituisci con il percorso del tuo file audio\n",
    "    display(Audio(filename=sound_file, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e21f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47929abe",
   "metadata": {},
   "source": [
    "### Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a regression neural network\n",
    "\n",
    "class RegressorNNVar(nn.Module):\n",
    "    def __init__(self, hidden_size, activation_function, num_layers, dropout_prob, input_size=10, output_size=3):\n",
    "        super(RegressorNNVar, self).__init__()\n",
    "\n",
    "        # input layer\n",
    "        self.layers=[nn.Linear(input_size, hidden_size[0])]\n",
    "\n",
    "        # hidden layers\n",
    "        # The number of layers designated are created with a for cycle, using the vector hidden_size to obtain the correct number of neurons for each layer\n",
    "        i=0\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_size[i], hidden_size[i+1]))\n",
    "            self.layers.append(activation_function)\n",
    "            self.layers.append(nn.Dropout(p=dropout_prob)) #disattivo casualmente un'insieme di unitÃ \n",
    "            i+=1\n",
    "\n",
    "        # output layers\n",
    "        self.layers.append(nn.Linear(hidden_size[i], output_size))\n",
    "\n",
    "        # Create a Sequential container for the layers\n",
    "        self.model=nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5433b",
   "metadata": {},
   "source": [
    "### Training model and grid search definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71fc517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(x_train, y_train, x_test, y_test, proportions, neuron_number, learning_rate, momentum, batch_size,\n",
    "                   reg_coeff, activation_function, num_layers, dropout, decay, mom_decay, epoch_decay, min_lr, min_mom,\n",
    "                   optimizer, num_epochs=1000, plot_curves=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Train the regression model and evaluate it on a test (can also be validation, depending on the context).\n",
    "\n",
    "    Parameters:\n",
    "    - x_train (torch.Tensor): Training input data.\n",
    "    - y_train (torch.Tensor): Training target data.\n",
    "    - x_test (torch.Tensor): Test input data.\n",
    "    - y_test (torch.Tensor): Test target data.\n",
    "    - proportions (list of floats): Defines how many neurons each layer should contain in proportion to total neurons.\n",
    "    - neuron_number (int): Total number of neurons across all layers.\n",
    "    - learning_rate (float): Learning rate for the optimizer.\n",
    "    - momentum (float): Momentum for the optimizer.\n",
    "    - batch_size (int): Batch size for training.\n",
    "    - reg_coeff (float): Regularization coefficient for weight decay.\n",
    "    - activation (torch.nn.Module): Activation function for the model.\n",
    "    - num_layers (int): Number of hidden layers in the model.\n",
    "    - dropout (float):\n",
    "    - decay (float): Decay factor for learning rate.\n",
    "    - mom_decay (float): Decay factor for momentum.\n",
    "    - epoch_decay (int): How many epochs operate the decay.\n",
    "    - min_lr (float): Minimum value that learning rate can assume.\n",
    "    - min_mom (float): Minimum value that momentum can assume.\n",
    "    - optimiz (string): Which optimizer to use.\n",
    "    - num_epochs (int, optional): Number of training epochs (default: 1000).\n",
    "    - plot_curves (bool, optional): Whether to plot training curves (default: False).\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Tuple containing the number of epochs, final training loss, final test loss, final training MEE, MAE and MSE, and final test MEE, MAE and MSE.\n",
    "\n",
    "    The function trains a neural network regression model using the specified hyperparameters and evaluates its performance on the test set.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Defining a hidden_size vector which as much zeros as the number of layers our network should have\n",
    "    hidden_size =[]\n",
    "    for j in range(num_layers):\n",
    "        hidden_size.append(0)\n",
    "\n",
    "    # For each element of hidden_size, set its value as the multiplication between the total neuron number and the percentage in proportions\n",
    "    for i in range(len(proportions)):\n",
    "        hidden_size[i] = int(proportions[i]*neuron_number)\n",
    "\n",
    "    # Create an instance of the model\n",
    "    model=RegressorNNVar(hidden_size, activation_function, num_layers, dropout)\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion=nn.MSELoss()\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), learning_rate, momentum, weight_decay=reg_coeff)\n",
    "\n",
    "    if optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=reg_coeff)\n",
    "\n",
    "    if optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=reg_coeff)\n",
    "\n",
    "    # Preprocessing dataset\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train) # Create a tensor containing dataset\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True) # Dividing data in minibatchs\n",
    "\n",
    "    # Lists to store training and test losses, mees, maes and mses\n",
    "    train_losses, test_losses, train_mees, test_mees, train_maes, test_maes, train_mses, test_mses = [], [], [], [], [], [], [], []\n",
    "\n",
    "    # Parameters to stop at training convergence\n",
    "    min_mee=float(\"inf\")\n",
    "    patience_counter, patience = 0, 20\n",
    "\n",
    "\n",
    "    # Starting the training cycle for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Set the model to training mode\n",
    "\n",
    "        for inputs, labels in train_dataloader:\n",
    "            outputs=model(inputs) # Forward pass\n",
    "            loss=criterion(outputs, labels) # Compute the loss\n",
    "\n",
    "            optimizer.zero_grad() # Zero the gradients\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step() # Update weights\n",
    "\n",
    "        # Obtaining the current learning rate and momentum values from optimizers\n",
    "        for param_group in optimizer.param_groups:\n",
    "            lr_att=param_group['lr']\n",
    "            if type(optimizer) == torch.optim.SGD:\n",
    "                mom_att=param_group['momentum']\n",
    "            if type(optimizer) == torch.optim.RMSprop:\n",
    "                mom_att=param_group['alpha']\n",
    "            else:\n",
    "                mom_att=0 # For Adam's case\n",
    "\n",
    "        # Each time it reaches a epoch_decay number of epochs, it computes the decay of parameters\n",
    "        if (epoch%epoch_decay)==0:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                if lr_att>min_lr:\n",
    "                    param_group['lr'] *= decay # Learning rate decay for each optimizer\n",
    "                if mom_att>min_mom:\n",
    "                    if type(optimizer) == torch.optim.SGD:\n",
    "                        param_group['momentum'] = max(initial_momentum, param_group['momentum'] * mom_decay) # Momentum decay for SGD\n",
    "                    if type(optimizer) == torch.optim.RMSprop:\n",
    "                        param_group['alpha'] *= mom_decay # Momentum decay for RMSprop\n",
    "\n",
    "\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "\n",
    "        # Calculate loss\n",
    "        y_pred=model(x_train)\n",
    "        train_loss=criterion(y_pred, y_train)\n",
    "        # Calculate mee, mae and mse\n",
    "        train_mee, train_mae, train_mse = metrics(y_pred,y_train)\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_mees.append(train_mee)\n",
    "        train_maes.append(train_mae)\n",
    "        train_mses.append(train_mse)\n",
    "\n",
    "        # Evaluation on the test/validation set\n",
    "        with torch.no_grad():\n",
    "            test_outputs=model(x_test)\n",
    "            test_loss=criterion(test_outputs, y_test)\n",
    "\n",
    "            # Calculate mee, mae and msee\n",
    "            test_mee, test_mae, test_mse = metrics(test_outputs,y_test)\n",
    "            test_mees.append(test_mee)\n",
    "            test_maes.append(test_mae)\n",
    "            test_mses.append(test_mse)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "        print(f'Training - Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss.item():.4f}, '\n",
    "            f'MEE: {train_mee:.4f} | Test - Epoch [{epoch+1}/{num_epochs}], '\n",
    "            f'Loss: {test_loss.item():.4f} MEE: {test_mee:.4f} ', end='\\r')\n",
    "\n",
    "        # Check for convergence\n",
    "        if train_mee < min_mee and abs(train_mee-min_mee)>1e-3:\n",
    "            patience_counter=0\n",
    "            min_mee=train_mee\n",
    "        else:\n",
    "            patience_counter +=1\n",
    "\n",
    "        if patience_counter==patience:\n",
    "            break\n",
    "\n",
    "    print(f'\\n N. Epochs = {epoch+1} - Loss (train | test)= ({train_loss.item():.4} | {test_loss.item():.4}) - MEE (train | test) = ({train_mee} | {test_mee}) - MAE (train | test) = ({train_mae} | {test_mae} - MSE (train | test) = ({train_mse} | {test_mse}))')\n",
    "\n",
    "\n",
    "    if plot_curves:\n",
    "        hyperparams=[hidden_size, learning_rate, momentum, batch_size, reg_coeff, activation_function, num_layers, dropout, decay, mom_decay, epoch_decay, num_epochs]\n",
    "        plot_training_curves(epoch, train_losses, test_losses, train_mees, test_mees, hyperparams)\n",
    "\n",
    "\n",
    "\n",
    "    return model, epoch+1, train_loss.item(), test_loss.item(), train_mee, test_mee, train_mae, test_mae, train_mse, test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search_kfold(proportionss, neuron_numbers, learning_rates, momentums, batch_sizes, reg_coeffs,\n",
    "                              activations, layerss, dropouts, decays, mom_decays, epoch_decays, min_lrs, min_moms, optimiz,\n",
    "                              k_folds, x, y, plot_curves=False, num_epochs=1000, N=1):\n",
    "\n",
    "    \"\"\"\n",
    "    Perform grid search with k-fold cross-validation for hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - proportionss (list): List of vectors that define how many neurons each layer should contain in proportion to total neurons.\n",
    "    - neuron_numbers (list): List of neuron numbers to search.\n",
    "    - learning_rates (list): List of learning rates to search.\n",
    "    - momentums (list): List of momentum values to search.\n",
    "    - batch_sizes (list): List of batch sizes to search.\n",
    "    - reg_coeffs (list): List of regularization coefficients to search.\n",
    "    - activations (list): List of activation functions to search.\n",
    "    - layerss (list): List of numbers of hidden layers to search.\n",
    "    - dropouts (list):\n",
    "    - decays (list): List of decay factors for learning rate.\n",
    "    - mom_decays (list): List of decay factors for momentum.\n",
    "    - epoch_decays (list): List of epochs that define when to decay the parameters.\n",
    "    - min_lrs (list): List of minimum values that learning rate can assume.\n",
    "    - min_moms (list): List of minimum values that momentum can assume.\n",
    "    - optimizs (string): Which optimizer to use.\n",
    "    - k_foldss (int): Number of folds for cross-validation.\n",
    "    - x (numpy.ndarray): Input data.\n",
    "    - y (numpy.ndarray): Target data.\n",
    "    - plot_curves (bool, optional): Whether to plot training curves (default: False).\n",
    "    - num_epochs (int, optional): Number of training epochs (default: 1000).\n",
    "    - N (int): Number of times to train the model with different initializations (default: 1).\n",
    "\n",
    "    Returns:\n",
    "    - list: List of best hyperparameters.\n",
    "\n",
    "    The function performs grid search with k-fold cross-validation for Monk classifier hyperparameters and returns the best hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    best_mee=float('inf') #setto la migliore mee come infinito\n",
    "    best_loss=float('inf')\n",
    "    best_hyperparameters=[]\n",
    "    finale=\"\\n----- RISULTATI -----\\n\"\n",
    "\n",
    "    # Counting the total of models the grid search will train\n",
    "    count=1\n",
    "    tot_modelli=len(proportionss)*len(neuron_numbers)*len(learning_rates)*len(momentums)*len(batch_sizes)*len(reg_coeffs)*len(activations)*len(decays)*len(mom_decays)*len(epoch_decays)*len(dropouts)*len(min_lrs)*len(min_moms)\n",
    "\n",
    "    tot_iniz=k_folds*N\n",
    "\n",
    "    # Pick out each parameter set from the grid search possible combinations\n",
    "    for proportions, neuron_number, learning_rate, momentum, batch_size, reg_coeff, activation, layers, decay, mom_decay, epoch_decay, min_lr, min_mom, dropout in product(proportionss, neuron_numbers, learning_rates, momentums, batch_sizes, reg_coeffs, activations, layerss, decays, mom_decays, epoch_decays, min_lrs, min_moms, dropouts):\n",
    "        print(\"\\nModello \"+str(count)+\"\\\\\"+str(tot_modelli))\n",
    "        print(f'activation={activation}; layers={layers}; proportions={proportions}; neuron_number={neuron_number}; lr={learning_rate}; alpha = {momentum}; batch size = {batch_size}; lambda = {reg_coeff}; optim = {optimiz}; decay = {decay}; epoch decay = {epoch_decay}; minimum learning rate = {min_lr}; minimum momentum = {min_mom}')\n",
    "\n",
    "        iniz=1\n",
    "\n",
    "        kf=KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        # Lists to store training and validation losses, MEEs, MAEs, MSEs and maximum epochs for each initialization\n",
    "        train_losses, val_losses, train_mees, val_mees, train_maes, val_maes, train_mses, val_mses, max_epochs = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "        # Perform K-fold cross-validation\n",
    "        for fold, (train_indices, val_indices) in enumerate(kf.split(x,y)):\n",
    "\n",
    "            # Split the data into training and validation (or test) sets\n",
    "            X_train, X_val = x[train_indices], x[val_indices]\n",
    "            Y_train, Y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "            # trainings\n",
    "            for i in range (0,N):\n",
    "\n",
    "                print(\"\\nInizializzazione \"+str(iniz)+\"\\\\\"+str(tot_iniz))\n",
    "                _, max_epoch, train_loss, val_loss, train_mee, val_mee, train_mae, val_mae, train_mse, val_mse = training_model(\n",
    "                x_train=X_train, y_train=Y_train, x_test=X_val, y_test=Y_val, proportions=proportions, neuron_number=neuron_number,\n",
    "                learning_rate=learning_rate, momentum=momentum, batch_size=batch_size, reg_coeff=reg_coeff, activation_function=activation,\n",
    "                num_layers=layers, dropout=dropout, decay=decay, mom_decay=mom_decay, epoch_decay=epoch_decay, min_lr=min_lr, min_mom=min_mom, optimizer=optimiz, num_epochs=num_epochs, plot_curves=plot_curves)\n",
    "\n",
    "                # Saving training results\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                train_mees.append(train_mee)\n",
    "                val_mees.append(val_mee)\n",
    "                train_maes.append(train_mae)\n",
    "                val_maes.append(val_mae)\n",
    "                train_mses.append(train_mse)\n",
    "                val_mses.append(val_mse)\n",
    "                max_epochs.append(max_epoch)\n",
    "                iniz=iniz+1\n",
    "\n",
    "\n",
    "        out=f'Final Results: activation={activation}; layers={layers}; proportions = {proportions}; neuron number={neuron_number}; lr={learning_rate}; alpha = {momentum}; batch size = {batch_size}; lambda = {reg_coeff}; optim = {optimiz}; learning rate decay = {decay}; momentum decay = {mom_decay}; epoch decay = {epoch_decay}; minimum learning rate = {min_lr}; minimum momentum = {min_mom} \\n--> train_loss = {np.mean(train_losses):.4} +- {np.std(train_losses):.4} | val_loss = {np.mean(val_losses):.4} +- {np.std(val_losses):.4} \\ntrain_mee = {np.mean(train_mees):.4} +- {np.std(train_mees):.4} | val_mee = {np.mean(val_mees):.4} +- {np.std(val_mees):.4} \\ntrain_mae = {np.mean(train_maes):.4} +- {np.std(train_maes):.4} | val_mae = {np.mean(val_maes):.4} +- {np.std(val_maes):.4} \\ntrain_mse = {np.mean(train_mses):.4} +- {np.std(train_mses):.4} | val_mse = {np.mean(val_mses):.4} +- {np.std(val_mses):.4}\\nmean max epoch = {round(np.mean(max_epochs))}\\n\\n'\n",
    "        count=count+1\n",
    "        finale=finale+out\n",
    "\n",
    "        # Checking out the best models for MEE and Loss\n",
    "        if np.mean(val_mees)<best_mee:\n",
    "            best_mee_out=out\n",
    "            best_mee=np.mean(val_mees)\n",
    "            best_hyperparams=[proportions, neuron_number, learning_rate, momentum, batch_size, reg_coeff, activation, layers, dropout, decay, mom_decay, epoch_decay, min_lr, min_mom]\n",
    "        if np.mean(val_losses)<best_loss:\n",
    "            best_loss_out=out\n",
    "            best_loss=np.mean(val_losses)\n",
    "            best_hyperparams_loss=[proportions, neuron_number, learning_rate, momentum, batch_size, reg_coeff, activation, layers, dropout, decay, mom_decay, epoch_decay, min_lr, min_mom]\n",
    "            #fallo anche per loss\n",
    "\n",
    "    finale=finale+\"\\n---- MIGLIORI RISULTATI MEE ----\\n\"+best_mee_out+\"\\n---- MIGLIORI RISULTATI LOSS ----\\n\"+best_loss_out\n",
    "    print(finale)\n",
    "    print(best_hyperparams)\n",
    "    return best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653abc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_different_initializations(x_train, y_train, x_test, y_test, proportions, neuron_number, learning_rate,\n",
    "                                         momentum, batch_size, reg_coeff, activation, layers, dropout, decay, mom_decay,\n",
    "                                         epoch_decay, min_lr, min_mom, optimiz, max_num_epochs=1000, plot_curves=False, N=5):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Train the model multiple times with different weight initializations to estimate performance mean and variance.\n",
    "\n",
    "    Parameters:\n",
    "    - x_train (torch.Tensor): Training input data.\n",
    "    - y_train (torch.Tensor): Training target data.\n",
    "    - x_test (torch.Tensor): Test input data.\n",
    "    - y_test (torch.Tensor): Test target data.\n",
    "    - proportions (list of floats): Defines how many neurons each layer should contain in proportion to total neurons.\n",
    "    - neuron_number (int): Total number of neurons across all layers.\n",
    "    - learning_rate (float): Learning rate for the optimizer.\n",
    "    - momentum (float): Momentum for the optimizer.\n",
    "    - bs (int): Batch size for training.\n",
    "    - reg_coeff (float): Regularization coefficient for weight decay.\n",
    "    - activation (torch.nn.Module): Activation function for the model.\n",
    "    - layers (int): Number of hidden layers in the model.\n",
    "    - dropout (float):\n",
    "    - decay (float): Decay factor for learning rate.\n",
    "    - mom_decay (float): Decay factor for momentum.\n",
    "    - epoch_decay (int): How many epochs operate the decay.\n",
    "    - min_lr (float): Minimum value that learning rate can assume.\n",
    "    - min_mom (float): Minimum value that momentum can assume.\n",
    "    - optimiz (string): Which optimizer to use.\n",
    "    - num_epochs (int, optional): Number of training epochs (default: 1000).\n",
    "    - plot_curves (bool, optional): Whether to plot training curves (default: False).\n",
    "    - N (int, optional): Number of times to train the model with different initializations (default: 5).\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    Prints the mean and standard deviation of training and test loss, as well as training and test Mean Euclidean Error (MEE).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Lists to store training and test losses, mees, maes and mses\n",
    "    train_losses, test_losses, train_mees, test_mees, train_maes, test_maes, train_mses, test_mses = [], [], [], [], [], [], [], []\n",
    "    print(f'activation={activation}; layers={layers}; proportions={proportions}; neuron_number={neuron_number}; lr={learning_rate}; alpha = {momentum}; batch size = {batch_size}; lambda = {reg_coeff}; optim = {optimiz}; learning rate decay = {decay}; momentum decay = {mom_decay}; epoch decay = {epoch_decay}; minimum learning rate = {min_lr}; minimum momentum = {min_mom}\\n')\n",
    "\n",
    "    # trainings\n",
    "    for i in range (0,N):\n",
    "        _, epoch, train_loss, test_loss, train_mee, test_mee, train_mae, test_mae, train_mse, test_mse = training_model(x_train, y_train, x_test, y_test, proportions, neuron_number, learning_rate, momentum, batch_size, reg_coeff, activation, layers, dropout, decay, mom_decay, epoch_decay, min_lr, min_mom, optimiz, max_num_epochs, plot_curves)\n",
    "\n",
    "        # Saving training results\n",
    "        train_losses.append(train_loss)\n",
    "        train_maes.append(train_mae)\n",
    "        train_mses.append(train_mse)\n",
    "        train_mees.append(train_mee)\n",
    "        test_losses.append(test_loss)\n",
    "        test_mees.append(test_mee)\n",
    "        test_maes.append(test_mae)\n",
    "        test_mses.append(test_mse)\n",
    "\n",
    "\n",
    "    print(f'Train Loss: {np.mean(train_losses):.4} +- {np.std(train_losses):.4}')\n",
    "    print(f'Test Loss: {np.mean(test_losses):.4} +- {np.std(test_losses):.4}')\n",
    "    print(f'Train MEE: {np.mean(train_mees):.4} +- {np.std(train_mees):.4}')\n",
    "    print(f'Test MEE: {np.mean(test_mees):.4} +- {np.std(test_mees):.4}')\n",
    "    print(f'Train MAE: {np.mean(train_maes):.4} +- {np.std(train_maes):.4}')\n",
    "    print(f'Test MAE: {np.mean(test_maes):.4} +- {np.std(test_maes):.4}')\n",
    "    print(f'Train MSE: {np.mean(train_mses):.4} +- {np.std(train_mses):.4}')\n",
    "    print(f'Test MSE: {np.mean(test_mses):.4} +- {np.std(test_mses):.4}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ecbea0",
   "metadata": {},
   "source": [
    "### Funzioni ausiliarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e253d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_pred, y_true):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the mean Euclidean error, mean Squared error and mean Absolute error between two sets of 3D vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor1: PyTorch tensor of size (N, 3) representing the first set of 3D vectors\n",
    "    - tensor2: PyTorch tensor of size (N, 3) representing the second set of 3D vectors\n",
    "\n",
    "    Returns:\n",
    "    - mean_error: Mean Euclidean error between the two sets of vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the tensors have the correct shape\n",
    "    if y_pred.shape[1] != 3 or y_true.shape[1] != 3 or y_pred.shape[0] != y_true.shape[0]:\n",
    "        raise ValueError(\"Input tensors must be of size (N, 3)\")\n",
    "\n",
    "\n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = torch.norm(y_pred - y_true, dim=1)\n",
    "\n",
    "    # Calculate the mean Euclidean error\n",
    "    mean_error = torch.mean(euclidean_distance)\n",
    "\n",
    "\n",
    "    # Calculate the Mean Absolute Error (MAE)\n",
    "    mae = F.l1_loss(y_pred, y_true)\n",
    "\n",
    "    # Calculate the Mean Squared Error (MSE)\n",
    "    mse = F.mse_loss(y_pred, y_true)\n",
    "\n",
    "\n",
    "    return mean_error.item(), mae.item(), mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eff5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(epoch, train_losses, test_losses, train_mees, test_mees, hyperparams):\n",
    "\n",
    "    \"\"\"\n",
    "    Plot training and test curves for loss and Mean Euclidean Error (MEE).\n",
    "\n",
    "    Parameters:\n",
    "    - epoch (int): The total number of training epochs.\n",
    "    - train_losses (list): List of training losses for each epoch.\n",
    "    - test_losses (list): List of test losses for each epoch.\n",
    "    - train_mees (list): List of training MEE values for each epoch.\n",
    "    - test_mees (list): List of test MEE values for each epoch.\n",
    "    - hyperparams (list): List of hyperparameters used for the plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    Plots four subplots:\n",
    "    1. Training and test loss curves.\n",
    "    2. Training and test MEE curves.\n",
    "    3. Zoomed-in training and test loss curves with y-axis limit [0, 10].\n",
    "    4. Zoomed-in training and test MEE curves with y-axis limit [0, 10].\n",
    "\n",
    "    The hyperparameters are used in the plot title to provide additional context.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.suptitle(f'Batch Size={hyperparams[3]},Activation Function={hyperparams[5]}, Layers={hyperparams[6]} Hidden Units={hyperparams[0]}, Eta={hyperparams[1]}, Alpha={hyperparams[2]}, Lambda={hyperparams[4]}')\n",
    "    # Loss plots\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, epoch + 2), train_losses, label='Training Loss', color = 'red')\n",
    "    plt.plot(range(1, epoch + 2), test_losses, label='Test Loss', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # MEE plots\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, epoch + 2), train_mees, label='Training MEE', color='red')\n",
    "    plt.plot(range(1, epoch + 2), test_mees, label='Test MEE', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss plots\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(train_losses, label='Training Loss', color = 'red')\n",
    "    plt.plot(test_losses, label='Validation Loss', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.ylim(0,10)\n",
    "    plt.legend()\n",
    "\n",
    "    # MEE plots\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(train_mees, label='Training MEE', color='red')\n",
    "    plt.plot(test_mees, label='Validation MEE', color = 'blue', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.ylim(0,10)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01313d95",
   "metadata": {},
   "source": [
    "### Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631203b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('ML-CUP23-TR.csv', delimiter=',')\n",
    "X = dataset[:,1:11]\n",
    "y = dataset[:,11:14]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# Split the data into training and testing sets (80%/20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d676012",
   "metadata": {},
   "source": [
    "## Grid searches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5c6f1",
   "metadata": {},
   "source": [
    "### Prove con Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f98419",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Performing the first grid search with decay related parameters\n",
    "proportionss = [[1/3, 1/3, 1/3]]\n",
    "hidden_neurons = [1000]\n",
    "learning_rates = [1e-04, 3e-04]\n",
    "momentums = [0]\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "decays = [0.9, 0.95]\n",
    "mom_decays = [1]\n",
    "epoch_decays = [100, 150, 200]\n",
    "min_lrs = [1e-09, 1e-08, 1e-07]\n",
    "min_moms = [0]\n",
    "optimiz = 'Adam'\n",
    "\n",
    "\n",
    "# Finer grid serach\n",
    "proportionss = [[1/3, 1/3, 1/3]]\n",
    "hidden_neurons = [1000]\n",
    "learning_rates = [1e-04, 2e-04]\n",
    "momentums = [0]\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "decays = [0.9, 0.85, 0.8]\n",
    "mom_decays = [1]\n",
    "epoch_decays = [200, 250, 300]\n",
    "min_lrs = [1e-07, 5e-08]\n",
    "min_moms = [0]\n",
    "optimiz = 'Adam'\n",
    "'''\n",
    "\n",
    "\n",
    "best_hp = perform_grid_search_kfold(proportionss,\n",
    "                                    hidden_neurons,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    activations,\n",
    "                                    layerss,\n",
    "                                    dropouts,\n",
    "                                    decays,\n",
    "                                    mom_decays,\n",
    "                                    epoch_decays,\n",
    "                                    min_lrs,\n",
    "                                    min_moms,\n",
    "                                    optimiz,\n",
    "                                    k_folds=3,\n",
    "                                    x=X_train,\n",
    "                                    y=y_train,\n",
    "                                    num_epochs=5000,\n",
    "                                    plot_curves=False,\n",
    "                                    N=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb2b09",
   "metadata": {},
   "source": [
    "### Prove con SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Performing the first grid search with decay related parameters\n",
    "proportionss = [[1/3, 1/3, 1/3]]\n",
    "hidden_neurons = [1000]\n",
    "learning_rates = [1e-04, 3e-04]\n",
    "momentums = [0.9, 0.8]\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "decays = [0.9, 0.95]\n",
    "mom_decays = [0.9, 0.8, 1]\n",
    "epoch_decays = [100, 150]\n",
    "min_lrs = [1e-09, 1e-08]\n",
    "min_moms = [0.5, 0.3]\n",
    "optimiz = 'SGD'\n",
    "\n",
    "\n",
    "# Finer grid serach\n",
    "proportionss = [[1/3, 1/3, 1/3]]\n",
    "hidden_neurons = [1000]\n",
    "learning_rates = [2e-04, 3e-04]\n",
    "momentums = [0.9, 0.85]\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "decays = [0.95]\n",
    "mom_decays = [1]\n",
    "epoch_decays = [150, 200]\n",
    "min_lrs = [1e-8, 5e-9, 5e-8]\n",
    "min_moms = [0.3, 0.2]\n",
    "optimiz = 'SGD'\n",
    "'''\n",
    "\n",
    "\n",
    "best_hp = perform_grid_search_kfold(proportionss,\n",
    "                                    hidden_neurons,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    activations,\n",
    "                                    layerss,\n",
    "                                    dropouts,\n",
    "                                    decays,\n",
    "                                    mom_decays,\n",
    "                                    epoch_decays,\n",
    "                                    min_lrs,\n",
    "                                    min_moms,\n",
    "                                    optimiz,\n",
    "                                    k_folds=3,\n",
    "                                    x=X_train,\n",
    "                                    y=y_train,\n",
    "                                    num_epochs=5000,\n",
    "                                    plot_curves=False,\n",
    "                                    N=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd436d",
   "metadata": {},
   "source": [
    "### Prove con RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc7bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Performing the first grid search with decay related parameters\n",
    "proportionss = [[1/3, 1/3, 1/3]]\n",
    "hidden_neurons = [1000]\n",
    "learning_rates = [1e-5, 1e-04]\n",
    "momentums = [0.9, 0.8]\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "decays = [0.9, 0.95]\n",
    "mom_decays = [0.9, 0.8, 1]\n",
    "epoch_decays = [100, 200, 400]\n",
    "min_lrs = [1e-09, 1e-08]\n",
    "min_moms = [0.5, 0.3]\n",
    "optimiz = 'RMSprop'\n",
    "\n",
    "\n",
    "\n",
    "# Finer grid serach\n",
    "proportionss = [[1/3, 1/3, 1/3]]\n",
    "hidden_neurons = [1000]\n",
    "learning_rates = [1e-05, 3e-05]\n",
    "momentums = [0.9, 0.85]\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "decays = [0.9, 0.85]\n",
    "mom_decays = [0.8, 0.85]\n",
    "epoch_decays = [200, 300]\n",
    "min_lrs = [1e-09, 5e-09, 5e-10]\n",
    "min_moms = [0.4, 0.5, 0.6]\n",
    "optimiz = 'RMSprop'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# See the difference with and without momentum decay\n",
    "proportionss = [[1/3, 1/3, 1/3]]\n",
    "hidden_neurons = [1000]\n",
    "learning_rates = [1e-05]\n",
    "momentums = [0.9]\n",
    "batch_sizes = [128]\n",
    "reg_coeffs = [1e-3]\n",
    "activations = [nn.Tanh()]\n",
    "layerss = [3]\n",
    "dropouts = [0]\n",
    "decays = [0.85]\n",
    "mom_decays = [0.8, 1]\n",
    "epoch_decays = [200]\n",
    "min_lrs = [5e-10]\n",
    "min_moms = [0.6]\n",
    "optimiz = 'RMSprop'\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "best_hp = perform_grid_search_kfold(proportionss,\n",
    "                                    hidden_neurons,\n",
    "                                    learning_rates,\n",
    "                                    momentums,\n",
    "                                    batch_sizes,\n",
    "                                    reg_coeffs,\n",
    "                                    activations,\n",
    "                                    layerss,\n",
    "                                    dropouts,\n",
    "                                    decays,\n",
    "                                    mom_decays,\n",
    "                                    epoch_decays,\n",
    "                                    min_lrs,\n",
    "                                    min_moms,\n",
    "                                    optimiz,\n",
    "                                    k_folds=3,\n",
    "                                    x=X_train,\n",
    "                                    y=y_train,\n",
    "                                    num_epochs=5000,\n",
    "                                    plot_curves=False,\n",
    "                                    N=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling a function that reproduces a sound, used as an alarm after grid searches\n",
    "play_sound()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
