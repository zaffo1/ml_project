We use this file to report a complete description of our process of hyperparameter tuning.
In the following we report the path followed for the different models we explored.
In all cases we split the data in training+validation (80%) and internal test (20%),
and performed a 3-fold CV on the training+validation data to perform model selection.
Inside each fold we also trained the model on 3 different initializations and took the average score.

When we say that a configuration of hyperparameters was better, we mean that it obtained
better results on the k-fold cross validation used for model selection.
In this process we do NOT look in any way at the internal test set.
--------------------------------------------------------------------------------------------------------
1. k-NN
for the k-nn model we considered 3 hyperparameters:
-k (number of nearest neighbors)
-weights (how to weight points in each neighborhood)
-p (power parameter for the Minkowski metric)
we performed the following grid search:
k = range(1, 31)
weights=['uniform','distance']
p = range(1,10)
We found the best values to be k=5, weights='distance' (weight points by the inverse of their distance),
p = 1 (equivalent to using manhattan_distance (l1))
--------------------------------------------------------------------------------------------------------
2. Neural Network
Using SGD as optimizer, we performed some initial trials.
First we performed some manual trials to fix the activation function. We explored the Tanh and the ReLU functions,
and decided to use the Tanh, because the results were similar, but the curves with Tanh were smoother.
We also performed some manual trials to choose the batch size, since we used a minibatch approach. We decided to use a minibatch
of 128, becouse it was a sufficient number to speed up the computations thanks to the use of the gpu and the curves were in general smoother.
Finally we performed some more trials in which we explored the effect of lambda. We concluded that it had not a very important effect,
maybe because of the nature of our dataset, but in general we obtained the best results with lambda=1e-3.
After these manual trials, we fixed activation=Tanh; batch=128; lambda=1e-3.

At this point we performed first a coarse grid search:
hidden_neurons = [100,1000]
learning_rates = [1e-3,1e-4,1e-5]
momentums = [0.5,0.8]
batch_sizes = [128]
lambda = [1e-3]
activations = [nn.Tanh()]
number of layers = [1,2,3]

from this fisrt grid search we wanted to understand the best number of layers,
the order of magnitude of the lr, the order of magnitude of the number of neurons,
and a general idea of the best momentum.
The best results were: activation=Tanh(); layers=3; neuron number=1000; lr=0.0001; alpha = 0.8; batch size = 128; lambda = 0.001

We now performed a refined grid search, in which we kept all fixed except the number of neurons and the momentum:
hidden_neurons = [1000,2000,3000]
learning_rates = [1e-4]
momentums = [0.7,0.8,0.9]
batch_sizes = [128]
lambda = [1e-3]
activations = [nn.Tanh()]
number of layers = [3]
Best Hyperparameters: activation=Tanh(); layers=3; neuron number=1000; lr=0.0001; alpha = 0.9; batch size = 128; lambda = 0.001

This was our selected best model using SGD.
Now we wanted to explore the use of other optimizers, such as Adam and RMSProp.
We then followed for both the optimizers a similar procedure as for the SGD,
first with a coarse grid search and then with a finer one.
The best hyperparameters found with Adam were:
activation=Tanh(); layers=3; neuron number=1000; lr=0.0001; batch size = 128; lambda = 0.001
And with RMSProp;
activation=Tanh(); layers=3; neuron number=1000; lr=0.00001; alpha = 0.9; batch size = 128; lambda = 0.001

At this point we compared the validation MEE obtained with the three optimizers, and the best one was
the one obtained with RMSProp: 0.721 +- 0.095
----

Now we explored more advanced techniques.
First we tried to explore variable learning rates and momentums, using a manual decay.
We performed some grid searches to tune the decay value and the patience.

We also tried to use ReduceLROnPlateau. In this case we had 3 hyperparameters:
-initial learning rate
-the factor of reduction of the lr
-the patience (#epochs)
Keeping the other model hyperparameters fixed, we explored with a grid search:
-initial lr = [1e-4,2e-4,3e-4] for sgd and adam, [1e-5,2e-5,3e-5] for rsmprop
-factors = [0.5,0.6,0.7,0.8,0.9]
-lr_patiences = [5,10,15]

We found our best Val MEE = 0.687 +- 0.084  using RMSProp and hyperparameters: lr_init=3e-5, factor=0.7, patience=10

---
Finally we wanted to try different distributions of neurons among three layers.
Keeping fixed other parameters, we performed a grid search changing the proportion of neurons.
The proportions we tried are:
[[0.1,0.8,0.1],[0.2,0.6,0.2],[0.25,0.50,0.25],
[0.2,0.7,0.1],[0.1,0.7,0.2],[0.3,0.6,0.1]]
[[0.8,0.1,0.1],[0.7,0.2,0.1],[0.6,0.3,0.1],[0.6,0.2,0.2],
[0.8,0.15,0.05],[0.5,0.4,0.1],[0.5,0.25,0.25], [0.5,0.3,0.2]]
[[0.1,0.1,0.8],[0.1,0.2,0.7],[0.1,0.3,0.6],[0.2,0.2,0.6],[0.05,0.15,0.8],[0.1,0.4,0.5],[0.25,0.25,0.5], [0.2,0.3,0.5]]
[[0.2,0.3,0.5],[0.25,0.25,0.5],[0.15,0.35,0.5],[0.15,0.3,0.55],[0.1,0.25,0.55],[0.05,0.40,0.55],[0.25,0.35,0.4]]

The best configuration was: [0.25, 0.35, 0.4], with
train_mee = 0.2794 +- 0.0043 | val_mee = 0.698 +- 0.085
The performance is comparable with the case of equally distributed neurons.

----------------------------------------------------------------------------------------------------------------------------
3. SVM

Firt we want to choose the best kernel for our svm. We compared between rbf and polinomial.
Both cases have C and epsilons as hyperparameters. rbf kernel has also gamma, which is related to the
std. of the gaussian. for the polinomial kernel, the degree of the polinomial is another hyperparameter.
We performed two coarse grid searches, one for each kernel:

kernel = rbf (2 min)
Cs = [0.01,0.1,1,10,100,1000]
epsilons = [0.01,0.1,1,10]
gammas = ['scale',0.01,0.1,1,10]
Best Hp: [1000, 0.1, ‘scale’]  with MEE = 0.640 +- 0.050

kernel = poly (34  min)
Cs = [1000,2000,3000,4000,5000]
degrees = np.arange(3,30,1)
epsilons = [0.1,0.2,0.3,0.4,0.5]
Best Hp: [1000, 0.01, 5]  with MEE = 0.82 +- 0.13

The best kernel is rbf. We then try to explore finer values with another grid search:

kernel = rbf
Cs = [1000,2000,3000,4000,5000]
epsilons = [0.1,0.2,0.3,0.4,0.5]
gammas = ['scale',0.1,0.2,0.3,0.4,0.5]
Best Hp: ['rbf', 3000, 0.1. ‘scale’]
with MEE = 0.617 +- 0.066

We now fixed all values except C, and vary C in the range [200,10000] at steps of 200.
We find the best C is 2800.

Now we fixed C to 2800, and other values except epsilon, and varied epsilon in the range [0.01,0.20] at steps of 0.01.
We concluded that the best epsilon was 0.05

The final best values were then kernel=rbf, epsilon = 0.05, C = 2800;

